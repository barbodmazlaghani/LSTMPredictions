{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "\n",
        "# !cp \"/content/gdrive/My Drive/DataAugumentation.zip\" .\n",
        "#ADDED NEW SOURCE\n",
        "!cp \"/content/gdrive/My Drive/data_aug(3_slices_with_repeated)_acceleration_full_data_20000.zip\" .\n",
        "!unzip -qq DataAugumentation.zip\n",
        "!unzip -qq data_aug_3_slices_with_repeated_cluster_5.zip\n",
        "!rm DataAugumentation.zip\n",
        "!rm data_aug_3_slices_with_repeated_cluster_5.zip\n",
        "data_path = 'DataAugumentation'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X_RQgrvEbkfv",
        "outputId": "ee30f8da-fad1-41e3-cabf-b8080356bc2c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "unzip:  cannot find or open DataAugumentation.zip, DataAugumentation.zip.zip or DataAugumentation.zip.ZIP.\n",
            "unzip:  cannot find or open data_aug_3_slices_with_repeated_cluster_5.zip, data_aug_3_slices_with_repeated_cluster_5.zip.zip or data_aug_3_slices_with_repeated_cluster_5.zip.ZIP.\n",
            "rm: cannot remove 'DataAugumentation.zip': No such file or directory\n",
            "rm: cannot remove 'data_aug_3_slices_with_repeated_cluster_5.zip': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -qq '/content/data_aug(3_slices_with_repeated)_acceleration_full_data_20000.zip'"
      ],
      "metadata": {
        "id": "5xBVXOonbnUe"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import joblib\n",
        "\n",
        "SEQUENCE_LENGTH = 600\n",
        "BATCH_SIZE = 45\n",
        "EPOCHS = 250\n",
        "LEARNING_RATE = 1e-5\n",
        "PLOT_SAVE_DIR = 'predicted_vs_actual_plots'\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(DEVICE)\n",
        "# Data processing function\n",
        "def process_file(file_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    df['Time'] = df['Time'] - df['Time'].iloc[0]\n",
        "\n",
        "    df['Momentary fuel consumption'] = df['Trip fuel consumption'].diff().fillna(0)\n",
        "    df['Acceleration'] = df['Speed'].diff().fillna(0)\n",
        "\n",
        "    features = df[['Engine speed', 'Speed', 'slope', 'Acceleration']]\n",
        "    target = df['Momentary fuel consumption']\n",
        "\n",
        "    features = features.iloc[:SEQUENCE_LENGTH]\n",
        "    target = target.iloc[:SEQUENCE_LENGTH]\n",
        "\n",
        "    return features.values, target.values\n",
        "\n",
        "scaler_X = MinMaxScaler()\n",
        "scaler_y = MinMaxScaler()\n",
        "\n",
        "X_original = []\n",
        "y_original = []\n",
        "X_augmented = []\n",
        "y_augmented = []\n",
        "\n",
        "base_folder_path = '/content/'\n",
        "\n",
        "# Process the data\n",
        "for i in range(6):\n",
        "    if i == 5:\n",
        "        folder_path = os.path.join(base_folder_path, f'data_aug(3_slices_with_repeated)_cluster_{i}')\n",
        "        for filename in os.listdir(folder_path):\n",
        "            if filename.endswith('.csv'):\n",
        "                file_path = os.path.join(folder_path, filename)\n",
        "                features, target = process_file(file_path)\n",
        "\n",
        "                slices = filename.split('_')\n",
        "                is_original_trip = slices[2] == slices[6] and slices[6] == slices[10]\n",
        "\n",
        "                if is_original_trip:\n",
        "                    X_original.append(features)\n",
        "                    y_original.append(target)\n",
        "                else:\n",
        "                    X_augmented.append(features)\n",
        "                    y_augmented.append(target)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dIRpL-EGbjhP",
        "outputId": "7d54c207-5aad-4cfe-b687-fad8ddfb8ed3"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Function to pad or truncate sequences\n",
        "def pad_or_truncate(sequence, length):\n",
        "    if len(sequence) > length:\n",
        "        return sequence[:length]\n",
        "    elif len(sequence) < length:\n",
        "        return np.pad(sequence, ((0, length - len(sequence)), (0, 0)), mode='constant')\n",
        "    else:\n",
        "        return sequence\n",
        "\n"
      ],
      "metadata": {
        "id": "btJOSO6edHGt"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Apply padding/truncating to ensure consistent sequence length\n",
        "X_original = [pad_or_truncate(x, SEQUENCE_LENGTH) for x in X_original]\n",
        "y_original = [pad_or_truncate(y.reshape(-1, 1), SEQUENCE_LENGTH) for y in y_original]\n",
        "X_augmented = [pad_or_truncate(x, SEQUENCE_LENGTH) for x in X_augmented]\n",
        "y_augmented = [pad_or_truncate(y.reshape(-1, 1), SEQUENCE_LENGTH) for y in y_augmented]\n"
      ],
      "metadata": {
        "id": "nOGD-6Ibekzm"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Convert data to PyTorch tensors\n",
        "try:\n",
        "    X_original = torch.tensor(np.array(X_original), dtype=torch.float32).to(DEVICE)\n",
        "    y_original = torch.tensor(np.array(y_original), dtype=torch.float32).to(DEVICE)\n",
        "    X_augmented = torch.tensor(np.array(X_augmented), dtype=torch.float32).to(DEVICE)\n",
        "    y_augmented = torch.tensor(np.array(y_augmented), dtype=torch.float32).to(DEVICE)\n",
        "except Exception as e:\n",
        "    print(f\"Error during tensor conversion: {e}\")\n",
        "    print(f\"Shapes: X_original - {np.array(X_original).shape}, y_original - {np.array(y_original).shape}\")\n",
        "    print(f\"Shapes: X_augmented - {np.array(X_augmented).shape}, y_augmented - {np.array(y_augmented).shape}\")\n",
        "    raise\n"
      ],
      "metadata": {
        "id": "eTGExYs0enuV"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Split into training and test sets\n",
        "num_test = int(0.2 * len(X_original))\n",
        "X_test = X_original[:num_test]\n",
        "y_test = y_original[:num_test]\n",
        "X_train = torch.cat([X_original[num_test:], X_augmented])\n",
        "y_train = torch.cat([y_original[num_test:], y_augmented])\n",
        "\n",
        "# X_train = X_original[num_test:]\n",
        "# y_train = y_original[num_test:]\n"
      ],
      "metadata": {
        "id": "DN9TrizAeqPk"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Convert the min and max values to tensors\n",
        "min_val_x = torch.tensor([0, 0, -10, -10], dtype=torch.float32).to(DEVICE)\n",
        "max_val_x = torch.tensor([8000, 150, 10, 10], dtype=torch.float32).to(DEVICE)\n",
        "\n",
        "min_val_y = torch.tensor([0], dtype=torch.float32).to(DEVICE)\n",
        "max_val_y = torch.tensor([20000], dtype=torch.float32).to(DEVICE)\n",
        "\n",
        "# Custom normalization function for X\n",
        "def custom_normalize_X(data, min_vals, max_vals):\n",
        "    for i in range(data.shape[-1]):\n",
        "        data[:, :, i] = (data[:, :, i] - min_vals[i]) / (max_vals[i] - min_vals[i])\n",
        "    return data\n",
        "\n",
        "# Custom normalization function for y\n",
        "def custom_normalize_y(data, min_val, max_val):\n",
        "    return (data - min_val) / (max_val - min_val)\n",
        "\n",
        "# Normalize X_train and X_test\n",
        "X_train_normalized = custom_normalize_X(X_train, min_val_x, max_val_x)\n",
        "X_test_normalized = custom_normalize_X(X_test, min_val_x, max_val_x)\n",
        "\n",
        "# Normalize y_train and y_test\n",
        "y_train_normalized = custom_normalize_y(y_train, min_val_y, max_val_y)\n",
        "y_test_normalized = custom_normalize_y(y_test, min_val_y, max_val_y)\n"
      ],
      "metadata": {
        "id": "YSaA3VdOes7k"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define the PyTorch model\n",
        "class FuelConsumptionModel(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(FuelConsumptionModel, self).__init__()\n",
        "        self.lstm1 = nn.LSTM(input_size,32, batch_first=True, bidirectional=True)\n",
        "        self.dropout1 = nn.Dropout(0.2)\n",
        "        self.lstm2 = nn.LSTM(64, 32, batch_first=True, bidirectional=True)\n",
        "        self.dropout2 = nn.Dropout(0.2)\n",
        "        self.dense = nn.Linear(64, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x, _ = self.lstm1(x)\n",
        "        x = self.dropout1(x)\n",
        "        x, _ = self.lstm2(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.dense(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "doWF6sefexia"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Instantiate the model, define the loss function and the optimizer\n",
        "model = FuelConsumptionModel(input_size=X_train_normalized.shape[-1]).to(DEVICE)\n",
        "criterion = nn.L1Loss()\n",
        "weight_decay = 1e-4  # L2 regularization factor\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=weight_decay)"
      ],
      "metadata": {
        "id": "mRtldQG5e4-a"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assume 80% of the data is used for training and 20% for validation\n",
        "train_size = int(0.8 * len(X_train_normalized))\n",
        "val_size = len(X_train_normalized) - train_size\n",
        "\n",
        "# Split the data while preserving the order\n",
        "X_train_split = X_train_normalized[:train_size]\n",
        "y_train_split = y_train_normalized[:train_size]\n",
        "\n",
        "X_val_split = X_train_normalized[train_size:]\n",
        "y_val_split = y_train_normalized[train_size:]\n",
        "\n",
        "# Create DataLoader for training and validation sets\n",
        "train_loader = DataLoader(TensorDataset(X_train_split, y_train_split), batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(TensorDataset(X_val_split, y_val_split), batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NrURTgcFgyYL"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Early stopping parameters\n",
        "patience = 10  # Number of epochs to wait before stopping if no improvement\n",
        "best_loss = float('inf')  # Initialize best loss to infinity\n",
        "epochs_without_improvement = 0  # Counter for epochs without improvement\n",
        "\n",
        "# Training loop with early stopping\n",
        "model.train()\n",
        "for epoch in range(EPOCHS):\n",
        "    running_loss = 0.0\n",
        "    model.train()  # Ensure model is in training mode\n",
        "\n",
        "    # Training phase\n",
        "    for inputs, targets in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    # Calculate average loss for the epoch\n",
        "    avg_training_loss = running_loss / len(train_loader)\n",
        "    print(f\"Epoch [{epoch+1}/{EPOCHS}], Training Loss: {avg_training_loss:.4f}\")\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()  # Switch to evaluation mode\n",
        "    val_running_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in val_loader:  # Assume you have a validation DataLoader `val_loader`\n",
        "            outputs = model(inputs)\n",
        "            val_loss = criterion(outputs, targets)\n",
        "            val_running_loss += val_loss.item()\n",
        "\n",
        "    avg_val_loss = val_running_loss / len(val_loader)\n",
        "    print(f\"Epoch [{epoch+1}/{EPOCHS}], Validation Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "    # Early stopping check\n",
        "    if avg_val_loss < best_loss:\n",
        "        best_loss = avg_val_loss\n",
        "        epochs_without_improvement = 0\n",
        "        torch.save(model.state_dict(), 'best_fuel_consumption_model.pth')  # Save the best model\n",
        "    else:\n",
        "        epochs_without_improvement += 1\n",
        "        if epochs_without_improvement >= patience:\n",
        "            print(f\"Early stopping at epoch {epoch+1}\")\n",
        "            break\n",
        "\n",
        "# After training, you can save the final model\n",
        "torch.save(model.state_dict(), 'final_fuel_consumption_model.pth')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_z0wpMf9gK-h",
        "outputId": "6d992d28-d4c7-4e20-9b54-7a72600666f3"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/250], Training Loss: 0.0678\n",
            "Epoch [1/250], Validation Loss: 0.0437\n",
            "Epoch [2/250], Training Loss: 0.0515\n",
            "Epoch [2/250], Validation Loss: 0.0421\n",
            "Epoch [3/250], Training Loss: 0.0493\n",
            "Epoch [3/250], Validation Loss: 0.0403\n",
            "Epoch [4/250], Training Loss: 0.0471\n",
            "Epoch [4/250], Validation Loss: 0.0386\n",
            "Epoch [5/250], Training Loss: 0.0451\n",
            "Epoch [5/250], Validation Loss: 0.0371\n",
            "Epoch [6/250], Training Loss: 0.0435\n",
            "Epoch [6/250], Validation Loss: 0.0361\n",
            "Epoch [7/250], Training Loss: 0.0421\n",
            "Epoch [7/250], Validation Loss: 0.0355\n",
            "Epoch [8/250], Training Loss: 0.0409\n",
            "Epoch [8/250], Validation Loss: 0.0351\n",
            "Epoch [9/250], Training Loss: 0.0399\n",
            "Epoch [9/250], Validation Loss: 0.0347\n",
            "Epoch [10/250], Training Loss: 0.0389\n",
            "Epoch [10/250], Validation Loss: 0.0343\n",
            "Epoch [11/250], Training Loss: 0.0381\n",
            "Epoch [11/250], Validation Loss: 0.0340\n",
            "Epoch [12/250], Training Loss: 0.0374\n",
            "Epoch [12/250], Validation Loss: 0.0337\n",
            "Epoch [13/250], Training Loss: 0.0367\n",
            "Epoch [13/250], Validation Loss: 0.0334\n",
            "Epoch [14/250], Training Loss: 0.0362\n",
            "Epoch [14/250], Validation Loss: 0.0331\n",
            "Epoch [15/250], Training Loss: 0.0357\n",
            "Epoch [15/250], Validation Loss: 0.0329\n",
            "Epoch [16/250], Training Loss: 0.0353\n",
            "Epoch [16/250], Validation Loss: 0.0327\n",
            "Epoch [17/250], Training Loss: 0.0349\n",
            "Epoch [17/250], Validation Loss: 0.0325\n",
            "Epoch [18/250], Training Loss: 0.0346\n",
            "Epoch [18/250], Validation Loss: 0.0323\n",
            "Epoch [19/250], Training Loss: 0.0343\n",
            "Epoch [19/250], Validation Loss: 0.0322\n",
            "Epoch [20/250], Training Loss: 0.0340\n",
            "Epoch [20/250], Validation Loss: 0.0320\n",
            "Epoch [21/250], Training Loss: 0.0338\n",
            "Epoch [21/250], Validation Loss: 0.0319\n",
            "Epoch [22/250], Training Loss: 0.0336\n",
            "Epoch [22/250], Validation Loss: 0.0317\n",
            "Epoch [23/250], Training Loss: 0.0334\n",
            "Epoch [23/250], Validation Loss: 0.0316\n",
            "Epoch [24/250], Training Loss: 0.0332\n",
            "Epoch [24/250], Validation Loss: 0.0314\n",
            "Epoch [25/250], Training Loss: 0.0330\n",
            "Epoch [25/250], Validation Loss: 0.0313\n",
            "Epoch [26/250], Training Loss: 0.0328\n",
            "Epoch [26/250], Validation Loss: 0.0312\n",
            "Epoch [27/250], Training Loss: 0.0326\n",
            "Epoch [27/250], Validation Loss: 0.0311\n",
            "Epoch [28/250], Training Loss: 0.0324\n",
            "Epoch [28/250], Validation Loss: 0.0309\n",
            "Epoch [29/250], Training Loss: 0.0322\n",
            "Epoch [29/250], Validation Loss: 0.0308\n",
            "Epoch [30/250], Training Loss: 0.0321\n",
            "Epoch [30/250], Validation Loss: 0.0307\n",
            "Epoch [31/250], Training Loss: 0.0319\n",
            "Epoch [31/250], Validation Loss: 0.0306\n",
            "Epoch [32/250], Training Loss: 0.0318\n",
            "Epoch [32/250], Validation Loss: 0.0304\n",
            "Epoch [33/250], Training Loss: 0.0316\n",
            "Epoch [33/250], Validation Loss: 0.0303\n",
            "Epoch [34/250], Training Loss: 0.0315\n",
            "Epoch [34/250], Validation Loss: 0.0302\n",
            "Epoch [35/250], Training Loss: 0.0313\n",
            "Epoch [35/250], Validation Loss: 0.0301\n",
            "Epoch [36/250], Training Loss: 0.0312\n",
            "Epoch [36/250], Validation Loss: 0.0299\n",
            "Epoch [37/250], Training Loss: 0.0310\n",
            "Epoch [37/250], Validation Loss: 0.0298\n",
            "Epoch [38/250], Training Loss: 0.0309\n",
            "Epoch [38/250], Validation Loss: 0.0297\n",
            "Epoch [39/250], Training Loss: 0.0308\n",
            "Epoch [39/250], Validation Loss: 0.0296\n",
            "Epoch [40/250], Training Loss: 0.0306\n",
            "Epoch [40/250], Validation Loss: 0.0295\n",
            "Epoch [41/250], Training Loss: 0.0305\n",
            "Epoch [41/250], Validation Loss: 0.0294\n",
            "Epoch [42/250], Training Loss: 0.0304\n",
            "Epoch [42/250], Validation Loss: 0.0293\n",
            "Epoch [43/250], Training Loss: 0.0303\n",
            "Epoch [43/250], Validation Loss: 0.0292\n",
            "Epoch [44/250], Training Loss: 0.0301\n",
            "Epoch [44/250], Validation Loss: 0.0291\n",
            "Epoch [45/250], Training Loss: 0.0300\n",
            "Epoch [45/250], Validation Loss: 0.0290\n",
            "Epoch [46/250], Training Loss: 0.0299\n",
            "Epoch [46/250], Validation Loss: 0.0289\n",
            "Epoch [47/250], Training Loss: 0.0298\n",
            "Epoch [47/250], Validation Loss: 0.0288\n",
            "Epoch [48/250], Training Loss: 0.0297\n",
            "Epoch [48/250], Validation Loss: 0.0287\n",
            "Epoch [49/250], Training Loss: 0.0296\n",
            "Epoch [49/250], Validation Loss: 0.0286\n",
            "Epoch [50/250], Training Loss: 0.0295\n",
            "Epoch [50/250], Validation Loss: 0.0285\n",
            "Epoch [51/250], Training Loss: 0.0294\n",
            "Epoch [51/250], Validation Loss: 0.0284\n",
            "Epoch [52/250], Training Loss: 0.0293\n",
            "Epoch [52/250], Validation Loss: 0.0283\n",
            "Epoch [53/250], Training Loss: 0.0292\n",
            "Epoch [53/250], Validation Loss: 0.0282\n",
            "Epoch [54/250], Training Loss: 0.0291\n",
            "Epoch [54/250], Validation Loss: 0.0282\n",
            "Epoch [55/250], Training Loss: 0.0290\n",
            "Epoch [55/250], Validation Loss: 0.0281\n",
            "Epoch [56/250], Training Loss: 0.0289\n",
            "Epoch [56/250], Validation Loss: 0.0280\n",
            "Epoch [57/250], Training Loss: 0.0288\n",
            "Epoch [57/250], Validation Loss: 0.0279\n",
            "Epoch [58/250], Training Loss: 0.0287\n",
            "Epoch [58/250], Validation Loss: 0.0278\n",
            "Epoch [59/250], Training Loss: 0.0286\n",
            "Epoch [59/250], Validation Loss: 0.0277\n",
            "Epoch [60/250], Training Loss: 0.0285\n",
            "Epoch [60/250], Validation Loss: 0.0276\n",
            "Epoch [61/250], Training Loss: 0.0284\n",
            "Epoch [61/250], Validation Loss: 0.0274\n",
            "Epoch [62/250], Training Loss: 0.0283\n",
            "Epoch [62/250], Validation Loss: 0.0273\n",
            "Epoch [63/250], Training Loss: 0.0281\n",
            "Epoch [63/250], Validation Loss: 0.0272\n",
            "Epoch [64/250], Training Loss: 0.0280\n",
            "Epoch [64/250], Validation Loss: 0.0270\n",
            "Epoch [65/250], Training Loss: 0.0279\n",
            "Epoch [65/250], Validation Loss: 0.0269\n",
            "Epoch [66/250], Training Loss: 0.0277\n",
            "Epoch [66/250], Validation Loss: 0.0267\n",
            "Epoch [67/250], Training Loss: 0.0276\n",
            "Epoch [67/250], Validation Loss: 0.0265\n",
            "Epoch [68/250], Training Loss: 0.0274\n",
            "Epoch [68/250], Validation Loss: 0.0264\n",
            "Epoch [69/250], Training Loss: 0.0272\n",
            "Epoch [69/250], Validation Loss: 0.0262\n",
            "Epoch [70/250], Training Loss: 0.0271\n",
            "Epoch [70/250], Validation Loss: 0.0260\n",
            "Epoch [71/250], Training Loss: 0.0269\n",
            "Epoch [71/250], Validation Loss: 0.0259\n",
            "Epoch [72/250], Training Loss: 0.0268\n",
            "Epoch [72/250], Validation Loss: 0.0257\n",
            "Epoch [73/250], Training Loss: 0.0266\n",
            "Epoch [73/250], Validation Loss: 0.0256\n",
            "Epoch [74/250], Training Loss: 0.0265\n",
            "Epoch [74/250], Validation Loss: 0.0254\n",
            "Epoch [75/250], Training Loss: 0.0263\n",
            "Epoch [75/250], Validation Loss: 0.0252\n",
            "Epoch [76/250], Training Loss: 0.0262\n",
            "Epoch [76/250], Validation Loss: 0.0251\n",
            "Epoch [77/250], Training Loss: 0.0260\n",
            "Epoch [77/250], Validation Loss: 0.0249\n",
            "Epoch [78/250], Training Loss: 0.0259\n",
            "Epoch [78/250], Validation Loss: 0.0248\n",
            "Epoch [79/250], Training Loss: 0.0258\n",
            "Epoch [79/250], Validation Loss: 0.0246\n",
            "Epoch [80/250], Training Loss: 0.0256\n",
            "Epoch [80/250], Validation Loss: 0.0245\n",
            "Epoch [81/250], Training Loss: 0.0255\n",
            "Epoch [81/250], Validation Loss: 0.0244\n",
            "Epoch [82/250], Training Loss: 0.0254\n",
            "Epoch [82/250], Validation Loss: 0.0243\n",
            "Epoch [83/250], Training Loss: 0.0253\n",
            "Epoch [83/250], Validation Loss: 0.0242\n",
            "Epoch [84/250], Training Loss: 0.0252\n",
            "Epoch [84/250], Validation Loss: 0.0241\n",
            "Epoch [85/250], Training Loss: 0.0252\n",
            "Epoch [85/250], Validation Loss: 0.0240\n",
            "Epoch [86/250], Training Loss: 0.0251\n",
            "Epoch [86/250], Validation Loss: 0.0240\n",
            "Epoch [87/250], Training Loss: 0.0250\n",
            "Epoch [87/250], Validation Loss: 0.0239\n",
            "Epoch [88/250], Training Loss: 0.0249\n",
            "Epoch [88/250], Validation Loss: 0.0238\n",
            "Epoch [89/250], Training Loss: 0.0249\n",
            "Epoch [89/250], Validation Loss: 0.0238\n",
            "Epoch [90/250], Training Loss: 0.0248\n",
            "Epoch [90/250], Validation Loss: 0.0237\n",
            "Epoch [91/250], Training Loss: 0.0248\n",
            "Epoch [91/250], Validation Loss: 0.0237\n",
            "Epoch [92/250], Training Loss: 0.0247\n",
            "Epoch [92/250], Validation Loss: 0.0236\n",
            "Epoch [93/250], Training Loss: 0.0247\n",
            "Epoch [93/250], Validation Loss: 0.0236\n",
            "Epoch [94/250], Training Loss: 0.0246\n",
            "Epoch [94/250], Validation Loss: 0.0235\n",
            "Epoch [95/250], Training Loss: 0.0246\n",
            "Epoch [95/250], Validation Loss: 0.0235\n",
            "Epoch [96/250], Training Loss: 0.0245\n",
            "Epoch [96/250], Validation Loss: 0.0234\n",
            "Epoch [97/250], Training Loss: 0.0245\n",
            "Epoch [97/250], Validation Loss: 0.0234\n",
            "Epoch [98/250], Training Loss: 0.0244\n",
            "Epoch [98/250], Validation Loss: 0.0234\n",
            "Epoch [99/250], Training Loss: 0.0244\n",
            "Epoch [99/250], Validation Loss: 0.0233\n",
            "Epoch [100/250], Training Loss: 0.0243\n",
            "Epoch [100/250], Validation Loss: 0.0233\n",
            "Epoch [101/250], Training Loss: 0.0243\n",
            "Epoch [101/250], Validation Loss: 0.0232\n",
            "Epoch [102/250], Training Loss: 0.0243\n",
            "Epoch [102/250], Validation Loss: 0.0232\n",
            "Epoch [103/250], Training Loss: 0.0242\n",
            "Epoch [103/250], Validation Loss: 0.0232\n",
            "Epoch [104/250], Training Loss: 0.0242\n",
            "Epoch [104/250], Validation Loss: 0.0231\n",
            "Epoch [105/250], Training Loss: 0.0241\n",
            "Epoch [105/250], Validation Loss: 0.0231\n",
            "Epoch [106/250], Training Loss: 0.0241\n",
            "Epoch [106/250], Validation Loss: 0.0231\n",
            "Epoch [107/250], Training Loss: 0.0241\n",
            "Epoch [107/250], Validation Loss: 0.0230\n",
            "Epoch [108/250], Training Loss: 0.0240\n",
            "Epoch [108/250], Validation Loss: 0.0230\n",
            "Epoch [109/250], Training Loss: 0.0240\n",
            "Epoch [109/250], Validation Loss: 0.0230\n",
            "Epoch [110/250], Training Loss: 0.0240\n",
            "Epoch [110/250], Validation Loss: 0.0229\n",
            "Epoch [111/250], Training Loss: 0.0239\n",
            "Epoch [111/250], Validation Loss: 0.0229\n",
            "Epoch [112/250], Training Loss: 0.0239\n",
            "Epoch [112/250], Validation Loss: 0.0229\n",
            "Epoch [113/250], Training Loss: 0.0239\n",
            "Epoch [113/250], Validation Loss: 0.0228\n",
            "Epoch [114/250], Training Loss: 0.0238\n",
            "Epoch [114/250], Validation Loss: 0.0228\n",
            "Epoch [115/250], Training Loss: 0.0238\n",
            "Epoch [115/250], Validation Loss: 0.0228\n",
            "Epoch [116/250], Training Loss: 0.0238\n",
            "Epoch [116/250], Validation Loss: 0.0227\n",
            "Epoch [117/250], Training Loss: 0.0237\n",
            "Epoch [117/250], Validation Loss: 0.0227\n",
            "Epoch [118/250], Training Loss: 0.0237\n",
            "Epoch [118/250], Validation Loss: 0.0227\n",
            "Epoch [119/250], Training Loss: 0.0237\n",
            "Epoch [119/250], Validation Loss: 0.0226\n",
            "Epoch [120/250], Training Loss: 0.0237\n",
            "Epoch [120/250], Validation Loss: 0.0226\n",
            "Epoch [121/250], Training Loss: 0.0236\n",
            "Epoch [121/250], Validation Loss: 0.0226\n",
            "Epoch [122/250], Training Loss: 0.0236\n",
            "Epoch [122/250], Validation Loss: 0.0226\n",
            "Epoch [123/250], Training Loss: 0.0236\n",
            "Epoch [123/250], Validation Loss: 0.0225\n",
            "Epoch [124/250], Training Loss: 0.0236\n",
            "Epoch [124/250], Validation Loss: 0.0225\n",
            "Epoch [125/250], Training Loss: 0.0235\n",
            "Epoch [125/250], Validation Loss: 0.0225\n",
            "Epoch [126/250], Training Loss: 0.0235\n",
            "Epoch [126/250], Validation Loss: 0.0224\n",
            "Epoch [127/250], Training Loss: 0.0235\n",
            "Epoch [127/250], Validation Loss: 0.0224\n",
            "Epoch [128/250], Training Loss: 0.0235\n",
            "Epoch [128/250], Validation Loss: 0.0224\n",
            "Epoch [129/250], Training Loss: 0.0234\n",
            "Epoch [129/250], Validation Loss: 0.0224\n",
            "Epoch [130/250], Training Loss: 0.0234\n",
            "Epoch [130/250], Validation Loss: 0.0224\n",
            "Epoch [131/250], Training Loss: 0.0234\n",
            "Epoch [131/250], Validation Loss: 0.0223\n",
            "Epoch [132/250], Training Loss: 0.0234\n",
            "Epoch [132/250], Validation Loss: 0.0223\n",
            "Epoch [133/250], Training Loss: 0.0233\n",
            "Epoch [133/250], Validation Loss: 0.0223\n",
            "Epoch [134/250], Training Loss: 0.0233\n",
            "Epoch [134/250], Validation Loss: 0.0223\n",
            "Epoch [135/250], Training Loss: 0.0233\n",
            "Epoch [135/250], Validation Loss: 0.0222\n",
            "Epoch [136/250], Training Loss: 0.0233\n",
            "Epoch [136/250], Validation Loss: 0.0222\n",
            "Epoch [137/250], Training Loss: 0.0233\n",
            "Epoch [137/250], Validation Loss: 0.0222\n",
            "Epoch [138/250], Training Loss: 0.0232\n",
            "Epoch [138/250], Validation Loss: 0.0222\n",
            "Epoch [139/250], Training Loss: 0.0232\n",
            "Epoch [139/250], Validation Loss: 0.0221\n",
            "Epoch [140/250], Training Loss: 0.0232\n",
            "Epoch [140/250], Validation Loss: 0.0221\n",
            "Epoch [141/250], Training Loss: 0.0232\n",
            "Epoch [141/250], Validation Loss: 0.0221\n",
            "Epoch [142/250], Training Loss: 0.0232\n",
            "Epoch [142/250], Validation Loss: 0.0221\n",
            "Epoch [143/250], Training Loss: 0.0231\n",
            "Epoch [143/250], Validation Loss: 0.0221\n",
            "Epoch [144/250], Training Loss: 0.0231\n",
            "Epoch [144/250], Validation Loss: 0.0220\n",
            "Epoch [145/250], Training Loss: 0.0231\n",
            "Epoch [145/250], Validation Loss: 0.0220\n",
            "Epoch [146/250], Training Loss: 0.0231\n",
            "Epoch [146/250], Validation Loss: 0.0220\n",
            "Epoch [147/250], Training Loss: 0.0231\n",
            "Epoch [147/250], Validation Loss: 0.0220\n",
            "Epoch [148/250], Training Loss: 0.0230\n",
            "Epoch [148/250], Validation Loss: 0.0220\n",
            "Epoch [149/250], Training Loss: 0.0230\n",
            "Epoch [149/250], Validation Loss: 0.0219\n",
            "Epoch [150/250], Training Loss: 0.0230\n",
            "Epoch [150/250], Validation Loss: 0.0219\n",
            "Epoch [151/250], Training Loss: 0.0230\n",
            "Epoch [151/250], Validation Loss: 0.0219\n",
            "Epoch [152/250], Training Loss: 0.0230\n",
            "Epoch [152/250], Validation Loss: 0.0219\n",
            "Epoch [153/250], Training Loss: 0.0230\n",
            "Epoch [153/250], Validation Loss: 0.0219\n",
            "Epoch [154/250], Training Loss: 0.0230\n",
            "Epoch [154/250], Validation Loss: 0.0218\n",
            "Epoch [155/250], Training Loss: 0.0229\n",
            "Epoch [155/250], Validation Loss: 0.0218\n",
            "Epoch [156/250], Training Loss: 0.0229\n",
            "Epoch [156/250], Validation Loss: 0.0218\n",
            "Epoch [157/250], Training Loss: 0.0229\n",
            "Epoch [157/250], Validation Loss: 0.0218\n",
            "Epoch [158/250], Training Loss: 0.0229\n",
            "Epoch [158/250], Validation Loss: 0.0218\n",
            "Epoch [159/250], Training Loss: 0.0229\n",
            "Epoch [159/250], Validation Loss: 0.0218\n",
            "Epoch [160/250], Training Loss: 0.0229\n",
            "Epoch [160/250], Validation Loss: 0.0218\n",
            "Epoch [161/250], Training Loss: 0.0229\n",
            "Epoch [161/250], Validation Loss: 0.0217\n",
            "Epoch [162/250], Training Loss: 0.0228\n",
            "Epoch [162/250], Validation Loss: 0.0217\n",
            "Epoch [163/250], Training Loss: 0.0228\n",
            "Epoch [163/250], Validation Loss: 0.0217\n",
            "Epoch [164/250], Training Loss: 0.0228\n",
            "Epoch [164/250], Validation Loss: 0.0217\n",
            "Epoch [165/250], Training Loss: 0.0228\n",
            "Epoch [165/250], Validation Loss: 0.0217\n",
            "Epoch [166/250], Training Loss: 0.0228\n",
            "Epoch [166/250], Validation Loss: 0.0217\n",
            "Epoch [167/250], Training Loss: 0.0228\n",
            "Epoch [167/250], Validation Loss: 0.0217\n",
            "Epoch [168/250], Training Loss: 0.0228\n",
            "Epoch [168/250], Validation Loss: 0.0216\n",
            "Epoch [169/250], Training Loss: 0.0227\n",
            "Epoch [169/250], Validation Loss: 0.0216\n",
            "Epoch [170/250], Training Loss: 0.0227\n",
            "Epoch [170/250], Validation Loss: 0.0216\n",
            "Epoch [171/250], Training Loss: 0.0227\n",
            "Epoch [171/250], Validation Loss: 0.0216\n",
            "Epoch [172/250], Training Loss: 0.0227\n",
            "Epoch [172/250], Validation Loss: 0.0216\n",
            "Epoch [173/250], Training Loss: 0.0227\n",
            "Epoch [173/250], Validation Loss: 0.0216\n",
            "Epoch [174/250], Training Loss: 0.0227\n",
            "Epoch [174/250], Validation Loss: 0.0215\n",
            "Epoch [175/250], Training Loss: 0.0227\n",
            "Epoch [175/250], Validation Loss: 0.0215\n",
            "Epoch [176/250], Training Loss: 0.0227\n",
            "Epoch [176/250], Validation Loss: 0.0215\n",
            "Epoch [177/250], Training Loss: 0.0226\n",
            "Epoch [177/250], Validation Loss: 0.0215\n",
            "Epoch [178/250], Training Loss: 0.0226\n",
            "Epoch [178/250], Validation Loss: 0.0215\n",
            "Epoch [179/250], Training Loss: 0.0226\n",
            "Epoch [179/250], Validation Loss: 0.0215\n",
            "Epoch [180/250], Training Loss: 0.0226\n",
            "Epoch [180/250], Validation Loss: 0.0215\n",
            "Epoch [181/250], Training Loss: 0.0226\n",
            "Epoch [181/250], Validation Loss: 0.0215\n",
            "Epoch [182/250], Training Loss: 0.0226\n",
            "Epoch [182/250], Validation Loss: 0.0214\n",
            "Epoch [183/250], Training Loss: 0.0226\n",
            "Epoch [183/250], Validation Loss: 0.0214\n",
            "Epoch [184/250], Training Loss: 0.0226\n",
            "Epoch [184/250], Validation Loss: 0.0214\n",
            "Epoch [185/250], Training Loss: 0.0226\n",
            "Epoch [185/250], Validation Loss: 0.0214\n",
            "Epoch [186/250], Training Loss: 0.0225\n",
            "Epoch [186/250], Validation Loss: 0.0214\n",
            "Epoch [187/250], Training Loss: 0.0225\n",
            "Epoch [187/250], Validation Loss: 0.0214\n",
            "Epoch [188/250], Training Loss: 0.0225\n",
            "Epoch [188/250], Validation Loss: 0.0214\n",
            "Epoch [189/250], Training Loss: 0.0225\n",
            "Epoch [189/250], Validation Loss: 0.0213\n",
            "Epoch [190/250], Training Loss: 0.0225\n",
            "Epoch [190/250], Validation Loss: 0.0213\n",
            "Epoch [191/250], Training Loss: 0.0225\n",
            "Epoch [191/250], Validation Loss: 0.0213\n",
            "Epoch [192/250], Training Loss: 0.0225\n",
            "Epoch [192/250], Validation Loss: 0.0213\n",
            "Epoch [193/250], Training Loss: 0.0225\n",
            "Epoch [193/250], Validation Loss: 0.0213\n",
            "Epoch [194/250], Training Loss: 0.0225\n",
            "Epoch [194/250], Validation Loss: 0.0213\n",
            "Epoch [195/250], Training Loss: 0.0224\n",
            "Epoch [195/250], Validation Loss: 0.0213\n",
            "Epoch [196/250], Training Loss: 0.0224\n",
            "Epoch [196/250], Validation Loss: 0.0213\n",
            "Epoch [197/250], Training Loss: 0.0224\n",
            "Epoch [197/250], Validation Loss: 0.0213\n",
            "Epoch [198/250], Training Loss: 0.0224\n",
            "Epoch [198/250], Validation Loss: 0.0212\n",
            "Epoch [199/250], Training Loss: 0.0224\n",
            "Epoch [199/250], Validation Loss: 0.0212\n",
            "Epoch [200/250], Training Loss: 0.0224\n",
            "Epoch [200/250], Validation Loss: 0.0212\n",
            "Epoch [201/250], Training Loss: 0.0224\n",
            "Epoch [201/250], Validation Loss: 0.0212\n",
            "Epoch [202/250], Training Loss: 0.0224\n",
            "Epoch [202/250], Validation Loss: 0.0212\n",
            "Epoch [203/250], Training Loss: 0.0224\n",
            "Epoch [203/250], Validation Loss: 0.0212\n",
            "Epoch [204/250], Training Loss: 0.0223\n",
            "Epoch [204/250], Validation Loss: 0.0212\n",
            "Epoch [205/250], Training Loss: 0.0223\n",
            "Epoch [205/250], Validation Loss: 0.0212\n",
            "Epoch [206/250], Training Loss: 0.0223\n",
            "Epoch [206/250], Validation Loss: 0.0212\n",
            "Epoch [207/250], Training Loss: 0.0223\n",
            "Epoch [207/250], Validation Loss: 0.0211\n",
            "Epoch [208/250], Training Loss: 0.0223\n",
            "Epoch [208/250], Validation Loss: 0.0211\n",
            "Epoch [209/250], Training Loss: 0.0223\n",
            "Epoch [209/250], Validation Loss: 0.0211\n",
            "Epoch [210/250], Training Loss: 0.0223\n",
            "Epoch [210/250], Validation Loss: 0.0211\n",
            "Epoch [211/250], Training Loss: 0.0223\n",
            "Epoch [211/250], Validation Loss: 0.0211\n",
            "Epoch [212/250], Training Loss: 0.0223\n",
            "Epoch [212/250], Validation Loss: 0.0211\n",
            "Epoch [213/250], Training Loss: 0.0223\n",
            "Epoch [213/250], Validation Loss: 0.0211\n",
            "Epoch [214/250], Training Loss: 0.0223\n",
            "Epoch [214/250], Validation Loss: 0.0211\n",
            "Epoch [215/250], Training Loss: 0.0222\n",
            "Epoch [215/250], Validation Loss: 0.0211\n",
            "Epoch [216/250], Training Loss: 0.0222\n",
            "Epoch [216/250], Validation Loss: 0.0210\n",
            "Epoch [217/250], Training Loss: 0.0222\n",
            "Epoch [217/250], Validation Loss: 0.0210\n",
            "Epoch [218/250], Training Loss: 0.0222\n",
            "Epoch [218/250], Validation Loss: 0.0210\n",
            "Epoch [219/250], Training Loss: 0.0222\n",
            "Epoch [219/250], Validation Loss: 0.0210\n",
            "Epoch [220/250], Training Loss: 0.0222\n",
            "Epoch [220/250], Validation Loss: 0.0210\n",
            "Epoch [221/250], Training Loss: 0.0222\n",
            "Epoch [221/250], Validation Loss: 0.0210\n",
            "Epoch [222/250], Training Loss: 0.0222\n",
            "Epoch [222/250], Validation Loss: 0.0210\n",
            "Epoch [223/250], Training Loss: 0.0222\n",
            "Epoch [223/250], Validation Loss: 0.0210\n",
            "Epoch [224/250], Training Loss: 0.0222\n",
            "Epoch [224/250], Validation Loss: 0.0210\n",
            "Epoch [225/250], Training Loss: 0.0221\n",
            "Epoch [225/250], Validation Loss: 0.0209\n",
            "Epoch [226/250], Training Loss: 0.0221\n",
            "Epoch [226/250], Validation Loss: 0.0209\n",
            "Epoch [227/250], Training Loss: 0.0221\n",
            "Epoch [227/250], Validation Loss: 0.0209\n",
            "Epoch [228/250], Training Loss: 0.0221\n",
            "Epoch [228/250], Validation Loss: 0.0209\n",
            "Epoch [229/250], Training Loss: 0.0221\n",
            "Epoch [229/250], Validation Loss: 0.0209\n",
            "Epoch [230/250], Training Loss: 0.0221\n",
            "Epoch [230/250], Validation Loss: 0.0209\n",
            "Epoch [231/250], Training Loss: 0.0221\n",
            "Epoch [231/250], Validation Loss: 0.0209\n",
            "Epoch [232/250], Training Loss: 0.0221\n",
            "Epoch [232/250], Validation Loss: 0.0209\n",
            "Epoch [233/250], Training Loss: 0.0221\n",
            "Epoch [233/250], Validation Loss: 0.0209\n",
            "Epoch [234/250], Training Loss: 0.0221\n",
            "Epoch [234/250], Validation Loss: 0.0208\n",
            "Epoch [235/250], Training Loss: 0.0221\n",
            "Epoch [235/250], Validation Loss: 0.0208\n",
            "Epoch [236/250], Training Loss: 0.0220\n",
            "Epoch [236/250], Validation Loss: 0.0208\n",
            "Epoch [237/250], Training Loss: 0.0220\n",
            "Epoch [237/250], Validation Loss: 0.0208\n",
            "Epoch [238/250], Training Loss: 0.0220\n",
            "Epoch [238/250], Validation Loss: 0.0208\n",
            "Epoch [239/250], Training Loss: 0.0220\n",
            "Epoch [239/250], Validation Loss: 0.0208\n",
            "Epoch [240/250], Training Loss: 0.0220\n",
            "Epoch [240/250], Validation Loss: 0.0208\n",
            "Epoch [241/250], Training Loss: 0.0220\n",
            "Epoch [241/250], Validation Loss: 0.0208\n",
            "Epoch [242/250], Training Loss: 0.0220\n",
            "Epoch [242/250], Validation Loss: 0.0208\n",
            "Epoch [243/250], Training Loss: 0.0220\n",
            "Epoch [243/250], Validation Loss: 0.0207\n",
            "Epoch [244/250], Training Loss: 0.0220\n",
            "Epoch [244/250], Validation Loss: 0.0208\n",
            "Epoch [245/250], Training Loss: 0.0220\n",
            "Epoch [245/250], Validation Loss: 0.0207\n",
            "Epoch [246/250], Training Loss: 0.0219\n",
            "Epoch [246/250], Validation Loss: 0.0207\n",
            "Epoch [247/250], Training Loss: 0.0219\n",
            "Epoch [247/250], Validation Loss: 0.0207\n",
            "Epoch [248/250], Training Loss: 0.0219\n",
            "Epoch [248/250], Validation Loss: 0.0207\n",
            "Epoch [249/250], Training Loss: 0.0219\n",
            "Epoch [249/250], Validation Loss: 0.0207\n",
            "Epoch [250/250], Training Loss: 0.0219\n",
            "Epoch [250/250], Validation Loss: 0.0207\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "QgiHLL31mUEE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "yg9vSugumUBg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "7o2xWWKvmT67"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#test"
      ],
      "metadata": {
        "id": "X7RXfsU_mNR8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "# Constants\n",
        "SEQUENCE_LENGTH = 600\n",
        "PLOT_SAVE_DIR = 'predicted_vs_actual_plots'\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Define the PyTorch model structure (same as the one used for training)\n",
        "class FuelConsumptionModel(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(FuelConsumptionModel, self).__init__()\n",
        "        self.lstm1 = nn.LSTM(input_size, 32, batch_first=True, bidirectional=True)\n",
        "        self.dropout1 = nn.Dropout(0.2)\n",
        "        self.lstm2 = nn.LSTM(64, 32, batch_first=True, bidirectional=True)\n",
        "        self.dropout2 = nn.Dropout(0.2)\n",
        "        self.dense = nn.Linear(64, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x, _ = self.lstm1(x)\n",
        "        x = self.dropout1(x)\n",
        "        x, _ = self.lstm2(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.dense(x)\n",
        "        return x\n",
        "\n",
        "# Load the trained model\n",
        "def load_trained_model(model_path, input_size):\n",
        "    model = FuelConsumptionModel(input_size=input_size)\n",
        "    model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n",
        "    model.to(DEVICE)\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "# Process the file and prepare segments\n",
        "def process_file(file_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "    df['Time'] = df['Time'] - df['Time'].iloc[0]\n",
        "    df['Trip fuel consumption'] = df['Trip fuel consumption'] - df['Trip fuel consumption'].iloc[0]\n",
        "    df['Acceleration'] = df['Speed'].diff().fillna(0)\n",
        "    features = df[['Engine speed', 'Speed', 'slope', 'Acceleration']]\n",
        "    df['Momentary fuel consumption'] = df['Trip fuel consumption'].diff().fillna(0)\n",
        "    target = df['Momentary fuel consumption']\n",
        "    return features, target\n",
        "\n",
        "# Pad and normalize the data\n",
        "def pad_and_normalize(data, sequence_length=SEQUENCE_LENGTH):\n",
        "    padded_data = np.zeros((len(data), sequence_length, data[0].shape[1]))\n",
        "    for i, seq in enumerate(data):\n",
        "        length = min(len(seq), sequence_length)\n",
        "        padded_data[i, :length] = seq[:length]\n",
        "\n",
        "    # Normalization (same as in your script)\n",
        "    min_val_x = [0, 0, -10, -10]\n",
        "    max_val_x = [8000, 150, 10, 10]\n",
        "    for i in range(padded_data.shape[-1]):\n",
        "        padded_data[:, :, i] = (padded_data[:, :, i] - min_val_x[i]) / (max_val_x[i] - min_val_x[i])\n",
        "\n",
        "    return torch.tensor(padded_data, dtype=torch.float32).to(DEVICE)\n",
        "\n",
        "# Predict and plot the results\n",
        "def plot_predicted_vs_real(input_file, model):\n",
        "    features, actual_values = process_file(input_file)\n",
        "    num_segments = len(features) // SEQUENCE_LENGTH\n",
        "    predictions = []\n",
        "\n",
        "    for i in range(num_segments):\n",
        "        segment = features.iloc[i * SEQUENCE_LENGTH:(i + 1) * SEQUENCE_LENGTH]\n",
        "        segment_normalized = pad_and_normalize([segment.values])\n",
        "        with torch.no_grad():\n",
        "            segment_predictions = model(segment_normalized).cpu().numpy()\n",
        "        predictions.extend(segment_predictions.flatten() * 20000)\n",
        "\n",
        "    # Handle any remaining data\n",
        "    remainder = len(features) % SEQUENCE_LENGTH\n",
        "    if remainder != 0:\n",
        "        last_segment = features.iloc[-remainder:]\n",
        "        last_segment_normalized = pad_and_normalize([last_segment.values])\n",
        "        with torch.no_grad():\n",
        "            last_segment_predictions = model(last_segment_normalized).cpu().numpy()\n",
        "        predictions.extend(last_segment_predictions.flatten() * 20000)\n",
        "\n",
        "    predictions = np.array(predictions)\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(np.cumsum(actual_values.values[:len(predictions)], axis=0), label='Real', color='blue')\n",
        "    plt.plot(np.cumsum(predictions[:len(actual_values)], axis=0), label='Predicted', color='red')\n",
        "    plt.xlabel('Index')\n",
        "    plt.ylabel('Fuel Consumption')\n",
        "    plt.title('Predicted vs Real Fuel Consumption')\n",
        "    plt.legend()\n",
        "\n",
        "    directory, filename = os.path.split(input_file)\n",
        "    plot_filename = os.path.join(directory, f'{os.path.splitext(filename)[0]}_predicted_vs_real.png')\n",
        "    plt.savefig(plot_filename)\n",
        "    plt.close()\n",
        "\n",
        "    print(f\"Plot saved as: {plot_filename}\")\n",
        "\n",
        "    # Save predictions and actual values to CSV\n",
        "    results_df = pd.DataFrame({\n",
        "        'Speed': features[\"Speed\"].iloc[:len(predictions)],\n",
        "        'Actual': np.cumsum(actual_values.values[:len(predictions)], axis=0),\n",
        "        'Predicted': np.cumsum(predictions[:len(actual_values)], axis=0)\n",
        "    })\n",
        "\n",
        "    csv_filename = os.path.join(directory, f'{os.path.splitext(filename)[0]}_predicted_vs_real.csv')\n",
        "    results_df.to_csv(csv_filename, index=False)\n",
        "\n",
        "# Paths to model and input file\n",
        "model_path = '/content/best_fuel_consumption_model.pth'  # Adjust this to your PyTorch model path\n",
        "input_file_path = '/content/NEDC_1000_slope_added.csv'\n",
        "\n",
        "# Load model and predict\n",
        "input_size = 4  # Number of features in the input data\n",
        "model = load_trained_model(model_path, input_size)\n",
        "plot_predicted_vs_real(input_file_path, model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZreM025mMne",
        "outputId": "16818867-1384-48bb-e809-2e5ea2f3bd22"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Plot saved as: /content/NEDC_1000_slope_added_predicted_vs_real.png\n"
          ]
        }
      ]
    }
  ]
}
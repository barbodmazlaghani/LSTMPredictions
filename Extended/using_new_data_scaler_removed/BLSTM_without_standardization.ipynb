{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6AFXakzVdtXB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0def2156-516d-401f-82b8-b64908644c2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "unzip:  cannot find or open DataAugumentation.zip, DataAugumentation.zip.zip or DataAugumentation.zip.ZIP.\n",
            "unzip:  cannot find or open data_aug_3_slices_with_repeated_cluster_5.zip, data_aug_3_slices_with_repeated_cluster_5.zip.zip or data_aug_3_slices_with_repeated_cluster_5.zip.ZIP.\n",
            "rm: cannot remove 'DataAugumentation.zip': No such file or directory\n",
            "rm: cannot remove 'data_aug_3_slices_with_repeated_cluster_5.zip': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "\n",
        "# !cp \"/content/gdrive/My Drive/DataAugumentation.zip\" .\n",
        "#ADDED NEW SOURCE\n",
        "!cp \"/content/gdrive/My Drive/data_aug(3_slices_with_repeated)_acceleration_full_data_20000.zip\" .\n",
        "!unzip -qq DataAugumentation.zip\n",
        "!unzip -qq data_aug_3_slices_with_repeated_cluster_5.zip\n",
        "!rm DataAugumentation.zip\n",
        "!rm data_aug_3_slices_with_repeated_cluster_5.zip\n",
        "data_path = 'DataAugumentation'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "7wEzhX_xIQ2i"
      },
      "outputs": [],
      "source": [
        "!unzip -qq data_aug_3_slices_with_repeated_acceleration_full_data_20000.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "_da74MVGeyeM"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import joblib\n",
        "\n",
        "SEQUENCE_LENGTH = 600\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 50\n",
        "LEARNING_RATE = 1e-4\n",
        "PLOT_SAVE_DIR = 'predicted_vs_actual_plots'\n",
        "\n",
        "\n",
        "def process_file(file_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    df['Time'] = df['Time'] - df['Time'].iloc[0]\n",
        "\n",
        "    df['Momentary fuel consumption'] = df['Trip fuel consumption'].diff().fillna(0)\n",
        "    df['Acceleration'] = df['Speed'].diff().fillna(0)\n",
        "\n",
        "    # features = df[['Engine speed', 'Throttle position', 'Accelerator pedal position', 'Speed']]\n",
        "    features = df[['Engine speed', 'Speed', 'slope', 'Acceleration']]\n",
        "    target = df['Momentary fuel consumption']\n",
        "\n",
        "    features = features.iloc[:SEQUENCE_LENGTH]\n",
        "    target = target.iloc[:SEQUENCE_LENGTH]\n",
        "\n",
        "    return features.values, target.values\n",
        "\n",
        "\n",
        "\n",
        "def pad_and_normalize(data, scaler, sequence_length=SEQUENCE_LENGTH):\n",
        "    padded_data = pad_sequences(data, maxlen=sequence_length, dtype='float32', padding='post', truncating='post')\n",
        "    # normalized_data = scaler.transform(padded_data.reshape(-1, padded_data.shape[-1])).reshape(padded_data.shape)\n",
        "    return padded_data\n",
        "\n",
        "\n",
        "scaler_X = MinMaxScaler()\n",
        "scaler_y = MinMaxScaler()\n",
        "\n",
        "X_original = []\n",
        "y_original = []\n",
        "X_augmented = []\n",
        "y_augmented = []\n",
        "\n",
        "base_folder_path = '/content/'\n",
        "\n",
        "# CHANGED TO 6 FOR NEW DATA\n",
        "for i in range(6):\n",
        "  if i == 5 :\n",
        "    folder_path = os.path.join(base_folder_path, f'data_aug(3_slices_with_repeated)_cluster_{i}')\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.endswith('.csv'):\n",
        "            file_path = os.path.join(folder_path, filename)\n",
        "            features, target = process_file(file_path)\n",
        "\n",
        "            slices = filename.split('_')\n",
        "            is_original_trip = slices[2] == slices[6] and slices[6] == slices[10]\n",
        "\n",
        "            if is_original_trip:\n",
        "                X_original.append(features)\n",
        "                y_original.append(target)\n",
        "            else:\n",
        "                X_augmented.append(features)\n",
        "                y_augmented.append(target)\n",
        "\n",
        "# # Pad and convert lists to numpy arrays\n",
        "# # X_original = pad_sequences(X_original, maxlen=SEQUENCE_LENGTH, dtype='float32', padding='post', truncating='post')\n",
        "# # y_original = pad_sequences(y_original, maxlen=SEQUENCE_LENGTH, dtype='float32', padding='post', truncating='post')\n",
        "# X_augmented = pad_sequences(X_augmented, maxlen=SEQUENCE_LENGTH, dtype='float32', padding='post', truncating='post')\n",
        "# y_augmented = pad_sequences(y_augmented, maxlen=SEQUENCE_LENGTH, dtype='float32', padding='post', truncating='post')\n",
        "\n",
        "# num_test = int(0.05 * len(X_augmented))\n",
        "# X_test = X_augmented[:num_test]\n",
        "# y_test = y_augmented[:num_test]\n",
        "# X_train = X_augmented[num_test:]\n",
        "# y_train = y_augmented[num_test:]\n",
        "# Pad and convert lists to numpy arrays\n",
        "X_original = pad_sequences(X_original, maxlen=SEQUENCE_LENGTH, dtype='float32', padding='post', truncating='post')\n",
        "y_original = pad_sequences(y_original, maxlen=SEQUENCE_LENGTH, dtype='float32', padding='post', truncating='post')\n",
        "X_augmented = pad_sequences(X_augmented, maxlen=SEQUENCE_LENGTH, dtype='float32', padding='post', truncating='post')\n",
        "y_augmented = pad_sequences(y_augmented, maxlen=SEQUENCE_LENGTH, dtype='float32', padding='post', truncating='post')\n",
        "\n",
        "num_test = int(0.2 * len(X_original))\n",
        "X_test = X_original[:num_test]\n",
        "y_test = y_original[:num_test]\n",
        "X_train = np.concatenate([X_original[num_test:], X_augmented])\n",
        "y_train = np.concatenate([y_original[num_test:], y_augmented])\n",
        "\n",
        "# scaler_X.fit(X_train.reshape(-1, X_train.shape[-1]))\n",
        "# scaler_y.fit(y_train.reshape(-1, 1))\n",
        "\n",
        "# joblib.dump(scaler_X, 'scaler_X.pkl')\n",
        "# joblib.dump(scaler_y, 'scaler_y.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_original)"
      ],
      "metadata": {
        "id": "bB-tlRgrlguy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37c207b6-90a5-41b5-9c65-ac4e85a4d796"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[ 8.6200000e+02  0.0000000e+00  0.0000000e+00  0.0000000e+00]\n",
            "  [ 8.5150000e+02  0.0000000e+00  0.0000000e+00  0.0000000e+00]\n",
            "  [ 8.5975000e+02  0.0000000e+00  0.0000000e+00  0.0000000e+00]\n",
            "  ...\n",
            "  [ 2.5687500e+03  8.1000000e+01 -7.0588237e-01  1.0000000e+00]\n",
            "  [ 2.5735000e+03  8.2000000e+01 -7.0588237e-01  1.0000000e+00]\n",
            "  [ 2.5585000e+03  8.1000000e+01 -7.0588237e-01 -1.0000000e+00]]\n",
            "\n",
            " [[ 8.7700000e+02  0.0000000e+00 -2.4154589e+00  0.0000000e+00]\n",
            "  [ 9.0275000e+02  0.0000000e+00 -2.4154589e+00  0.0000000e+00]\n",
            "  [ 8.8925000e+02  0.0000000e+00 -2.4154589e+00  0.0000000e+00]\n",
            "  ...\n",
            "  [ 1.7845000e+03  2.3000000e+01 -2.7695351e+00 -1.0000000e+00]\n",
            "  [ 1.8482500e+03  2.5000000e+01 -2.7695351e+00  2.0000000e+00]\n",
            "  [ 1.9955000e+03  2.7000000e+01 -2.7695351e+00  2.0000000e+00]]\n",
            "\n",
            " [[ 2.6197500e+03  8.3000000e+01  4.5696878e-01  0.0000000e+00]\n",
            "  [ 2.6142500e+03  8.3000000e+01  4.5696878e-01  0.0000000e+00]\n",
            "  [ 2.6457500e+03  8.4000000e+01  4.5696878e-01  1.0000000e+00]\n",
            "  ...\n",
            "  [ 7.5225000e+02  0.0000000e+00  9.7087377e-01  0.0000000e+00]\n",
            "  [ 7.5725000e+02  0.0000000e+00  9.7087377e-01  0.0000000e+00]\n",
            "  [ 2.1082500e+03  0.0000000e+00  9.7087377e-01  0.0000000e+00]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[ 2.1455000e+03  4.7000000e+01  2.6292726e-01  0.0000000e+00]\n",
            "  [ 1.9967500e+03  4.4000000e+01  2.6292726e-01 -3.0000000e+00]\n",
            "  [ 1.8470000e+03  4.1000000e+01  2.6292726e-01 -3.0000000e+00]\n",
            "  ...\n",
            "  [ 2.6120000e+03  8.3000000e+01 -2.9965752e-01 -1.0000000e+00]\n",
            "  [ 2.6050000e+03  8.2000000e+01 -2.9965752e-01 -1.0000000e+00]\n",
            "  [ 2.5792500e+03  8.2000000e+01 -2.9965752e-01  0.0000000e+00]]\n",
            "\n",
            " [[ 2.0162500e+03  6.3000000e+01 -1.0240655e+00  0.0000000e+00]\n",
            "  [ 2.0197500e+03  6.4000000e+01 -1.0240655e+00  1.0000000e+00]\n",
            "  [ 2.0505000e+03  6.5000000e+01 -1.0240655e+00  1.0000000e+00]\n",
            "  ...\n",
            "  [ 2.0860000e+03  2.7000000e+01 -1.0526316e+00  0.0000000e+00]\n",
            "  [ 2.1512500e+03  2.9000000e+01 -1.0526316e+00  2.0000000e+00]\n",
            "  [ 2.1517500e+03  3.1000000e+01 -1.0526316e+00  2.0000000e+00]]\n",
            "\n",
            " [[ 2.1210000e+03  8.9000000e+01 -1.2336160e+00  0.0000000e+00]\n",
            "  [ 2.1345000e+03  9.0000000e+01 -1.2336160e+00  1.0000000e+00]\n",
            "  [ 2.1452500e+03  9.0000000e+01 -1.2336160e+00  0.0000000e+00]\n",
            "  ...\n",
            "  [ 2.1940000e+03  7.0000000e+01 -1.0240655e+00 -2.0000000e+00]\n",
            "  [ 2.0802500e+03  6.6000000e+01 -1.0240655e+00 -4.0000000e+00]\n",
            "  [ 1.9990000e+03  6.3000000e+01 -1.0240655e+00 -3.0000000e+00]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_augmented.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LvnIAhxqrqpF",
        "outputId": "272bd413-34e1-4a28-9f05-98f9a031357c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(20985, 600, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KIG0lqanf0UG",
        "outputId": "70917719-5fc9-4d69-cc46-d99796ad381d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train_normalized shape: (21009, 600, 4)\n",
            "y_train_normalized shape: (21009, 600)\n",
            "X_test_normalized shape: (5, 600, 4)\n",
            "y_test_normalized shape: (5, 600)\n"
          ]
        }
      ],
      "source": [
        "X_train_normalized = pad_and_normalize(X_train, scaler_X)\n",
        "y_train_normalized = y_train\n",
        "X_test_normalized = pad_and_normalize(X_test, scaler_X)\n",
        "y_test_normalized = y_test\n",
        "\n",
        "print('X_train_normalized shape:', X_train_normalized.shape)\n",
        "print('y_train_normalized shape:', y_train_normalized.shape)\n",
        "print('X_test_normalized shape:', X_test_normalized.shape)\n",
        "print('y_test_normalized shape:', y_test_normalized.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "H4x4L2ush3E5"
      },
      "outputs": [],
      "source": [
        "# model = Sequential([\n",
        "#     Bidirectional(LSTM(32, return_sequences=True, input_shape=(SEQUENCE_LENGTH, X_train_normalized.shape[-1]))),\n",
        "#     Dropout(0.2),\n",
        "#     Bidirectional(LSTM(32, return_sequences=True)),\n",
        "#     Dropout(0.2),\n",
        "#     Dense(1)\n",
        "# ])\n",
        "\n",
        "# model.compile(optimizer=Adam(learning_rate=LEARNING_RATE), loss='mean_squared_error')\n",
        "\n",
        "# model.fit(X_train_normalized, y_train_normalized, epochs=EPOCHS, batch_size=BATCH_SIZE, validation_split=0.2)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "model = Sequential([\n",
        "    Bidirectional(LSTM(64, return_sequences=True, input_shape=(SEQUENCE_LENGTH, X_train_normalized.shape[-1]))),\n",
        "    Dropout(0.2),\n",
        "    Bidirectional(LSTM(64, return_sequences=True)),\n",
        "    Dropout(0.2),\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "callback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
        "model.compile(optimizer=Adam(learning_rate=LEARNING_RATE), loss='mean_squared_error')\n",
        "\n",
        "history = model.fit(X_train_normalized, y_train_normalized, epochs=250, batch_size=BATCH_SIZE, validation_split=0.2,verbose=2,callbacks=[callback])\n",
        "\n",
        "len(history.history['loss'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jba7dPRgYrj0",
        "outputId": "cab1f49b-e89e-41ab-fc33-ba063b8ae406"
      },
      "execution_count": 10,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/250\n",
            "263/263 - 30s - 116ms/step - loss: 3450086.7500 - val_loss: 3442403.0000\n",
            "Epoch 2/250\n",
            "263/263 - 36s - 137ms/step - loss: 3424692.5000 - val_loss: 3429024.2500\n",
            "Epoch 3/250\n",
            "263/263 - 41s - 156ms/step - loss: 3413557.7500 - val_loss: 3419015.5000\n",
            "Epoch 4/250\n",
            "263/263 - 41s - 156ms/step - loss: 3404023.7500 - val_loss: 3409781.0000\n",
            "Epoch 5/250\n",
            "263/263 - 41s - 156ms/step - loss: 3395003.5000 - val_loss: 3400885.5000\n",
            "Epoch 6/250\n",
            "263/263 - 41s - 155ms/step - loss: 3386238.0000 - val_loss: 3392182.0000\n",
            "Epoch 7/250\n",
            "263/263 - 22s - 85ms/step - loss: 3377634.2500 - val_loss: 3383608.0000\n",
            "Epoch 8/250\n",
            "263/263 - 41s - 156ms/step - loss: 3369129.7500 - val_loss: 3375131.2500\n",
            "Epoch 9/250\n",
            "263/263 - 41s - 157ms/step - loss: 3360715.2500 - val_loss: 3366728.7500\n",
            "Epoch 10/250\n",
            "263/263 - 41s - 156ms/step - loss: 3352375.5000 - val_loss: 3358383.5000\n",
            "Epoch 11/250\n",
            "263/263 - 41s - 155ms/step - loss: 3344085.7500 - val_loss: 3350091.2500\n",
            "Epoch 12/250\n",
            "263/263 - 41s - 155ms/step - loss: 3335844.2500 - val_loss: 3341842.5000\n",
            "Epoch 13/250\n",
            "263/263 - 22s - 85ms/step - loss: 3327638.0000 - val_loss: 3333628.5000\n",
            "Epoch 14/250\n",
            "263/263 - 41s - 156ms/step - loss: 3319473.5000 - val_loss: 3325451.7500\n",
            "Epoch 15/250\n",
            "263/263 - 41s - 157ms/step - loss: 3311344.7500 - val_loss: 3317310.5000\n",
            "Epoch 16/250\n",
            "263/263 - 41s - 157ms/step - loss: 3303241.5000 - val_loss: 3309194.0000\n",
            "Epoch 17/250\n",
            "263/263 - 41s - 155ms/step - loss: 3295170.2500 - val_loss: 3301108.0000\n",
            "Epoch 18/250\n",
            "263/263 - 41s - 155ms/step - loss: 3287117.0000 - val_loss: 3293047.2500\n",
            "Epoch 19/250\n",
            "263/263 - 41s - 155ms/step - loss: 3279099.7500 - val_loss: 3285012.2500\n",
            "Epoch 20/250\n",
            "263/263 - 41s - 155ms/step - loss: 3271103.5000 - val_loss: 3276998.2500\n",
            "Epoch 21/250\n",
            "263/263 - 41s - 158ms/step - loss: 3263141.5000 - val_loss: 3269011.0000\n",
            "Epoch 22/250\n",
            "263/263 - 23s - 86ms/step - loss: 3255185.2500 - val_loss: 3261050.7500\n",
            "Epoch 23/250\n",
            "263/263 - 41s - 156ms/step - loss: 3247262.5000 - val_loss: 3253112.5000\n",
            "Epoch 24/250\n",
            "263/263 - 41s - 157ms/step - loss: 3239357.7500 - val_loss: 3245197.0000\n",
            "Epoch 25/250\n",
            "263/263 - 41s - 156ms/step - loss: 3231491.7500 - val_loss: 3237304.7500\n",
            "Epoch 26/250\n",
            "263/263 - 41s - 154ms/step - loss: 3223641.2500 - val_loss: 3229435.0000\n",
            "Epoch 27/250\n",
            "263/263 - 41s - 155ms/step - loss: 3215807.7500 - val_loss: 3221589.0000\n",
            "Epoch 28/250\n",
            "263/263 - 41s - 155ms/step - loss: 3208005.2500 - val_loss: 3213763.7500\n",
            "Epoch 29/250\n",
            "263/263 - 41s - 155ms/step - loss: 3200227.5000 - val_loss: 3205966.7500\n",
            "Epoch 30/250\n",
            "263/263 - 23s - 86ms/step - loss: 3192459.0000 - val_loss: 3198190.5000\n",
            "Epoch 31/250\n",
            "263/263 - 41s - 156ms/step - loss: 3184710.7500 - val_loss: 3190430.2500\n",
            "Epoch 32/250\n",
            "263/263 - 22s - 84ms/step - loss: 3176998.0000 - val_loss: 3182694.7500\n",
            "Epoch 33/250\n",
            "263/263 - 41s - 156ms/step - loss: 3169307.5000 - val_loss: 3174990.5000\n",
            "Epoch 34/250\n",
            "263/263 - 23s - 86ms/step - loss: 3161628.5000 - val_loss: 3167299.0000\n",
            "Epoch 35/250\n",
            "263/263 - 41s - 157ms/step - loss: 3153994.0000 - val_loss: 3159631.7500\n",
            "Epoch 36/250\n",
            "263/263 - 41s - 155ms/step - loss: 3146362.2500 - val_loss: 3151983.2500\n",
            "Epoch 37/250\n",
            "263/263 - 41s - 155ms/step - loss: 3138757.2500 - val_loss: 3144359.7500\n",
            "Epoch 38/250\n",
            "263/263 - 41s - 155ms/step - loss: 3131177.5000 - val_loss: 3136762.5000\n",
            "Epoch 39/250\n",
            "263/263 - 41s - 156ms/step - loss: 3123607.7500 - val_loss: 3129184.7500\n",
            "Epoch 40/250\n",
            "263/263 - 23s - 86ms/step - loss: 3116062.7500 - val_loss: 3121625.7500\n",
            "Epoch 41/250\n",
            "263/263 - 41s - 157ms/step - loss: 3108562.7500 - val_loss: 3114093.2500\n",
            "Epoch 42/250\n",
            "263/263 - 41s - 157ms/step - loss: 3101058.2500 - val_loss: 3106578.0000\n",
            "Epoch 43/250\n",
            "263/263 - 40s - 154ms/step - loss: 3093577.2500 - val_loss: 3099086.5000\n",
            "Epoch 44/250\n",
            "263/263 - 41s - 155ms/step - loss: 3086136.7500 - val_loss: 3091618.7500\n",
            "Epoch 45/250\n",
            "263/263 - 41s - 155ms/step - loss: 3078700.7500 - val_loss: 3084168.0000\n",
            "Epoch 46/250\n",
            "263/263 - 41s - 155ms/step - loss: 3071293.2500 - val_loss: 3076744.5000\n",
            "Epoch 47/250\n",
            "263/263 - 41s - 156ms/step - loss: 3063910.7500 - val_loss: 3069340.0000\n",
            "Epoch 48/250\n",
            "263/263 - 41s - 156ms/step - loss: 3056560.5000 - val_loss: 3061962.5000\n",
            "Epoch 49/250\n",
            "263/263 - 41s - 156ms/step - loss: 3049200.7500 - val_loss: 3054601.7500\n",
            "Epoch 50/250\n",
            "263/263 - 41s - 155ms/step - loss: 3041877.2500 - val_loss: 3047266.2500\n",
            "Epoch 51/250\n",
            "263/263 - 23s - 86ms/step - loss: 3034587.7500 - val_loss: 3039947.5000\n",
            "Epoch 52/250\n",
            "263/263 - 41s - 156ms/step - loss: 3027308.5000 - val_loss: 3032649.0000\n",
            "Epoch 53/250\n",
            "263/263 - 41s - 157ms/step - loss: 3020055.0000 - val_loss: 3025377.7500\n",
            "Epoch 54/250\n",
            "263/263 - 41s - 156ms/step - loss: 3012832.5000 - val_loss: 3018126.7500\n",
            "Epoch 55/250\n",
            "263/263 - 41s - 155ms/step - loss: 3005621.2500 - val_loss: 3010898.5000\n",
            "Epoch 56/250\n",
            "263/263 - 41s - 154ms/step - loss: 2998410.2500 - val_loss: 3003695.7500\n",
            "Epoch 57/250\n",
            "263/263 - 41s - 155ms/step - loss: 2991260.5000 - val_loss: 2996509.0000\n",
            "Epoch 58/250\n",
            "263/263 - 41s - 156ms/step - loss: 2984132.5000 - val_loss: 2989348.2500\n",
            "Epoch 59/250\n",
            "263/263 - 41s - 155ms/step - loss: 2976995.5000 - val_loss: 2982210.7500\n",
            "Epoch 60/250\n",
            "263/263 - 41s - 156ms/step - loss: 2969891.5000 - val_loss: 2975090.5000\n",
            "Epoch 61/250\n",
            "263/263 - 41s - 155ms/step - loss: 2962813.7500 - val_loss: 2967995.2500\n",
            "Epoch 62/250\n",
            "263/263 - 41s - 156ms/step - loss: 2955768.5000 - val_loss: 2960922.0000\n",
            "Epoch 63/250\n",
            "263/263 - 41s - 156ms/step - loss: 2948722.7500 - val_loss: 2953872.0000\n",
            "Epoch 64/250\n",
            "263/263 - 41s - 155ms/step - loss: 2941722.7500 - val_loss: 2946843.7500\n",
            "Epoch 65/250\n",
            "263/263 - 41s - 156ms/step - loss: 2934742.0000 - val_loss: 2939837.0000\n",
            "Epoch 66/250\n",
            "263/263 - 41s - 156ms/step - loss: 2927754.7500 - val_loss: 2932848.7500\n",
            "Epoch 67/250\n",
            "263/263 - 41s - 155ms/step - loss: 2920804.0000 - val_loss: 2925888.2500\n",
            "Epoch 68/250\n",
            "263/263 - 41s - 157ms/step - loss: 2913890.2500 - val_loss: 2918947.2500\n",
            "Epoch 69/250\n",
            "263/263 - 41s - 155ms/step - loss: 2906988.5000 - val_loss: 2912026.7500\n",
            "Epoch 70/250\n",
            "263/263 - 41s - 155ms/step - loss: 2900125.2500 - val_loss: 2905129.7500\n",
            "Epoch 71/250\n",
            "263/263 - 23s - 86ms/step - loss: 2893260.0000 - val_loss: 2898259.0000\n",
            "Epoch 72/250\n",
            "263/263 - 41s - 157ms/step - loss: 2886440.0000 - val_loss: 2891404.0000\n",
            "Epoch 73/250\n",
            "263/263 - 22s - 84ms/step - loss: 2879621.2500 - val_loss: 2884575.0000\n",
            "Epoch 74/250\n",
            "263/263 - 41s - 157ms/step - loss: 2872834.0000 - val_loss: 2877766.0000\n",
            "Epoch 75/250\n",
            "263/263 - 41s - 156ms/step - loss: 2866064.5000 - val_loss: 2870975.7500\n",
            "Epoch 76/250\n",
            "263/263 - 41s - 155ms/step - loss: 2859308.2500 - val_loss: 2864214.2500\n",
            "Epoch 77/250\n",
            "263/263 - 41s - 157ms/step - loss: 2852582.0000 - val_loss: 2857471.5000\n",
            "Epoch 78/250\n",
            "263/263 - 41s - 154ms/step - loss: 2845900.5000 - val_loss: 2850748.7500\n",
            "Epoch 79/250\n",
            "263/263 - 41s - 156ms/step - loss: 2839202.2500 - val_loss: 2844046.0000\n",
            "Epoch 80/250\n",
            "263/263 - 41s - 155ms/step - loss: 2832576.5000 - val_loss: 2837374.5000\n",
            "Epoch 81/250\n",
            "263/263 - 41s - 156ms/step - loss: 2825892.5000 - val_loss: 2830718.5000\n",
            "Epoch 82/250\n",
            "263/263 - 23s - 87ms/step - loss: 2819296.0000 - val_loss: 2824084.2500\n",
            "Epoch 83/250\n",
            "263/263 - 41s - 156ms/step - loss: 2812695.0000 - val_loss: 2817470.2500\n",
            "Epoch 84/250\n",
            "263/263 - 41s - 155ms/step - loss: 2806154.5000 - val_loss: 2810884.2500\n",
            "Epoch 85/250\n",
            "263/263 - 41s - 155ms/step - loss: 2799602.5000 - val_loss: 2804316.0000\n",
            "Epoch 86/250\n",
            "263/263 - 41s - 155ms/step - loss: 2793051.5000 - val_loss: 2797773.0000\n",
            "Epoch 87/250\n",
            "263/263 - 41s - 155ms/step - loss: 2786567.5000 - val_loss: 2791249.2500\n",
            "Epoch 88/250\n",
            "263/263 - 41s - 156ms/step - loss: 2780075.2500 - val_loss: 2784749.2500\n",
            "Epoch 89/250\n",
            "263/263 - 23s - 86ms/step - loss: 2773610.0000 - val_loss: 2778268.2500\n",
            "Epoch 90/250\n",
            "263/263 - 41s - 156ms/step - loss: 2767220.7500 - val_loss: 2771809.5000\n",
            "Epoch 91/250\n",
            "263/263 - 41s - 157ms/step - loss: 2760744.5000 - val_loss: 2765372.7500\n",
            "Epoch 92/250\n",
            "263/263 - 41s - 155ms/step - loss: 2754376.7500 - val_loss: 2758962.0000\n",
            "Epoch 93/250\n",
            "263/263 - 22s - 85ms/step - loss: 2747996.2500 - val_loss: 2752566.5000\n",
            "Epoch 94/250\n",
            "263/263 - 23s - 87ms/step - loss: 2741646.7500 - val_loss: 2746199.5000\n",
            "Epoch 95/250\n",
            "263/263 - 41s - 155ms/step - loss: 2735297.7500 - val_loss: 2739848.2500\n",
            "Epoch 96/250\n",
            "263/263 - 41s - 155ms/step - loss: 2729033.7500 - val_loss: 2733526.2500\n",
            "Epoch 97/250\n",
            "263/263 - 41s - 155ms/step - loss: 2722746.0000 - val_loss: 2727225.0000\n",
            "Epoch 98/250\n",
            "263/263 - 41s - 155ms/step - loss: 2716444.7500 - val_loss: 2720938.2500\n",
            "Epoch 99/250\n",
            "263/263 - 41s - 155ms/step - loss: 2710241.7500 - val_loss: 2714682.5000\n",
            "Epoch 100/250\n",
            "263/263 - 41s - 156ms/step - loss: 2704013.7500 - val_loss: 2708443.7500\n",
            "Epoch 101/250\n",
            "263/263 - 41s - 156ms/step - loss: 2697792.7500 - val_loss: 2702227.0000\n",
            "Epoch 102/250\n",
            "263/263 - 41s - 156ms/step - loss: 2691634.5000 - val_loss: 2696036.0000\n",
            "Epoch 103/250\n",
            "263/263 - 41s - 156ms/step - loss: 2685470.0000 - val_loss: 2689849.7500\n",
            "Epoch 104/250\n",
            "263/263 - 41s - 156ms/step - loss: 2679324.2500 - val_loss: 2683688.5000\n",
            "Epoch 105/250\n",
            "263/263 - 23s - 86ms/step - loss: 2673233.2500 - val_loss: 2677566.5000\n",
            "Epoch 106/250\n",
            "263/263 - 41s - 157ms/step - loss: 2667129.0000 - val_loss: 2671459.0000\n",
            "Epoch 107/250\n",
            "263/263 - 41s - 157ms/step - loss: 2661057.0000 - val_loss: 2665373.0000\n",
            "Epoch 108/250\n",
            "263/263 - 40s - 154ms/step - loss: 2655021.0000 - val_loss: 2659310.5000\n",
            "Epoch 109/250\n",
            "263/263 - 41s - 155ms/step - loss: 2649031.7500 - val_loss: 2653268.5000\n",
            "Epoch 110/250\n",
            "263/263 - 41s - 155ms/step - loss: 2643032.7500 - val_loss: 2647250.0000\n",
            "Epoch 111/250\n",
            "263/263 - 41s - 155ms/step - loss: 2637058.2500 - val_loss: 2641255.7500\n",
            "Epoch 112/250\n",
            "263/263 - 41s - 156ms/step - loss: 2631097.2500 - val_loss: 2635284.2500\n",
            "Epoch 113/250\n",
            "263/263 - 41s - 155ms/step - loss: 2625175.7500 - val_loss: 2629326.7500\n",
            "Epoch 114/250\n",
            "263/263 - 41s - 156ms/step - loss: 2619267.0000 - val_loss: 2623401.2500\n",
            "Epoch 115/250\n",
            "263/263 - 41s - 156ms/step - loss: 2613344.0000 - val_loss: 2617486.7500\n",
            "Epoch 116/250\n",
            "263/263 - 23s - 87ms/step - loss: 2607495.2500 - val_loss: 2611600.2500\n",
            "Epoch 117/250\n",
            "263/263 - 41s - 157ms/step - loss: 2601678.5000 - val_loss: 2605732.7500\n",
            "Epoch 118/250\n",
            "263/263 - 41s - 155ms/step - loss: 2595833.2500 - val_loss: 2599878.0000\n",
            "Epoch 119/250\n",
            "263/263 - 41s - 154ms/step - loss: 2589850.0000 - val_loss: 2593789.7500\n",
            "Epoch 120/250\n",
            "263/263 - 41s - 155ms/step - loss: 2583859.2500 - val_loss: 2587808.0000\n",
            "Epoch 121/250\n",
            "263/263 - 41s - 155ms/step - loss: 2577984.0000 - val_loss: 2581834.7500\n",
            "Epoch 122/250\n",
            "263/263 - 41s - 156ms/step - loss: 2572076.5000 - val_loss: 2575976.7500\n",
            "Epoch 123/250\n",
            "263/263 - 41s - 156ms/step - loss: 2566299.7500 - val_loss: 2570167.2500\n",
            "Epoch 124/250\n",
            "263/263 - 23s - 87ms/step - loss: 2560519.0000 - val_loss: 2564380.7500\n",
            "Epoch 125/250\n",
            "263/263 - 42s - 158ms/step - loss: 2554772.0000 - val_loss: 2558626.2500\n",
            "Epoch 126/250\n",
            "263/263 - 41s - 155ms/step - loss: 2549082.2500 - val_loss: 2552889.5000\n",
            "Epoch 127/250\n",
            "263/263 - 22s - 85ms/step - loss: 2543379.5000 - val_loss: 2547096.0000\n",
            "Epoch 128/250\n",
            "263/263 - 41s - 157ms/step - loss: 2537522.2500 - val_loss: 2541097.0000\n",
            "Epoch 129/250\n",
            "263/263 - 41s - 156ms/step - loss: 2531478.0000 - val_loss: 2534875.7500\n",
            "Epoch 130/250\n",
            "263/263 - 41s - 156ms/step - loss: 2525324.0000 - val_loss: 2528514.0000\n",
            "Epoch 131/250\n",
            "263/263 - 41s - 157ms/step - loss: 2518965.5000 - val_loss: 2522154.2500\n",
            "Epoch 132/250\n",
            "263/263 - 22s - 84ms/step - loss: 2512721.0000 - val_loss: 2515989.0000\n",
            "Epoch 133/250\n",
            "263/263 - 41s - 156ms/step - loss: 2506717.5000 - val_loss: 2510001.5000\n",
            "Epoch 134/250\n",
            "263/263 - 41s - 157ms/step - loss: 2500796.7500 - val_loss: 2504114.2500\n",
            "Epoch 135/250\n",
            "263/263 - 41s - 156ms/step - loss: 2494869.2500 - val_loss: 2498266.0000\n",
            "Epoch 136/250\n",
            "263/263 - 41s - 156ms/step - loss: 2489073.0000 - val_loss: 2492377.0000\n",
            "Epoch 137/250\n",
            "263/263 - 41s - 156ms/step - loss: 2483290.7500 - val_loss: 2486557.7500\n",
            "Epoch 138/250\n",
            "263/263 - 41s - 157ms/step - loss: 2477544.7500 - val_loss: 2480816.7500\n",
            "Epoch 139/250\n",
            "263/263 - 41s - 155ms/step - loss: 2471782.2500 - val_loss: 2475003.2500\n",
            "Epoch 140/250\n",
            "263/263 - 41s - 155ms/step - loss: 2466079.2500 - val_loss: 2469327.0000\n",
            "Epoch 141/250\n",
            "263/263 - 41s - 155ms/step - loss: 2460345.2500 - val_loss: 2463504.2500\n",
            "Epoch 142/250\n",
            "263/263 - 41s - 155ms/step - loss: 2454694.0000 - val_loss: 2457853.7500\n",
            "Epoch 143/250\n",
            "263/263 - 41s - 156ms/step - loss: 2449044.2500 - val_loss: 2452149.0000\n",
            "Epoch 144/250\n",
            "263/263 - 41s - 155ms/step - loss: 2443426.5000 - val_loss: 2446506.2500\n",
            "Epoch 145/250\n",
            "263/263 - 41s - 156ms/step - loss: 2437839.0000 - val_loss: 2440869.7500\n",
            "Epoch 146/250\n",
            "263/263 - 41s - 156ms/step - loss: 2432196.5000 - val_loss: 2435309.7500\n",
            "Epoch 147/250\n",
            "263/263 - 41s - 155ms/step - loss: 2426682.7500 - val_loss: 2429773.2500\n",
            "Epoch 148/250\n",
            "263/263 - 41s - 156ms/step - loss: 2421087.0000 - val_loss: 2424146.5000\n",
            "Epoch 149/250\n",
            "263/263 - 41s - 156ms/step - loss: 2415591.0000 - val_loss: 2418501.0000\n",
            "Epoch 150/250\n",
            "263/263 - 41s - 156ms/step - loss: 2410014.5000 - val_loss: 2412970.2500\n",
            "Epoch 151/250\n",
            "263/263 - 41s - 156ms/step - loss: 2404503.7500 - val_loss: 2407431.7500\n",
            "Epoch 152/250\n",
            "263/263 - 41s - 156ms/step - loss: 2399038.5000 - val_loss: 2401927.2500\n",
            "Epoch 153/250\n",
            "263/263 - 41s - 156ms/step - loss: 2393542.0000 - val_loss: 2396472.7500\n",
            "Epoch 154/250\n",
            "263/263 - 41s - 156ms/step - loss: 2388109.0000 - val_loss: 2390982.0000\n",
            "Epoch 155/250\n",
            "263/263 - 41s - 156ms/step - loss: 2382719.7500 - val_loss: 2385489.0000\n",
            "Epoch 156/250\n",
            "263/263 - 41s - 156ms/step - loss: 2377278.0000 - val_loss: 2380040.2500\n",
            "Epoch 157/250\n",
            "263/263 - 41s - 156ms/step - loss: 2371801.5000 - val_loss: 2374588.5000\n",
            "Epoch 158/250\n",
            "263/263 - 41s - 156ms/step - loss: 2366482.5000 - val_loss: 2369184.7500\n",
            "Epoch 159/250\n",
            "263/263 - 41s - 155ms/step - loss: 2361031.0000 - val_loss: 2363768.5000\n",
            "Epoch 160/250\n",
            "263/263 - 41s - 156ms/step - loss: 2355727.5000 - val_loss: 2358594.7500\n",
            "Epoch 161/250\n",
            "263/263 - 41s - 156ms/step - loss: 2350355.5000 - val_loss: 2353026.7500\n",
            "Epoch 162/250\n",
            "263/263 - 41s - 156ms/step - loss: 2345008.5000 - val_loss: 2347649.0000\n",
            "Epoch 163/250\n",
            "263/263 - 41s - 155ms/step - loss: 2339740.7500 - val_loss: 2342315.0000\n",
            "Epoch 164/250\n",
            "263/263 - 41s - 156ms/step - loss: 2334433.5000 - val_loss: 2336990.2500\n",
            "Epoch 165/250\n",
            "263/263 - 23s - 86ms/step - loss: 2329095.2500 - val_loss: 2331672.2500\n",
            "Epoch 166/250\n",
            "263/263 - 41s - 156ms/step - loss: 2323836.5000 - val_loss: 2326357.2500\n",
            "Epoch 167/250\n",
            "263/263 - 41s - 156ms/step - loss: 2318617.5000 - val_loss: 2321070.5000\n",
            "Epoch 168/250\n",
            "263/263 - 41s - 154ms/step - loss: 2313382.0000 - val_loss: 2315814.5000\n",
            "Epoch 169/250\n",
            "263/263 - 41s - 155ms/step - loss: 2308177.0000 - val_loss: 2310656.5000\n",
            "Epoch 170/250\n",
            "263/263 - 41s - 155ms/step - loss: 2302918.0000 - val_loss: 2305420.2500\n",
            "Epoch 171/250\n",
            "263/263 - 41s - 155ms/step - loss: 2297751.2500 - val_loss: 2300153.2500\n",
            "Epoch 172/250\n",
            "263/263 - 23s - 86ms/step - loss: 2292594.5000 - val_loss: 2294892.5000\n",
            "Epoch 173/250\n",
            "263/263 - 41s - 157ms/step - loss: 2287393.7500 - val_loss: 2289777.0000\n",
            "Epoch 174/250\n",
            "263/263 - 41s - 156ms/step - loss: 2282256.0000 - val_loss: 2284570.7500\n",
            "Epoch 175/250\n",
            "263/263 - 41s - 155ms/step - loss: 2277044.5000 - val_loss: 2279599.0000\n",
            "Epoch 176/250\n",
            "263/263 - 41s - 154ms/step - loss: 2271935.5000 - val_loss: 2274284.0000\n",
            "Epoch 177/250\n",
            "263/263 - 41s - 155ms/step - loss: 2266811.0000 - val_loss: 2269036.7500\n",
            "Epoch 178/250\n",
            "263/263 - 41s - 155ms/step - loss: 2261729.7500 - val_loss: 2263918.5000\n",
            "Epoch 179/250\n",
            "263/263 - 41s - 155ms/step - loss: 2256688.5000 - val_loss: 2258794.2500\n",
            "Epoch 180/250\n",
            "263/263 - 41s - 156ms/step - loss: 2251626.5000 - val_loss: 2253779.7500\n",
            "Epoch 181/250\n",
            "263/263 - 41s - 155ms/step - loss: 2246534.2500 - val_loss: 2248641.7500\n",
            "Epoch 182/250\n",
            "263/263 - 41s - 156ms/step - loss: 2241459.5000 - val_loss: 2243523.2500\n",
            "Epoch 183/250\n",
            "263/263 - 41s - 155ms/step - loss: 2236432.7500 - val_loss: 2238573.0000\n",
            "Epoch 184/250\n",
            "263/263 - 23s - 86ms/step - loss: 2231369.7500 - val_loss: 2233414.7500\n",
            "Epoch 185/250\n",
            "263/263 - 41s - 156ms/step - loss: 2226386.0000 - val_loss: 2228318.2500\n",
            "Epoch 186/250\n",
            "263/263 - 41s - 157ms/step - loss: 2221331.5000 - val_loss: 2223382.7500\n",
            "Epoch 187/250\n",
            "263/263 - 41s - 155ms/step - loss: 2216297.5000 - val_loss: 2218289.7500\n",
            "Epoch 188/250\n",
            "263/263 - 41s - 155ms/step - loss: 2211341.0000 - val_loss: 2213212.0000\n",
            "Epoch 189/250\n",
            "263/263 - 41s - 156ms/step - loss: 2206377.5000 - val_loss: 2208294.2500\n",
            "Epoch 190/250\n",
            "263/263 - 41s - 155ms/step - loss: 2201449.7500 - val_loss: 2203312.2500\n",
            "Epoch 191/250\n",
            "263/263 - 41s - 156ms/step - loss: 2196555.0000 - val_loss: 2198336.0000\n",
            "Epoch 192/250\n",
            "263/263 - 41s - 155ms/step - loss: 2191639.7500 - val_loss: 2193385.2500\n",
            "Epoch 193/250\n",
            "263/263 - 41s - 156ms/step - loss: 2186729.7500 - val_loss: 2188540.2500\n",
            "Epoch 194/250\n",
            "263/263 - 41s - 155ms/step - loss: 2181846.0000 - val_loss: 2183537.2500\n",
            "Epoch 195/250\n",
            "263/263 - 41s - 156ms/step - loss: 2176978.0000 - val_loss: 2178665.7500\n",
            "Epoch 196/250\n",
            "263/263 - 23s - 86ms/step - loss: 2172188.2500 - val_loss: 2173860.5000\n",
            "Epoch 197/250\n",
            "263/263 - 41s - 156ms/step - loss: 2167316.0000 - val_loss: 2168930.2500\n",
            "Epoch 198/250\n",
            "263/263 - 22s - 84ms/step - loss: 2162498.7500 - val_loss: 2164110.7500\n",
            "Epoch 199/250\n",
            "263/263 - 41s - 156ms/step - loss: 2157725.7500 - val_loss: 2159255.5000\n",
            "Epoch 200/250\n",
            "263/263 - 41s - 156ms/step - loss: 2152910.2500 - val_loss: 2154530.0000\n",
            "Epoch 201/250\n",
            "263/263 - 41s - 155ms/step - loss: 2148200.2500 - val_loss: 2149592.5000\n",
            "Epoch 202/250\n",
            "263/263 - 41s - 156ms/step - loss: 2143375.5000 - val_loss: 2144795.7500\n",
            "Epoch 203/250\n",
            "263/263 - 41s - 156ms/step - loss: 2138581.5000 - val_loss: 2140000.5000\n",
            "Epoch 204/250\n",
            "263/263 - 41s - 156ms/step - loss: 2133846.2500 - val_loss: 2135275.7500\n",
            "Epoch 205/250\n",
            "263/263 - 41s - 156ms/step - loss: 2129221.5000 - val_loss: 2130472.5000\n",
            "Epoch 206/250\n",
            "263/263 - 41s - 155ms/step - loss: 2124435.0000 - val_loss: 2125680.2500\n",
            "Epoch 207/250\n",
            "263/263 - 41s - 156ms/step - loss: 2119735.5000 - val_loss: 2120963.2500\n",
            "Epoch 208/250\n",
            "263/263 - 41s - 155ms/step - loss: 2114999.0000 - val_loss: 2116309.2500\n",
            "Epoch 209/250\n",
            "263/263 - 41s - 156ms/step - loss: 2110335.2500 - val_loss: 2111552.5000\n",
            "Epoch 210/250\n",
            "263/263 - 41s - 156ms/step - loss: 2105684.0000 - val_loss: 2106989.0000\n",
            "Epoch 211/250\n",
            "263/263 - 23s - 87ms/step - loss: 2101023.5000 - val_loss: 2102148.5000\n",
            "Epoch 212/250\n",
            "263/263 - 22s - 85ms/step - loss: 2096399.3750 - val_loss: 2097531.0000\n",
            "Epoch 213/250\n",
            "263/263 - 41s - 156ms/step - loss: 2091749.7500 - val_loss: 2092894.0000\n",
            "Epoch 214/250\n",
            "263/263 - 41s - 156ms/step - loss: 2087184.6250 - val_loss: 2088226.0000\n",
            "Epoch 215/250\n",
            "263/263 - 23s - 86ms/step - loss: 2082593.1250 - val_loss: 2083897.7500\n",
            "Epoch 216/250\n",
            "263/263 - 42s - 158ms/step - loss: 2077977.7500 - val_loss: 2079081.8750\n",
            "Epoch 217/250\n",
            "263/263 - 41s - 154ms/step - loss: 2073414.3750 - val_loss: 2074326.5000\n",
            "Epoch 218/250\n",
            "263/263 - 41s - 156ms/step - loss: 2068839.5000 - val_loss: 2069799.3750\n",
            "Epoch 219/250\n",
            "263/263 - 41s - 154ms/step - loss: 2064256.2500 - val_loss: 2065163.6250\n",
            "Epoch 220/250\n",
            "263/263 - 23s - 86ms/step - loss: 2059795.2500 - val_loss: 2060676.2500\n",
            "Epoch 221/250\n",
            "263/263 - 41s - 156ms/step - loss: 2055261.5000 - val_loss: 2056037.7500\n",
            "Epoch 222/250\n",
            "263/263 - 23s - 86ms/step - loss: 2050719.7500 - val_loss: 2051499.2500\n",
            "Epoch 223/250\n",
            "263/263 - 23s - 86ms/step - loss: 2046196.3750 - val_loss: 2047010.1250\n",
            "Epoch 224/250\n",
            "263/263 - 41s - 156ms/step - loss: 2041722.3750 - val_loss: 2042442.3750\n",
            "Epoch 225/250\n",
            "263/263 - 41s - 156ms/step - loss: 2037360.3750 - val_loss: 2037983.0000\n",
            "Epoch 226/250\n",
            "263/263 - 41s - 156ms/step - loss: 2032755.8750 - val_loss: 2033529.7500\n",
            "Epoch 227/250\n",
            "263/263 - 41s - 155ms/step - loss: 2028347.2500 - val_loss: 2029059.0000\n",
            "Epoch 228/250\n",
            "263/263 - 22s - 85ms/step - loss: 2023942.6250 - val_loss: 2024581.2500\n",
            "Epoch 229/250\n",
            "263/263 - 41s - 156ms/step - loss: 2019507.8750 - val_loss: 2020297.6250\n",
            "Epoch 230/250\n",
            "263/263 - 41s - 156ms/step - loss: 2015048.0000 - val_loss: 2015671.0000\n",
            "Epoch 231/250\n",
            "263/263 - 41s - 156ms/step - loss: 2010667.6250 - val_loss: 2011115.0000\n",
            "Epoch 232/250\n",
            "263/263 - 41s - 156ms/step - loss: 2006286.3750 - val_loss: 2006940.2500\n",
            "Epoch 233/250\n",
            "263/263 - 41s - 155ms/step - loss: 2001825.6250 - val_loss: 2002275.7500\n",
            "Epoch 234/250\n",
            "263/263 - 41s - 155ms/step - loss: 1997463.5000 - val_loss: 1997926.0000\n",
            "Epoch 235/250\n",
            "263/263 - 41s - 155ms/step - loss: 1993060.6250 - val_loss: 1993489.7500\n",
            "Epoch 236/250\n",
            "263/263 - 41s - 156ms/step - loss: 1988757.2500 - val_loss: 1989313.1250\n",
            "Epoch 237/250\n",
            "263/263 - 41s - 156ms/step - loss: 1984422.2500 - val_loss: 1984739.0000\n",
            "Epoch 238/250\n",
            "263/263 - 41s - 155ms/step - loss: 1980084.3750 - val_loss: 1980384.7500\n",
            "Epoch 239/250\n",
            "263/263 - 23s - 87ms/step - loss: 1975767.1250 - val_loss: 1976084.1250\n",
            "Epoch 240/250\n",
            "263/263 - 41s - 156ms/step - loss: 1971419.2500 - val_loss: 1971609.3750\n",
            "Epoch 241/250\n",
            "263/263 - 41s - 157ms/step - loss: 1967143.3750 - val_loss: 1967411.5000\n",
            "Epoch 242/250\n",
            "263/263 - 41s - 154ms/step - loss: 1962815.6250 - val_loss: 1962946.3750\n",
            "Epoch 243/250\n",
            "263/263 - 41s - 155ms/step - loss: 1958618.1250 - val_loss: 1958661.6250\n",
            "Epoch 244/250\n",
            "263/263 - 41s - 155ms/step - loss: 1954349.0000 - val_loss: 1954459.2500\n",
            "Epoch 245/250\n",
            "263/263 - 41s - 157ms/step - loss: 1950100.7500 - val_loss: 1950065.2500\n",
            "Epoch 246/250\n",
            "263/263 - 41s - 156ms/step - loss: 1945869.0000 - val_loss: 1945876.5000\n",
            "Epoch 247/250\n",
            "263/263 - 23s - 89ms/step - loss: 1941746.2500 - val_loss: 1941645.0000\n",
            "Epoch 248/250\n",
            "263/263 - 41s - 157ms/step - loss: 1937398.8750 - val_loss: 1937429.2500\n",
            "Epoch 249/250\n",
            "263/263 - 41s - 155ms/step - loss: 1933255.1250 - val_loss: 1933136.6250\n",
            "Epoch 250/250\n",
            "263/263 - 40s - 153ms/step - loss: 1929106.8750 - val_loss: 1928836.7500\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "250"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "jFsHBQY6t9z9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66d7dfee-c05c-4b28-8269-9420047aa57f"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ],
      "source": [
        "model.save('modelBLSTM_new_majid_added.h5')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "aNRrpf-HvXPd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "outputId": "197d9d06-c83f-4793-f469-cc528ca816c5"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-d5df0069828e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    131\u001b[0m   )\n\u001b[1;32m    132\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     if (\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ja80AJY5uGE2"
      },
      "outputs": [],
      "source": [
        "model.save('/content/drive/My Drive/model.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3tububWduTQ_"
      },
      "outputs": [],
      "source": [
        "test_loss = model.evaluate(X_test_normalized, y_test_normalized)\n",
        "print(f\"Test Loss: {test_loss}\")\n",
        "\n",
        "y_pred_normalized = model.predict(X_test_normalized)\n",
        "\n",
        "y_test_inv = y_test_normalized\n",
        "y_pred_inv = y_pred_normalized\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "92x8n4gYzyBC"
      },
      "outputs": [],
      "source": [
        "mse = mean_squared_error(y_test_inv, y_pred_inv)\n",
        "print(f\"Mean Squared Error: {mse}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-2e5qPNjz2bv"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "if not os.path.exists(PLOT_SAVE_DIR):\n",
        "    os.makedirs(PLOT_SAVE_DIR)\n",
        "\n",
        "for i in range(len(X_test)):\n",
        "    # Convert momentary to cumulative\n",
        "    cumulative_actuals = np.cumsum(y_test_inv[i], axis=0)  # assuming y_test_inv is already the inverse transformed data\n",
        "    cumulative_predictions = np.cumsum(y_pred_inv[i], axis=0)  # assuming y_pred_inv is already the inverse transformed predictions\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(cumulative_actuals, label='Actual Trip Fuel Consumption')\n",
        "    plt.plot(cumulative_predictions, label='Predicted Trip Fuel Consumption')\n",
        "    plt.title(f'Trip {i + 1}: Actual vs Predicted Fuel Consumption')\n",
        "    plt.xlabel('Time Steps')\n",
        "    plt.ylabel('Trip Fuel Consumption (uL)')\n",
        "    plt.legend()\n",
        "\n",
        "    plot_filename = os.path.join(PLOT_SAVE_DIR, f'trip_{i + 1}_actual_vs_predicted.png')\n",
        "    plt.savefig(plot_filename)\n",
        "    plt.close()  # Close the plot to save memory\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sHA95E7f6iZu"
      },
      "outputs": [],
      "source": [
        "!zip -r data.zip predicted_vs_actual_plots"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "\n",
        "# !cp \"/content/gdrive/My Drive/DataAugumentation.zip\" .\n",
        "#ADDED NEW SOURCE\n",
        "!cp \"/content/gdrive/My Drive/data_aug(3_slices_with_repeated)_acceleration_full_data_20000.zip\" .\n",
        "!unzip -qq DataAugumentation.zip\n",
        "!unzip -qq data_aug_3_slices_with_repeated_cluster_5.zip\n",
        "!rm DataAugumentation.zip\n",
        "!rm data_aug_3_slices_with_repeated_cluster_5.zip\n",
        "data_path = 'DataAugumentation'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X_RQgrvEbkfv",
        "outputId": "e37cd590-7064-48e1-efeb-528785b4641c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "unzip:  cannot find or open DataAugumentation.zip, DataAugumentation.zip.zip or DataAugumentation.zip.ZIP.\n",
            "unzip:  cannot find or open data_aug_3_slices_with_repeated_cluster_5.zip, data_aug_3_slices_with_repeated_cluster_5.zip.zip or data_aug_3_slices_with_repeated_cluster_5.zip.ZIP.\n",
            "rm: cannot remove 'DataAugumentation.zip': No such file or directory\n",
            "rm: cannot remove 'data_aug_3_slices_with_repeated_cluster_5.zip': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -qq '/content/data_aug(3_slices_with_repeated)_acceleration_full_data_20000.zip'"
      ],
      "metadata": {
        "id": "5xBVXOonbnUe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import joblib\n",
        "\n",
        "SEQUENCE_LENGTH = 600\n",
        "BATCH_SIZE = 45\n",
        "EPOCHS = 250\n",
        "LEARNING_RATE = 1e-5\n",
        "PLOT_SAVE_DIR = 'predicted_vs_actual_plots'\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(DEVICE)\n",
        "# Data processing function\n",
        "def process_file(file_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    df['Time'] = df['Time'] - df['Time'].iloc[0]\n",
        "\n",
        "    df['Momentary fuel consumption'] = df['Trip fuel consumption'].diff().fillna(0)\n",
        "    df['Acceleration'] = df['Speed'].diff().fillna(0)\n",
        "\n",
        "    features = df[['Engine speed', 'Speed', 'slope', 'Acceleration']]\n",
        "    target = df['Momentary fuel consumption']\n",
        "\n",
        "    features = features.iloc[:SEQUENCE_LENGTH]\n",
        "    target = target.iloc[:SEQUENCE_LENGTH]\n",
        "\n",
        "    return features.values, target.values\n",
        "\n",
        "scaler_X = MinMaxScaler()\n",
        "scaler_y = MinMaxScaler()\n",
        "\n",
        "X_original = []\n",
        "y_original = []\n",
        "X_augmented = []\n",
        "y_augmented = []\n",
        "\n",
        "base_folder_path = '/content/'\n",
        "\n",
        "# Process the data\n",
        "for i in range(6):\n",
        "    if i == 5:\n",
        "        folder_path = os.path.join(base_folder_path, f'data_aug(3_slices_with_repeated)_cluster_{i}')\n",
        "        for filename in os.listdir(folder_path):\n",
        "            if filename.endswith('.csv'):\n",
        "                file_path = os.path.join(folder_path, filename)\n",
        "                features, target = process_file(file_path)\n",
        "\n",
        "                slices = filename.split('_')\n",
        "                is_original_trip = slices[2] == slices[6] and slices[6] == slices[10]\n",
        "\n",
        "                if is_original_trip:\n",
        "                    X_original.append(features)\n",
        "                    y_original.append(target)\n",
        "                else:\n",
        "                    X_augmented.append(features)\n",
        "                    y_augmented.append(target)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dIRpL-EGbjhP",
        "outputId": "a3524fd9-8abd-4fac-ccd0-70b923085b60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Function to pad or truncate sequences\n",
        "def pad_or_truncate(sequence, length):\n",
        "    if len(sequence) > length:\n",
        "        return sequence[:length]\n",
        "    elif len(sequence) < length:\n",
        "        return np.pad(sequence, ((0, length - len(sequence)), (0, 0)), mode='constant')\n",
        "    else:\n",
        "        return sequence\n",
        "\n"
      ],
      "metadata": {
        "id": "btJOSO6edHGt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Apply padding/truncating to ensure consistent sequence length\n",
        "X_original = [pad_or_truncate(x, SEQUENCE_LENGTH) for x in X_original]\n",
        "y_original = [pad_or_truncate(y.reshape(-1, 1), SEQUENCE_LENGTH) for y in y_original]\n",
        "X_augmented = [pad_or_truncate(x, SEQUENCE_LENGTH) for x in X_augmented]\n",
        "y_augmented = [pad_or_truncate(y.reshape(-1, 1), SEQUENCE_LENGTH) for y in y_augmented]\n"
      ],
      "metadata": {
        "id": "nOGD-6Ibekzm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Convert data to PyTorch tensors\n",
        "try:\n",
        "    X_original = torch.tensor(np.array(X_original), dtype=torch.float32).to(DEVICE)\n",
        "    y_original = torch.tensor(np.array(y_original), dtype=torch.float32).to(DEVICE)\n",
        "    X_augmented = torch.tensor(np.array(X_augmented), dtype=torch.float32).to(DEVICE)\n",
        "    y_augmented = torch.tensor(np.array(y_augmented), dtype=torch.float32).to(DEVICE)\n",
        "except Exception as e:\n",
        "    print(f\"Error during tensor conversion: {e}\")\n",
        "    print(f\"Shapes: X_original - {np.array(X_original).shape}, y_original - {np.array(y_original).shape}\")\n",
        "    print(f\"Shapes: X_augmented - {np.array(X_augmented).shape}, y_augmented - {np.array(y_augmented).shape}\")\n",
        "    raise\n"
      ],
      "metadata": {
        "id": "eTGExYs0enuV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Split into training and test sets\n",
        "# num_test = int(0.2 * len(X_original))\n",
        "# X_test = X_original[:num_test]\n",
        "# y_test = y_original[:num_test]\n",
        "# X_train = torch.cat([X_original[num_test:], X_augmented])\n",
        "# y_train = torch.cat([y_original[num_test:], y_augmented])\n",
        "\n",
        "\n",
        "X_train = torch.cat([X_original, X_augmented])\n",
        "y_train = torch.cat([y_original, y_augmented])\n",
        "# X_train = X_original[num_test:]\n",
        "# y_train = y_original[num_test:]\n"
      ],
      "metadata": {
        "id": "DN9TrizAeqPk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Convert the min and max values to tensors\n",
        "min_val_x = torch.tensor([0, 0, -10, -10], dtype=torch.float32).to(DEVICE)\n",
        "max_val_x = torch.tensor([8000, 150, 10, 10], dtype=torch.float32).to(DEVICE)\n",
        "\n",
        "min_val_y = torch.tensor([0], dtype=torch.float32).to(DEVICE)\n",
        "max_val_y = torch.tensor([20000], dtype=torch.float32).to(DEVICE)\n",
        "\n",
        "# Custom normalization function for X\n",
        "def custom_normalize_X(data, min_vals, max_vals):\n",
        "    for i in range(data.shape[-1]):\n",
        "        data[:, :, i] = (data[:, :, i] - min_vals[i]) / (max_vals[i] - min_vals[i])\n",
        "    return data\n",
        "\n",
        "# Custom normalization function for y\n",
        "def custom_normalize_y(data, min_val, max_val):\n",
        "    return (data - min_val) / (max_val - min_val)\n",
        "\n",
        "# Normalize X_train and X_test\n",
        "X_train_normalized = custom_normalize_X(X_train, min_val_x, max_val_x)\n",
        "# X_test_normalized = custom_normalize_X(X_test, min_val_x, max_val_x)\n",
        "\n",
        "# Normalize y_train and y_test\n",
        "y_train_normalized = custom_normalize_y(y_train, min_val_y, max_val_y)\n",
        "# y_test_normalized = custom_normalize_y(y_test, min_val_y, max_val_y)\n"
      ],
      "metadata": {
        "id": "YSaA3VdOes7k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define the PyTorch model\n",
        "class FuelConsumptionModel(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(FuelConsumptionModel, self).__init__()\n",
        "        self.lstm1 = nn.LSTM(input_size,32, batch_first=True, bidirectional=True)\n",
        "        self.dropout1 = nn.Dropout(0.2)\n",
        "        self.lstm2 = nn.LSTM(64, 32, batch_first=True, bidirectional=True)\n",
        "        self.dropout2 = nn.Dropout(0.2)\n",
        "        self.dense = nn.Linear(64, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x, _ = self.lstm1(x)\n",
        "        x = self.dropout1(x)\n",
        "        x, _ = self.lstm2(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.dense(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "doWF6sefexia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Instantiate the model, define the loss function and the optimizer\n",
        "model = FuelConsumptionModel(input_size=X_train_normalized.shape[-1]).to(DEVICE)\n",
        "criterion = nn.L1Loss()\n",
        "weight_decay = 1e-4  # L2 regularization factor\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=weight_decay)"
      ],
      "metadata": {
        "id": "mRtldQG5e4-a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assume 80% of the data is used for training and 20% for validation\n",
        "train_size = int(0.8 * len(X_train_normalized))\n",
        "val_size = len(X_train_normalized) - train_size\n",
        "\n",
        "# Split the data while preserving the order\n",
        "X_train_split = X_train_normalized[:train_size]\n",
        "y_train_split = y_train_normalized[:train_size]\n",
        "\n",
        "X_val_split = X_train_normalized[train_size:]\n",
        "y_val_split = y_train_normalized[train_size:]\n",
        "\n",
        "# Create DataLoader for training and validation sets\n",
        "train_loader = DataLoader(TensorDataset(X_train_split, y_train_split), batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(TensorDataset(X_val_split, y_val_split), batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NrURTgcFgyYL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Assuming `FuelConsumptionModel`, `X_train_normalized`, `train_loader`, `val_loader`, `DEVICE`, and `EPOCHS` are defined\n",
        "\n",
        "# Define search space for loss functions and weight decay values\n",
        "loss_functions = {\n",
        "    'L1Loss': nn.L1Loss(),\n",
        "    'MSELoss': nn.MSELoss(),\n",
        "    'SmoothL1Loss': nn.SmoothL1Loss()\n",
        "}  # Add or change loss functions as needed\n",
        "\n",
        "weight_decay_values = [1e-4, 1e-5 , 5e-5 , 1e-6]  # Define your weight decay values\n",
        "\n",
        "# Initialize variables to track the best parameters\n",
        "best_loss_function = None\n",
        "best_weight_decay = None\n",
        "best_model_weights = None\n",
        "best_val_loss = float('inf')\n",
        "\n",
        "# Loop over all combinations of loss functions and weight decay values\n",
        "for loss_name, loss_func in loss_functions.items():\n",
        "    for wd in weight_decay_values:\n",
        "        # Initialize model, criterion, and optimizer for each combination\n",
        "        model = FuelConsumptionModel(input_size=X_train_normalized.shape[-1]).to(DEVICE)\n",
        "        criterion = loss_func\n",
        "        optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=wd)\n",
        "\n",
        "        # Early stopping parameters\n",
        "        patience = 10  # Number of epochs to wait before stopping if no improvement\n",
        "        best_loss = float('inf')  # Initialize best loss to infinity\n",
        "        epochs_without_improvement = 0  # Counter for epochs without improvement\n",
        "\n",
        "        # Training loop with early stopping\n",
        "        model.train()\n",
        "        for epoch in range(EPOCHS):\n",
        "            running_loss = 0.0\n",
        "            model.train()  # Ensure model is in training mode\n",
        "\n",
        "            # Training phase\n",
        "            for inputs, targets in train_loader:\n",
        "                inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)  # Ensure data is on the correct device\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, targets)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                running_loss += loss.item()\n",
        "\n",
        "            # Calculate average loss for the epoch\n",
        "            avg_training_loss = running_loss / len(train_loader)\n",
        "            # print(f\"Epoch [{epoch+1}/{EPOCHS}], Training Loss: {avg_training_loss:.4f}\")\n",
        "\n",
        "            # Validation phase\n",
        "            model.eval()  # Switch to evaluation mode\n",
        "            val_running_loss = 0.0\n",
        "            with torch.no_grad():\n",
        "                for inputs, targets in val_loader:  # Assume you have a validation DataLoader `val_loader`\n",
        "                    inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)  # Ensure data is on the correct device\n",
        "                    outputs = model(inputs)\n",
        "                    val_loss = criterion(outputs, targets)\n",
        "                    val_running_loss += val_loss.item()\n",
        "\n",
        "            avg_val_loss = val_running_loss / len(val_loader)\n",
        "            print(f\"Epoch [{epoch+1}/{EPOCHS}] ---- Training Loss: {avg_training_loss:.4f} ---- Validation Loss: {avg_val_loss:.4f}    {loss_name}  {wd:.0e}\")\n",
        "\n",
        "            # Early stopping check\n",
        "            if avg_val_loss < best_loss:\n",
        "                best_loss = avg_val_loss\n",
        "                epochs_without_improvement = 0\n",
        "            else:\n",
        "                epochs_without_improvement += 1\n",
        "                if epochs_without_improvement >= patience:\n",
        "                    print(f\"Early stopping at epoch {epoch+1}\")\n",
        "                    break\n",
        "\n",
        "        # Save the model for the current combination\n",
        "        model_filename = f'model_loss_{loss_name}_wd_{wd:.0e}.pth'\n",
        "        torch.save(model.state_dict(), model_filename)\n",
        "        print(f\"Saved model: {model_filename}\")\n",
        "\n",
        "        # Update best parameters based on validation loss\n",
        "        if best_loss < best_val_loss:\n",
        "            best_val_loss = best_loss\n",
        "            best_loss_function = loss_name\n",
        "            best_weight_decay = wd\n",
        "            best_model_weights = model.state_dict()  # Store the best model weights\n",
        "\n",
        "# After all combinations, print the best loss function and weight decay\n",
        "print(f\"Best Validation Loss: {best_val_loss:.4f}\")\n",
        "print(f\"Best Loss Function: {best_loss_function}\")\n",
        "print(f\"Best Weight Decay: {best_weight_decay}\")\n",
        "\n",
        "# Save the best model weights\n",
        "torch.save(best_model_weights, 'best_fuel_consumption_model_overall.pth')\n",
        "print(\"Best model saved as 'best_fuel_consumption_model_overall.pth'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_z0wpMf9gK-h",
        "outputId": "1ba09db4-3660-4e45-87fb-dea9d170d610"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/250] ---- Training Loss: 0.0687 ---- Validation Loss: 0.0418    L1Loss  1e-04\n",
            "Epoch [2/250] ---- Training Loss: 0.0474 ---- Validation Loss: 0.0405    L1Loss  1e-04\n",
            "Epoch [3/250] ---- Training Loss: 0.0459 ---- Validation Loss: 0.0391    L1Loss  1e-04\n",
            "Epoch [4/250] ---- Training Loss: 0.0444 ---- Validation Loss: 0.0378    L1Loss  1e-04\n",
            "Epoch [5/250] ---- Training Loss: 0.0430 ---- Validation Loss: 0.0368    L1Loss  1e-04\n",
            "Epoch [6/250] ---- Training Loss: 0.0419 ---- Validation Loss: 0.0362    L1Loss  1e-04\n",
            "Epoch [7/250] ---- Training Loss: 0.0408 ---- Validation Loss: 0.0357    L1Loss  1e-04\n",
            "Epoch [8/250] ---- Training Loss: 0.0398 ---- Validation Loss: 0.0353    L1Loss  1e-04\n",
            "Epoch [9/250] ---- Training Loss: 0.0387 ---- Validation Loss: 0.0349    L1Loss  1e-04\n",
            "Epoch [10/250] ---- Training Loss: 0.0378 ---- Validation Loss: 0.0345    L1Loss  1e-04\n",
            "Epoch [11/250] ---- Training Loss: 0.0370 ---- Validation Loss: 0.0341    L1Loss  1e-04\n",
            "Epoch [12/250] ---- Training Loss: 0.0362 ---- Validation Loss: 0.0338    L1Loss  1e-04\n",
            "Epoch [13/250] ---- Training Loss: 0.0356 ---- Validation Loss: 0.0335    L1Loss  1e-04\n",
            "Epoch [14/250] ---- Training Loss: 0.0351 ---- Validation Loss: 0.0332    L1Loss  1e-04\n",
            "Epoch [15/250] ---- Training Loss: 0.0347 ---- Validation Loss: 0.0330    L1Loss  1e-04\n",
            "Epoch [16/250] ---- Training Loss: 0.0343 ---- Validation Loss: 0.0328    L1Loss  1e-04\n",
            "Epoch [17/250] ---- Training Loss: 0.0340 ---- Validation Loss: 0.0326    L1Loss  1e-04\n",
            "Epoch [18/250] ---- Training Loss: 0.0337 ---- Validation Loss: 0.0325    L1Loss  1e-04\n",
            "Epoch [19/250] ---- Training Loss: 0.0334 ---- Validation Loss: 0.0323    L1Loss  1e-04\n",
            "Epoch [20/250] ---- Training Loss: 0.0332 ---- Validation Loss: 0.0322    L1Loss  1e-04\n",
            "Epoch [21/250] ---- Training Loss: 0.0330 ---- Validation Loss: 0.0320    L1Loss  1e-04\n",
            "Epoch [22/250] ---- Training Loss: 0.0328 ---- Validation Loss: 0.0319    L1Loss  1e-04\n",
            "Epoch [23/250] ---- Training Loss: 0.0326 ---- Validation Loss: 0.0318    L1Loss  1e-04\n",
            "Epoch [24/250] ---- Training Loss: 0.0325 ---- Validation Loss: 0.0316    L1Loss  1e-04\n",
            "Epoch [25/250] ---- Training Loss: 0.0323 ---- Validation Loss: 0.0315    L1Loss  1e-04\n",
            "Epoch [26/250] ---- Training Loss: 0.0322 ---- Validation Loss: 0.0314    L1Loss  1e-04\n",
            "Epoch [27/250] ---- Training Loss: 0.0320 ---- Validation Loss: 0.0312    L1Loss  1e-04\n",
            "Epoch [28/250] ---- Training Loss: 0.0319 ---- Validation Loss: 0.0311    L1Loss  1e-04\n",
            "Epoch [29/250] ---- Training Loss: 0.0317 ---- Validation Loss: 0.0310    L1Loss  1e-04\n",
            "Epoch [30/250] ---- Training Loss: 0.0316 ---- Validation Loss: 0.0308    L1Loss  1e-04\n",
            "Epoch [31/250] ---- Training Loss: 0.0315 ---- Validation Loss: 0.0307    L1Loss  1e-04\n",
            "Epoch [32/250] ---- Training Loss: 0.0313 ---- Validation Loss: 0.0306    L1Loss  1e-04\n",
            "Epoch [33/250] ---- Training Loss: 0.0312 ---- Validation Loss: 0.0305    L1Loss  1e-04\n",
            "Epoch [34/250] ---- Training Loss: 0.0310 ---- Validation Loss: 0.0303    L1Loss  1e-04\n",
            "Epoch [35/250] ---- Training Loss: 0.0309 ---- Validation Loss: 0.0302    L1Loss  1e-04\n",
            "Epoch [36/250] ---- Training Loss: 0.0308 ---- Validation Loss: 0.0301    L1Loss  1e-04\n",
            "Epoch [37/250] ---- Training Loss: 0.0306 ---- Validation Loss: 0.0299    L1Loss  1e-04\n",
            "Epoch [38/250] ---- Training Loss: 0.0305 ---- Validation Loss: 0.0298    L1Loss  1e-04\n",
            "Epoch [39/250] ---- Training Loss: 0.0304 ---- Validation Loss: 0.0296    L1Loss  1e-04\n",
            "Epoch [40/250] ---- Training Loss: 0.0302 ---- Validation Loss: 0.0295    L1Loss  1e-04\n",
            "Epoch [41/250] ---- Training Loss: 0.0301 ---- Validation Loss: 0.0294    L1Loss  1e-04\n",
            "Epoch [42/250] ---- Training Loss: 0.0300 ---- Validation Loss: 0.0293    L1Loss  1e-04\n",
            "Epoch [43/250] ---- Training Loss: 0.0299 ---- Validation Loss: 0.0291    L1Loss  1e-04\n",
            "Epoch [44/250] ---- Training Loss: 0.0298 ---- Validation Loss: 0.0290    L1Loss  1e-04\n",
            "Epoch [45/250] ---- Training Loss: 0.0296 ---- Validation Loss: 0.0289    L1Loss  1e-04\n",
            "Epoch [46/250] ---- Training Loss: 0.0295 ---- Validation Loss: 0.0288    L1Loss  1e-04\n",
            "Epoch [47/250] ---- Training Loss: 0.0294 ---- Validation Loss: 0.0287    L1Loss  1e-04\n",
            "Epoch [48/250] ---- Training Loss: 0.0293 ---- Validation Loss: 0.0286    L1Loss  1e-04\n",
            "Epoch [49/250] ---- Training Loss: 0.0292 ---- Validation Loss: 0.0284    L1Loss  1e-04\n",
            "Epoch [50/250] ---- Training Loss: 0.0291 ---- Validation Loss: 0.0283    L1Loss  1e-04\n",
            "Epoch [51/250] ---- Training Loss: 0.0290 ---- Validation Loss: 0.0282    L1Loss  1e-04\n",
            "Epoch [52/250] ---- Training Loss: 0.0289 ---- Validation Loss: 0.0281    L1Loss  1e-04\n",
            "Epoch [53/250] ---- Training Loss: 0.0287 ---- Validation Loss: 0.0279    L1Loss  1e-04\n",
            "Epoch [54/250] ---- Training Loss: 0.0286 ---- Validation Loss: 0.0278    L1Loss  1e-04\n",
            "Epoch [55/250] ---- Training Loss: 0.0285 ---- Validation Loss: 0.0276    L1Loss  1e-04\n",
            "Epoch [56/250] ---- Training Loss: 0.0284 ---- Validation Loss: 0.0275    L1Loss  1e-04\n",
            "Epoch [57/250] ---- Training Loss: 0.0282 ---- Validation Loss: 0.0273    L1Loss  1e-04\n",
            "Epoch [58/250] ---- Training Loss: 0.0281 ---- Validation Loss: 0.0272    L1Loss  1e-04\n",
            "Epoch [59/250] ---- Training Loss: 0.0279 ---- Validation Loss: 0.0270    L1Loss  1e-04\n",
            "Epoch [60/250] ---- Training Loss: 0.0278 ---- Validation Loss: 0.0268    L1Loss  1e-04\n",
            "Epoch [61/250] ---- Training Loss: 0.0276 ---- Validation Loss: 0.0266    L1Loss  1e-04\n",
            "Epoch [62/250] ---- Training Loss: 0.0275 ---- Validation Loss: 0.0264    L1Loss  1e-04\n",
            "Epoch [63/250] ---- Training Loss: 0.0273 ---- Validation Loss: 0.0262    L1Loss  1e-04\n",
            "Epoch [64/250] ---- Training Loss: 0.0271 ---- Validation Loss: 0.0261    L1Loss  1e-04\n",
            "Epoch [65/250] ---- Training Loss: 0.0270 ---- Validation Loss: 0.0259    L1Loss  1e-04\n",
            "Epoch [66/250] ---- Training Loss: 0.0268 ---- Validation Loss: 0.0257    L1Loss  1e-04\n",
            "Epoch [67/250] ---- Training Loss: 0.0267 ---- Validation Loss: 0.0255    L1Loss  1e-04\n",
            "Epoch [68/250] ---- Training Loss: 0.0265 ---- Validation Loss: 0.0254    L1Loss  1e-04\n",
            "Epoch [69/250] ---- Training Loss: 0.0264 ---- Validation Loss: 0.0252    L1Loss  1e-04\n",
            "Epoch [70/250] ---- Training Loss: 0.0263 ---- Validation Loss: 0.0251    L1Loss  1e-04\n",
            "Epoch [71/250] ---- Training Loss: 0.0261 ---- Validation Loss: 0.0250    L1Loss  1e-04\n",
            "Epoch [72/250] ---- Training Loss: 0.0260 ---- Validation Loss: 0.0248    L1Loss  1e-04\n",
            "Epoch [73/250] ---- Training Loss: 0.0259 ---- Validation Loss: 0.0247    L1Loss  1e-04\n",
            "Epoch [74/250] ---- Training Loss: 0.0258 ---- Validation Loss: 0.0246    L1Loss  1e-04\n",
            "Epoch [75/250] ---- Training Loss: 0.0257 ---- Validation Loss: 0.0246    L1Loss  1e-04\n",
            "Epoch [76/250] ---- Training Loss: 0.0256 ---- Validation Loss: 0.0245    L1Loss  1e-04\n",
            "Epoch [77/250] ---- Training Loss: 0.0255 ---- Validation Loss: 0.0244    L1Loss  1e-04\n",
            "Epoch [78/250] ---- Training Loss: 0.0255 ---- Validation Loss: 0.0243    L1Loss  1e-04\n",
            "Epoch [79/250] ---- Training Loss: 0.0254 ---- Validation Loss: 0.0242    L1Loss  1e-04\n",
            "Epoch [80/250] ---- Training Loss: 0.0253 ---- Validation Loss: 0.0242    L1Loss  1e-04\n",
            "Epoch [81/250] ---- Training Loss: 0.0252 ---- Validation Loss: 0.0241    L1Loss  1e-04\n",
            "Epoch [82/250] ---- Training Loss: 0.0252 ---- Validation Loss: 0.0240    L1Loss  1e-04\n",
            "Epoch [83/250] ---- Training Loss: 0.0251 ---- Validation Loss: 0.0240    L1Loss  1e-04\n",
            "Epoch [84/250] ---- Training Loss: 0.0250 ---- Validation Loss: 0.0239    L1Loss  1e-04\n",
            "Epoch [85/250] ---- Training Loss: 0.0250 ---- Validation Loss: 0.0239    L1Loss  1e-04\n",
            "Epoch [86/250] ---- Training Loss: 0.0249 ---- Validation Loss: 0.0238    L1Loss  1e-04\n",
            "Epoch [87/250] ---- Training Loss: 0.0249 ---- Validation Loss: 0.0238    L1Loss  1e-04\n",
            "Epoch [88/250] ---- Training Loss: 0.0248 ---- Validation Loss: 0.0237    L1Loss  1e-04\n",
            "Epoch [89/250] ---- Training Loss: 0.0248 ---- Validation Loss: 0.0237    L1Loss  1e-04\n",
            "Epoch [90/250] ---- Training Loss: 0.0247 ---- Validation Loss: 0.0236    L1Loss  1e-04\n",
            "Epoch [91/250] ---- Training Loss: 0.0247 ---- Validation Loss: 0.0236    L1Loss  1e-04\n",
            "Epoch [92/250] ---- Training Loss: 0.0246 ---- Validation Loss: 0.0235    L1Loss  1e-04\n",
            "Epoch [93/250] ---- Training Loss: 0.0246 ---- Validation Loss: 0.0235    L1Loss  1e-04\n",
            "Epoch [94/250] ---- Training Loss: 0.0245 ---- Validation Loss: 0.0235    L1Loss  1e-04\n",
            "Epoch [95/250] ---- Training Loss: 0.0245 ---- Validation Loss: 0.0234    L1Loss  1e-04\n",
            "Epoch [96/250] ---- Training Loss: 0.0244 ---- Validation Loss: 0.0234    L1Loss  1e-04\n",
            "Epoch [97/250] ---- Training Loss: 0.0244 ---- Validation Loss: 0.0233    L1Loss  1e-04\n",
            "Epoch [98/250] ---- Training Loss: 0.0243 ---- Validation Loss: 0.0233    L1Loss  1e-04\n",
            "Epoch [99/250] ---- Training Loss: 0.0243 ---- Validation Loss: 0.0232    L1Loss  1e-04\n",
            "Epoch [100/250] ---- Training Loss: 0.0243 ---- Validation Loss: 0.0232    L1Loss  1e-04\n",
            "Epoch [101/250] ---- Training Loss: 0.0242 ---- Validation Loss: 0.0232    L1Loss  1e-04\n",
            "Epoch [102/250] ---- Training Loss: 0.0242 ---- Validation Loss: 0.0231    L1Loss  1e-04\n",
            "Epoch [103/250] ---- Training Loss: 0.0241 ---- Validation Loss: 0.0231    L1Loss  1e-04\n",
            "Epoch [104/250] ---- Training Loss: 0.0241 ---- Validation Loss: 0.0231    L1Loss  1e-04\n",
            "Epoch [105/250] ---- Training Loss: 0.0241 ---- Validation Loss: 0.0230    L1Loss  1e-04\n",
            "Epoch [106/250] ---- Training Loss: 0.0240 ---- Validation Loss: 0.0230    L1Loss  1e-04\n",
            "Epoch [107/250] ---- Training Loss: 0.0240 ---- Validation Loss: 0.0229    L1Loss  1e-04\n",
            "Epoch [108/250] ---- Training Loss: 0.0240 ---- Validation Loss: 0.0229    L1Loss  1e-04\n",
            "Epoch [109/250] ---- Training Loss: 0.0239 ---- Validation Loss: 0.0229    L1Loss  1e-04\n",
            "Epoch [110/250] ---- Training Loss: 0.0239 ---- Validation Loss: 0.0228    L1Loss  1e-04\n",
            "Epoch [111/250] ---- Training Loss: 0.0238 ---- Validation Loss: 0.0228    L1Loss  1e-04\n",
            "Epoch [112/250] ---- Training Loss: 0.0238 ---- Validation Loss: 0.0228    L1Loss  1e-04\n",
            "Epoch [113/250] ---- Training Loss: 0.0238 ---- Validation Loss: 0.0227    L1Loss  1e-04\n",
            "Epoch [114/250] ---- Training Loss: 0.0237 ---- Validation Loss: 0.0227    L1Loss  1e-04\n",
            "Epoch [115/250] ---- Training Loss: 0.0237 ---- Validation Loss: 0.0227    L1Loss  1e-04\n",
            "Epoch [116/250] ---- Training Loss: 0.0237 ---- Validation Loss: 0.0227    L1Loss  1e-04\n",
            "Epoch [117/250] ---- Training Loss: 0.0237 ---- Validation Loss: 0.0226    L1Loss  1e-04\n",
            "Epoch [118/250] ---- Training Loss: 0.0236 ---- Validation Loss: 0.0226    L1Loss  1e-04\n",
            "Epoch [119/250] ---- Training Loss: 0.0236 ---- Validation Loss: 0.0226    L1Loss  1e-04\n",
            "Epoch [120/250] ---- Training Loss: 0.0236 ---- Validation Loss: 0.0225    L1Loss  1e-04\n",
            "Epoch [121/250] ---- Training Loss: 0.0235 ---- Validation Loss: 0.0225    L1Loss  1e-04\n",
            "Epoch [122/250] ---- Training Loss: 0.0235 ---- Validation Loss: 0.0225    L1Loss  1e-04\n",
            "Epoch [123/250] ---- Training Loss: 0.0235 ---- Validation Loss: 0.0224    L1Loss  1e-04\n",
            "Epoch [124/250] ---- Training Loss: 0.0234 ---- Validation Loss: 0.0224    L1Loss  1e-04\n",
            "Epoch [125/250] ---- Training Loss: 0.0234 ---- Validation Loss: 0.0224    L1Loss  1e-04\n",
            "Epoch [126/250] ---- Training Loss: 0.0234 ---- Validation Loss: 0.0224    L1Loss  1e-04\n",
            "Epoch [127/250] ---- Training Loss: 0.0234 ---- Validation Loss: 0.0223    L1Loss  1e-04\n",
            "Epoch [128/250] ---- Training Loss: 0.0233 ---- Validation Loss: 0.0223    L1Loss  1e-04\n",
            "Epoch [129/250] ---- Training Loss: 0.0233 ---- Validation Loss: 0.0223    L1Loss  1e-04\n",
            "Epoch [130/250] ---- Training Loss: 0.0233 ---- Validation Loss: 0.0223    L1Loss  1e-04\n",
            "Epoch [131/250] ---- Training Loss: 0.0233 ---- Validation Loss: 0.0222    L1Loss  1e-04\n",
            "Epoch [132/250] ---- Training Loss: 0.0232 ---- Validation Loss: 0.0222    L1Loss  1e-04\n",
            "Epoch [133/250] ---- Training Loss: 0.0232 ---- Validation Loss: 0.0222    L1Loss  1e-04\n",
            "Epoch [134/250] ---- Training Loss: 0.0232 ---- Validation Loss: 0.0222    L1Loss  1e-04\n",
            "Epoch [135/250] ---- Training Loss: 0.0232 ---- Validation Loss: 0.0221    L1Loss  1e-04\n",
            "Epoch [136/250] ---- Training Loss: 0.0231 ---- Validation Loss: 0.0221    L1Loss  1e-04\n",
            "Epoch [137/250] ---- Training Loss: 0.0231 ---- Validation Loss: 0.0221    L1Loss  1e-04\n",
            "Epoch [138/250] ---- Training Loss: 0.0231 ---- Validation Loss: 0.0221    L1Loss  1e-04\n",
            "Epoch [139/250] ---- Training Loss: 0.0231 ---- Validation Loss: 0.0220    L1Loss  1e-04\n",
            "Epoch [140/250] ---- Training Loss: 0.0231 ---- Validation Loss: 0.0220    L1Loss  1e-04\n",
            "Epoch [141/250] ---- Training Loss: 0.0230 ---- Validation Loss: 0.0220    L1Loss  1e-04\n",
            "Epoch [142/250] ---- Training Loss: 0.0230 ---- Validation Loss: 0.0220    L1Loss  1e-04\n",
            "Epoch [143/250] ---- Training Loss: 0.0230 ---- Validation Loss: 0.0219    L1Loss  1e-04\n",
            "Epoch [144/250] ---- Training Loss: 0.0230 ---- Validation Loss: 0.0219    L1Loss  1e-04\n",
            "Epoch [145/250] ---- Training Loss: 0.0229 ---- Validation Loss: 0.0219    L1Loss  1e-04\n",
            "Epoch [146/250] ---- Training Loss: 0.0229 ---- Validation Loss: 0.0219    L1Loss  1e-04\n",
            "Epoch [147/250] ---- Training Loss: 0.0229 ---- Validation Loss: 0.0219    L1Loss  1e-04\n",
            "Epoch [148/250] ---- Training Loss: 0.0229 ---- Validation Loss: 0.0218    L1Loss  1e-04\n",
            "Epoch [149/250] ---- Training Loss: 0.0229 ---- Validation Loss: 0.0218    L1Loss  1e-04\n",
            "Epoch [150/250] ---- Training Loss: 0.0228 ---- Validation Loss: 0.0218    L1Loss  1e-04\n",
            "Epoch [151/250] ---- Training Loss: 0.0228 ---- Validation Loss: 0.0218    L1Loss  1e-04\n",
            "Epoch [152/250] ---- Training Loss: 0.0228 ---- Validation Loss: 0.0218    L1Loss  1e-04\n",
            "Epoch [153/250] ---- Training Loss: 0.0228 ---- Validation Loss: 0.0217    L1Loss  1e-04\n",
            "Epoch [154/250] ---- Training Loss: 0.0228 ---- Validation Loss: 0.0217    L1Loss  1e-04\n",
            "Epoch [155/250] ---- Training Loss: 0.0228 ---- Validation Loss: 0.0217    L1Loss  1e-04\n",
            "Epoch [156/250] ---- Training Loss: 0.0227 ---- Validation Loss: 0.0217    L1Loss  1e-04\n",
            "Epoch [157/250] ---- Training Loss: 0.0227 ---- Validation Loss: 0.0217    L1Loss  1e-04\n",
            "Epoch [158/250] ---- Training Loss: 0.0227 ---- Validation Loss: 0.0216    L1Loss  1e-04\n",
            "Epoch [159/250] ---- Training Loss: 0.0227 ---- Validation Loss: 0.0216    L1Loss  1e-04\n",
            "Epoch [160/250] ---- Training Loss: 0.0227 ---- Validation Loss: 0.0216    L1Loss  1e-04\n",
            "Epoch [161/250] ---- Training Loss: 0.0227 ---- Validation Loss: 0.0216    L1Loss  1e-04\n",
            "Epoch [162/250] ---- Training Loss: 0.0226 ---- Validation Loss: 0.0216    L1Loss  1e-04\n",
            "Epoch [163/250] ---- Training Loss: 0.0226 ---- Validation Loss: 0.0216    L1Loss  1e-04\n",
            "Epoch [164/250] ---- Training Loss: 0.0226 ---- Validation Loss: 0.0215    L1Loss  1e-04\n",
            "Epoch [165/250] ---- Training Loss: 0.0226 ---- Validation Loss: 0.0215    L1Loss  1e-04\n",
            "Epoch [166/250] ---- Training Loss: 0.0226 ---- Validation Loss: 0.0215    L1Loss  1e-04\n",
            "Epoch [167/250] ---- Training Loss: 0.0226 ---- Validation Loss: 0.0215    L1Loss  1e-04\n",
            "Epoch [168/250] ---- Training Loss: 0.0225 ---- Validation Loss: 0.0215    L1Loss  1e-04\n",
            "Epoch [169/250] ---- Training Loss: 0.0225 ---- Validation Loss: 0.0214    L1Loss  1e-04\n",
            "Epoch [170/250] ---- Training Loss: 0.0225 ---- Validation Loss: 0.0214    L1Loss  1e-04\n",
            "Epoch [171/250] ---- Training Loss: 0.0225 ---- Validation Loss: 0.0214    L1Loss  1e-04\n",
            "Epoch [172/250] ---- Training Loss: 0.0225 ---- Validation Loss: 0.0214    L1Loss  1e-04\n",
            "Epoch [173/250] ---- Training Loss: 0.0225 ---- Validation Loss: 0.0214    L1Loss  1e-04\n",
            "Epoch [174/250] ---- Training Loss: 0.0224 ---- Validation Loss: 0.0214    L1Loss  1e-04\n",
            "Epoch [175/250] ---- Training Loss: 0.0224 ---- Validation Loss: 0.0214    L1Loss  1e-04\n",
            "Epoch [176/250] ---- Training Loss: 0.0224 ---- Validation Loss: 0.0213    L1Loss  1e-04\n",
            "Epoch [177/250] ---- Training Loss: 0.0224 ---- Validation Loss: 0.0213    L1Loss  1e-04\n",
            "Epoch [178/250] ---- Training Loss: 0.0224 ---- Validation Loss: 0.0213    L1Loss  1e-04\n",
            "Epoch [179/250] ---- Training Loss: 0.0224 ---- Validation Loss: 0.0213    L1Loss  1e-04\n",
            "Epoch [180/250] ---- Training Loss: 0.0224 ---- Validation Loss: 0.0213    L1Loss  1e-04\n",
            "Epoch [181/250] ---- Training Loss: 0.0223 ---- Validation Loss: 0.0213    L1Loss  1e-04\n",
            "Epoch [182/250] ---- Training Loss: 0.0223 ---- Validation Loss: 0.0212    L1Loss  1e-04\n",
            "Epoch [183/250] ---- Training Loss: 0.0223 ---- Validation Loss: 0.0212    L1Loss  1e-04\n",
            "Epoch [184/250] ---- Training Loss: 0.0223 ---- Validation Loss: 0.0212    L1Loss  1e-04\n",
            "Epoch [185/250] ---- Training Loss: 0.0223 ---- Validation Loss: 0.0212    L1Loss  1e-04\n",
            "Epoch [186/250] ---- Training Loss: 0.0223 ---- Validation Loss: 0.0212    L1Loss  1e-04\n",
            "Epoch [187/250] ---- Training Loss: 0.0223 ---- Validation Loss: 0.0211    L1Loss  1e-04\n",
            "Epoch [188/250] ---- Training Loss: 0.0222 ---- Validation Loss: 0.0211    L1Loss  1e-04\n",
            "Epoch [189/250] ---- Training Loss: 0.0222 ---- Validation Loss: 0.0211    L1Loss  1e-04\n",
            "Epoch [190/250] ---- Training Loss: 0.0222 ---- Validation Loss: 0.0211    L1Loss  1e-04\n",
            "Epoch [191/250] ---- Training Loss: 0.0222 ---- Validation Loss: 0.0211    L1Loss  1e-04\n",
            "Epoch [192/250] ---- Training Loss: 0.0222 ---- Validation Loss: 0.0211    L1Loss  1e-04\n",
            "Epoch [193/250] ---- Training Loss: 0.0222 ---- Validation Loss: 0.0210    L1Loss  1e-04\n",
            "Epoch [194/250] ---- Training Loss: 0.0222 ---- Validation Loss: 0.0210    L1Loss  1e-04\n",
            "Epoch [195/250] ---- Training Loss: 0.0221 ---- Validation Loss: 0.0210    L1Loss  1e-04\n",
            "Epoch [196/250] ---- Training Loss: 0.0221 ---- Validation Loss: 0.0210    L1Loss  1e-04\n",
            "Epoch [197/250] ---- Training Loss: 0.0221 ---- Validation Loss: 0.0210    L1Loss  1e-04\n",
            "Epoch [198/250] ---- Training Loss: 0.0221 ---- Validation Loss: 0.0210    L1Loss  1e-04\n",
            "Epoch [199/250] ---- Training Loss: 0.0221 ---- Validation Loss: 0.0209    L1Loss  1e-04\n",
            "Epoch [200/250] ---- Training Loss: 0.0221 ---- Validation Loss: 0.0209    L1Loss  1e-04\n",
            "Epoch [201/250] ---- Training Loss: 0.0220 ---- Validation Loss: 0.0209    L1Loss  1e-04\n",
            "Epoch [202/250] ---- Training Loss: 0.0220 ---- Validation Loss: 0.0209    L1Loss  1e-04\n",
            "Epoch [203/250] ---- Training Loss: 0.0220 ---- Validation Loss: 0.0209    L1Loss  1e-04\n",
            "Epoch [204/250] ---- Training Loss: 0.0220 ---- Validation Loss: 0.0208    L1Loss  1e-04\n",
            "Epoch [205/250] ---- Training Loss: 0.0220 ---- Validation Loss: 0.0208    L1Loss  1e-04\n",
            "Epoch [206/250] ---- Training Loss: 0.0220 ---- Validation Loss: 0.0208    L1Loss  1e-04\n",
            "Epoch [207/250] ---- Training Loss: 0.0219 ---- Validation Loss: 0.0208    L1Loss  1e-04\n",
            "Epoch [208/250] ---- Training Loss: 0.0219 ---- Validation Loss: 0.0208    L1Loss  1e-04\n",
            "Epoch [209/250] ---- Training Loss: 0.0219 ---- Validation Loss: 0.0207    L1Loss  1e-04\n",
            "Epoch [210/250] ---- Training Loss: 0.0219 ---- Validation Loss: 0.0207    L1Loss  1e-04\n",
            "Epoch [211/250] ---- Training Loss: 0.0219 ---- Validation Loss: 0.0207    L1Loss  1e-04\n",
            "Epoch [212/250] ---- Training Loss: 0.0218 ---- Validation Loss: 0.0207    L1Loss  1e-04\n",
            "Epoch [213/250] ---- Training Loss: 0.0218 ---- Validation Loss: 0.0207    L1Loss  1e-04\n",
            "Epoch [214/250] ---- Training Loss: 0.0218 ---- Validation Loss: 0.0206    L1Loss  1e-04\n",
            "Epoch [215/250] ---- Training Loss: 0.0218 ---- Validation Loss: 0.0206    L1Loss  1e-04\n",
            "Epoch [216/250] ---- Training Loss: 0.0218 ---- Validation Loss: 0.0206    L1Loss  1e-04\n",
            "Epoch [217/250] ---- Training Loss: 0.0217 ---- Validation Loss: 0.0206    L1Loss  1e-04\n",
            "Epoch [218/250] ---- Training Loss: 0.0217 ---- Validation Loss: 0.0205    L1Loss  1e-04\n",
            "Epoch [219/250] ---- Training Loss: 0.0217 ---- Validation Loss: 0.0205    L1Loss  1e-04\n",
            "Epoch [220/250] ---- Training Loss: 0.0217 ---- Validation Loss: 0.0205    L1Loss  1e-04\n",
            "Epoch [221/250] ---- Training Loss: 0.0216 ---- Validation Loss: 0.0205    L1Loss  1e-04\n",
            "Epoch [222/250] ---- Training Loss: 0.0216 ---- Validation Loss: 0.0205    L1Loss  1e-04\n",
            "Epoch [223/250] ---- Training Loss: 0.0216 ---- Validation Loss: 0.0204    L1Loss  1e-04\n",
            "Epoch [224/250] ---- Training Loss: 0.0216 ---- Validation Loss: 0.0204    L1Loss  1e-04\n",
            "Epoch [225/250] ---- Training Loss: 0.0216 ---- Validation Loss: 0.0204    L1Loss  1e-04\n",
            "Epoch [226/250] ---- Training Loss: 0.0215 ---- Validation Loss: 0.0204    L1Loss  1e-04\n",
            "Epoch [227/250] ---- Training Loss: 0.0215 ---- Validation Loss: 0.0203    L1Loss  1e-04\n",
            "Epoch [228/250] ---- Training Loss: 0.0215 ---- Validation Loss: 0.0203    L1Loss  1e-04\n",
            "Epoch [229/250] ---- Training Loss: 0.0215 ---- Validation Loss: 0.0203    L1Loss  1e-04\n",
            "Epoch [230/250] ---- Training Loss: 0.0215 ---- Validation Loss: 0.0203    L1Loss  1e-04\n",
            "Epoch [231/250] ---- Training Loss: 0.0215 ---- Validation Loss: 0.0202    L1Loss  1e-04\n",
            "Epoch [232/250] ---- Training Loss: 0.0214 ---- Validation Loss: 0.0202    L1Loss  1e-04\n",
            "Epoch [233/250] ---- Training Loss: 0.0214 ---- Validation Loss: 0.0202    L1Loss  1e-04\n",
            "Epoch [234/250] ---- Training Loss: 0.0214 ---- Validation Loss: 0.0202    L1Loss  1e-04\n",
            "Epoch [235/250] ---- Training Loss: 0.0214 ---- Validation Loss: 0.0201    L1Loss  1e-04\n",
            "Epoch [236/250] ---- Training Loss: 0.0213 ---- Validation Loss: 0.0201    L1Loss  1e-04\n",
            "Epoch [237/250] ---- Training Loss: 0.0213 ---- Validation Loss: 0.0201    L1Loss  1e-04\n",
            "Epoch [238/250] ---- Training Loss: 0.0213 ---- Validation Loss: 0.0201    L1Loss  1e-04\n",
            "Epoch [239/250] ---- Training Loss: 0.0213 ---- Validation Loss: 0.0200    L1Loss  1e-04\n",
            "Epoch [240/250] ---- Training Loss: 0.0212 ---- Validation Loss: 0.0200    L1Loss  1e-04\n",
            "Epoch [241/250] ---- Training Loss: 0.0212 ---- Validation Loss: 0.0200    L1Loss  1e-04\n",
            "Epoch [242/250] ---- Training Loss: 0.0212 ---- Validation Loss: 0.0200    L1Loss  1e-04\n",
            "Epoch [243/250] ---- Training Loss: 0.0212 ---- Validation Loss: 0.0199    L1Loss  1e-04\n",
            "Epoch [244/250] ---- Training Loss: 0.0211 ---- Validation Loss: 0.0199    L1Loss  1e-04\n",
            "Epoch [245/250] ---- Training Loss: 0.0211 ---- Validation Loss: 0.0199    L1Loss  1e-04\n",
            "Epoch [246/250] ---- Training Loss: 0.0211 ---- Validation Loss: 0.0198    L1Loss  1e-04\n",
            "Epoch [247/250] ---- Training Loss: 0.0211 ---- Validation Loss: 0.0198    L1Loss  1e-04\n",
            "Epoch [248/250] ---- Training Loss: 0.0210 ---- Validation Loss: 0.0198    L1Loss  1e-04\n",
            "Epoch [249/250] ---- Training Loss: 0.0210 ---- Validation Loss: 0.0198    L1Loss  1e-04\n",
            "Epoch [250/250] ---- Training Loss: 0.0210 ---- Validation Loss: 0.0197    L1Loss  1e-04\n",
            "Saved model: model_loss_L1Loss_wd_1e-04.pth\n",
            "Epoch [1/250] ---- Training Loss: 0.0444 ---- Validation Loss: 0.0398    L1Loss  1e-05\n",
            "Epoch [2/250] ---- Training Loss: 0.0417 ---- Validation Loss: 0.0376    L1Loss  1e-05\n",
            "Epoch [3/250] ---- Training Loss: 0.0398 ---- Validation Loss: 0.0363    L1Loss  1e-05\n",
            "Epoch [4/250] ---- Training Loss: 0.0384 ---- Validation Loss: 0.0355    L1Loss  1e-05\n",
            "Epoch [5/250] ---- Training Loss: 0.0372 ---- Validation Loss: 0.0348    L1Loss  1e-05\n",
            "Epoch [6/250] ---- Training Loss: 0.0362 ---- Validation Loss: 0.0342    L1Loss  1e-05\n",
            "Epoch [7/250] ---- Training Loss: 0.0354 ---- Validation Loss: 0.0336    L1Loss  1e-05\n",
            "Epoch [8/250] ---- Training Loss: 0.0348 ---- Validation Loss: 0.0333    L1Loss  1e-05\n",
            "Epoch [9/250] ---- Training Loss: 0.0344 ---- Validation Loss: 0.0330    L1Loss  1e-05\n",
            "Epoch [10/250] ---- Training Loss: 0.0340 ---- Validation Loss: 0.0327    L1Loss  1e-05\n",
            "Epoch [11/250] ---- Training Loss: 0.0337 ---- Validation Loss: 0.0325    L1Loss  1e-05\n",
            "Epoch [12/250] ---- Training Loss: 0.0334 ---- Validation Loss: 0.0323    L1Loss  1e-05\n",
            "Epoch [13/250] ---- Training Loss: 0.0331 ---- Validation Loss: 0.0321    L1Loss  1e-05\n",
            "Epoch [14/250] ---- Training Loss: 0.0329 ---- Validation Loss: 0.0319    L1Loss  1e-05\n",
            "Epoch [15/250] ---- Training Loss: 0.0326 ---- Validation Loss: 0.0317    L1Loss  1e-05\n",
            "Epoch [16/250] ---- Training Loss: 0.0324 ---- Validation Loss: 0.0314    L1Loss  1e-05\n",
            "Epoch [17/250] ---- Training Loss: 0.0321 ---- Validation Loss: 0.0312    L1Loss  1e-05\n",
            "Epoch [18/250] ---- Training Loss: 0.0319 ---- Validation Loss: 0.0310    L1Loss  1e-05\n",
            "Epoch [19/250] ---- Training Loss: 0.0316 ---- Validation Loss: 0.0307    L1Loss  1e-05\n",
            "Epoch [20/250] ---- Training Loss: 0.0314 ---- Validation Loss: 0.0305    L1Loss  1e-05\n",
            "Epoch [21/250] ---- Training Loss: 0.0312 ---- Validation Loss: 0.0302    L1Loss  1e-05\n",
            "Epoch [22/250] ---- Training Loss: 0.0309 ---- Validation Loss: 0.0300    L1Loss  1e-05\n",
            "Epoch [23/250] ---- Training Loss: 0.0307 ---- Validation Loss: 0.0297    L1Loss  1e-05\n",
            "Epoch [24/250] ---- Training Loss: 0.0304 ---- Validation Loss: 0.0295    L1Loss  1e-05\n",
            "Epoch [25/250] ---- Training Loss: 0.0302 ---- Validation Loss: 0.0292    L1Loss  1e-05\n",
            "Epoch [26/250] ---- Training Loss: 0.0300 ---- Validation Loss: 0.0289    L1Loss  1e-05\n",
            "Epoch [27/250] ---- Training Loss: 0.0297 ---- Validation Loss: 0.0287    L1Loss  1e-05\n",
            "Epoch [28/250] ---- Training Loss: 0.0295 ---- Validation Loss: 0.0284    L1Loss  1e-05\n",
            "Epoch [29/250] ---- Training Loss: 0.0292 ---- Validation Loss: 0.0281    L1Loss  1e-05\n",
            "Epoch [30/250] ---- Training Loss: 0.0289 ---- Validation Loss: 0.0278    L1Loss  1e-05\n",
            "Epoch [31/250] ---- Training Loss: 0.0287 ---- Validation Loss: 0.0274    L1Loss  1e-05\n",
            "Epoch [32/250] ---- Training Loss: 0.0284 ---- Validation Loss: 0.0271    L1Loss  1e-05\n",
            "Epoch [33/250] ---- Training Loss: 0.0281 ---- Validation Loss: 0.0268    L1Loss  1e-05\n",
            "Epoch [34/250] ---- Training Loss: 0.0278 ---- Validation Loss: 0.0264    L1Loss  1e-05\n",
            "Epoch [35/250] ---- Training Loss: 0.0275 ---- Validation Loss: 0.0261    L1Loss  1e-05\n",
            "Epoch [36/250] ---- Training Loss: 0.0272 ---- Validation Loss: 0.0258    L1Loss  1e-05\n",
            "Epoch [37/250] ---- Training Loss: 0.0269 ---- Validation Loss: 0.0255    L1Loss  1e-05\n",
            "Epoch [38/250] ---- Training Loss: 0.0266 ---- Validation Loss: 0.0252    L1Loss  1e-05\n",
            "Epoch [39/250] ---- Training Loss: 0.0264 ---- Validation Loss: 0.0249    L1Loss  1e-05\n",
            "Epoch [40/250] ---- Training Loss: 0.0261 ---- Validation Loss: 0.0246    L1Loss  1e-05\n",
            "Epoch [41/250] ---- Training Loss: 0.0259 ---- Validation Loss: 0.0244    L1Loss  1e-05\n",
            "Epoch [42/250] ---- Training Loss: 0.0256 ---- Validation Loss: 0.0242    L1Loss  1e-05\n",
            "Epoch [43/250] ---- Training Loss: 0.0254 ---- Validation Loss: 0.0239    L1Loss  1e-05\n",
            "Epoch [44/250] ---- Training Loss: 0.0252 ---- Validation Loss: 0.0237    L1Loss  1e-05\n",
            "Epoch [45/250] ---- Training Loss: 0.0250 ---- Validation Loss: 0.0235    L1Loss  1e-05\n",
            "Epoch [46/250] ---- Training Loss: 0.0248 ---- Validation Loss: 0.0233    L1Loss  1e-05\n",
            "Epoch [47/250] ---- Training Loss: 0.0246 ---- Validation Loss: 0.0231    L1Loss  1e-05\n",
            "Epoch [48/250] ---- Training Loss: 0.0244 ---- Validation Loss: 0.0230    L1Loss  1e-05\n",
            "Epoch [49/250] ---- Training Loss: 0.0242 ---- Validation Loss: 0.0228    L1Loss  1e-05\n",
            "Epoch [50/250] ---- Training Loss: 0.0241 ---- Validation Loss: 0.0226    L1Loss  1e-05\n",
            "Epoch [51/250] ---- Training Loss: 0.0239 ---- Validation Loss: 0.0225    L1Loss  1e-05\n",
            "Epoch [52/250] ---- Training Loss: 0.0237 ---- Validation Loss: 0.0223    L1Loss  1e-05\n",
            "Epoch [53/250] ---- Training Loss: 0.0236 ---- Validation Loss: 0.0222    L1Loss  1e-05\n",
            "Epoch [54/250] ---- Training Loss: 0.0234 ---- Validation Loss: 0.0220    L1Loss  1e-05\n",
            "Epoch [55/250] ---- Training Loss: 0.0232 ---- Validation Loss: 0.0219    L1Loss  1e-05\n",
            "Epoch [56/250] ---- Training Loss: 0.0231 ---- Validation Loss: 0.0217    L1Loss  1e-05\n",
            "Epoch [57/250] ---- Training Loss: 0.0229 ---- Validation Loss: 0.0216    L1Loss  1e-05\n",
            "Epoch [58/250] ---- Training Loss: 0.0228 ---- Validation Loss: 0.0214    L1Loss  1e-05\n",
            "Epoch [59/250] ---- Training Loss: 0.0227 ---- Validation Loss: 0.0213    L1Loss  1e-05\n",
            "Epoch [60/250] ---- Training Loss: 0.0225 ---- Validation Loss: 0.0212    L1Loss  1e-05\n",
            "Epoch [61/250] ---- Training Loss: 0.0224 ---- Validation Loss: 0.0211    L1Loss  1e-05\n",
            "Epoch [62/250] ---- Training Loss: 0.0223 ---- Validation Loss: 0.0209    L1Loss  1e-05\n",
            "Epoch [63/250] ---- Training Loss: 0.0221 ---- Validation Loss: 0.0208    L1Loss  1e-05\n",
            "Epoch [64/250] ---- Training Loss: 0.0220 ---- Validation Loss: 0.0207    L1Loss  1e-05\n",
            "Epoch [65/250] ---- Training Loss: 0.0219 ---- Validation Loss: 0.0206    L1Loss  1e-05\n",
            "Epoch [66/250] ---- Training Loss: 0.0218 ---- Validation Loss: 0.0205    L1Loss  1e-05\n",
            "Epoch [67/250] ---- Training Loss: 0.0217 ---- Validation Loss: 0.0204    L1Loss  1e-05\n",
            "Epoch [68/250] ---- Training Loss: 0.0216 ---- Validation Loss: 0.0203    L1Loss  1e-05\n",
            "Epoch [69/250] ---- Training Loss: 0.0215 ---- Validation Loss: 0.0202    L1Loss  1e-05\n",
            "Epoch [70/250] ---- Training Loss: 0.0214 ---- Validation Loss: 0.0201    L1Loss  1e-05\n",
            "Epoch [71/250] ---- Training Loss: 0.0213 ---- Validation Loss: 0.0200    L1Loss  1e-05\n",
            "Epoch [72/250] ---- Training Loss: 0.0212 ---- Validation Loss: 0.0199    L1Loss  1e-05\n",
            "Epoch [73/250] ---- Training Loss: 0.0211 ---- Validation Loss: 0.0198    L1Loss  1e-05\n",
            "Epoch [74/250] ---- Training Loss: 0.0210 ---- Validation Loss: 0.0198    L1Loss  1e-05\n",
            "Epoch [75/250] ---- Training Loss: 0.0209 ---- Validation Loss: 0.0197    L1Loss  1e-05\n",
            "Epoch [76/250] ---- Training Loss: 0.0208 ---- Validation Loss: 0.0196    L1Loss  1e-05\n",
            "Epoch [77/250] ---- Training Loss: 0.0208 ---- Validation Loss: 0.0195    L1Loss  1e-05\n",
            "Epoch [78/250] ---- Training Loss: 0.0207 ---- Validation Loss: 0.0195    L1Loss  1e-05\n",
            "Epoch [79/250] ---- Training Loss: 0.0206 ---- Validation Loss: 0.0194    L1Loss  1e-05\n",
            "Epoch [80/250] ---- Training Loss: 0.0205 ---- Validation Loss: 0.0193    L1Loss  1e-05\n",
            "Epoch [81/250] ---- Training Loss: 0.0205 ---- Validation Loss: 0.0193    L1Loss  1e-05\n",
            "Epoch [82/250] ---- Training Loss: 0.0204 ---- Validation Loss: 0.0192    L1Loss  1e-05\n",
            "Epoch [83/250] ---- Training Loss: 0.0203 ---- Validation Loss: 0.0191    L1Loss  1e-05\n",
            "Epoch [84/250] ---- Training Loss: 0.0203 ---- Validation Loss: 0.0191    L1Loss  1e-05\n",
            "Epoch [85/250] ---- Training Loss: 0.0202 ---- Validation Loss: 0.0190    L1Loss  1e-05\n",
            "Epoch [86/250] ---- Training Loss: 0.0201 ---- Validation Loss: 0.0190    L1Loss  1e-05\n",
            "Epoch [87/250] ---- Training Loss: 0.0201 ---- Validation Loss: 0.0189    L1Loss  1e-05\n",
            "Epoch [88/250] ---- Training Loss: 0.0200 ---- Validation Loss: 0.0189    L1Loss  1e-05\n",
            "Epoch [89/250] ---- Training Loss: 0.0200 ---- Validation Loss: 0.0188    L1Loss  1e-05\n",
            "Epoch [90/250] ---- Training Loss: 0.0199 ---- Validation Loss: 0.0187    L1Loss  1e-05\n",
            "Epoch [91/250] ---- Training Loss: 0.0199 ---- Validation Loss: 0.0187    L1Loss  1e-05\n",
            "Epoch [92/250] ---- Training Loss: 0.0198 ---- Validation Loss: 0.0186    L1Loss  1e-05\n",
            "Epoch [93/250] ---- Training Loss: 0.0198 ---- Validation Loss: 0.0186    L1Loss  1e-05\n",
            "Epoch [94/250] ---- Training Loss: 0.0197 ---- Validation Loss: 0.0185    L1Loss  1e-05\n",
            "Epoch [95/250] ---- Training Loss: 0.0197 ---- Validation Loss: 0.0185    L1Loss  1e-05\n",
            "Epoch [96/250] ---- Training Loss: 0.0196 ---- Validation Loss: 0.0185    L1Loss  1e-05\n",
            "Epoch [97/250] ---- Training Loss: 0.0196 ---- Validation Loss: 0.0184    L1Loss  1e-05\n",
            "Epoch [98/250] ---- Training Loss: 0.0195 ---- Validation Loss: 0.0184    L1Loss  1e-05\n",
            "Epoch [99/250] ---- Training Loss: 0.0195 ---- Validation Loss: 0.0183    L1Loss  1e-05\n",
            "Epoch [100/250] ---- Training Loss: 0.0194 ---- Validation Loss: 0.0183    L1Loss  1e-05\n",
            "Epoch [101/250] ---- Training Loss: 0.0194 ---- Validation Loss: 0.0182    L1Loss  1e-05\n",
            "Epoch [102/250] ---- Training Loss: 0.0194 ---- Validation Loss: 0.0182    L1Loss  1e-05\n",
            "Epoch [103/250] ---- Training Loss: 0.0193 ---- Validation Loss: 0.0181    L1Loss  1e-05\n",
            "Epoch [104/250] ---- Training Loss: 0.0193 ---- Validation Loss: 0.0181    L1Loss  1e-05\n",
            "Epoch [105/250] ---- Training Loss: 0.0192 ---- Validation Loss: 0.0181    L1Loss  1e-05\n",
            "Epoch [106/250] ---- Training Loss: 0.0192 ---- Validation Loss: 0.0180    L1Loss  1e-05\n",
            "Epoch [107/250] ---- Training Loss: 0.0192 ---- Validation Loss: 0.0180    L1Loss  1e-05\n",
            "Epoch [108/250] ---- Training Loss: 0.0191 ---- Validation Loss: 0.0180    L1Loss  1e-05\n",
            "Epoch [109/250] ---- Training Loss: 0.0191 ---- Validation Loss: 0.0179    L1Loss  1e-05\n",
            "Epoch [110/250] ---- Training Loss: 0.0191 ---- Validation Loss: 0.0179    L1Loss  1e-05\n",
            "Epoch [111/250] ---- Training Loss: 0.0190 ---- Validation Loss: 0.0178    L1Loss  1e-05\n",
            "Epoch [112/250] ---- Training Loss: 0.0190 ---- Validation Loss: 0.0178    L1Loss  1e-05\n",
            "Epoch [113/250] ---- Training Loss: 0.0190 ---- Validation Loss: 0.0178    L1Loss  1e-05\n",
            "Epoch [114/250] ---- Training Loss: 0.0189 ---- Validation Loss: 0.0178    L1Loss  1e-05\n",
            "Epoch [115/250] ---- Training Loss: 0.0189 ---- Validation Loss: 0.0177    L1Loss  1e-05\n",
            "Epoch [116/250] ---- Training Loss: 0.0189 ---- Validation Loss: 0.0177    L1Loss  1e-05\n",
            "Epoch [117/250] ---- Training Loss: 0.0188 ---- Validation Loss: 0.0177    L1Loss  1e-05\n",
            "Epoch [118/250] ---- Training Loss: 0.0188 ---- Validation Loss: 0.0176    L1Loss  1e-05\n",
            "Epoch [119/250] ---- Training Loss: 0.0188 ---- Validation Loss: 0.0176    L1Loss  1e-05\n",
            "Epoch [120/250] ---- Training Loss: 0.0187 ---- Validation Loss: 0.0176    L1Loss  1e-05\n",
            "Epoch [121/250] ---- Training Loss: 0.0187 ---- Validation Loss: 0.0176    L1Loss  1e-05\n",
            "Epoch [122/250] ---- Training Loss: 0.0187 ---- Validation Loss: 0.0175    L1Loss  1e-05\n",
            "Epoch [123/250] ---- Training Loss: 0.0187 ---- Validation Loss: 0.0175    L1Loss  1e-05\n",
            "Epoch [124/250] ---- Training Loss: 0.0186 ---- Validation Loss: 0.0175    L1Loss  1e-05\n",
            "Epoch [125/250] ---- Training Loss: 0.0186 ---- Validation Loss: 0.0175    L1Loss  1e-05\n",
            "Epoch [126/250] ---- Training Loss: 0.0186 ---- Validation Loss: 0.0174    L1Loss  1e-05\n",
            "Epoch [127/250] ---- Training Loss: 0.0186 ---- Validation Loss: 0.0174    L1Loss  1e-05\n",
            "Epoch [128/250] ---- Training Loss: 0.0185 ---- Validation Loss: 0.0174    L1Loss  1e-05\n",
            "Epoch [129/250] ---- Training Loss: 0.0185 ---- Validation Loss: 0.0174    L1Loss  1e-05\n",
            "Epoch [130/250] ---- Training Loss: 0.0185 ---- Validation Loss: 0.0173    L1Loss  1e-05\n",
            "Epoch [131/250] ---- Training Loss: 0.0185 ---- Validation Loss: 0.0173    L1Loss  1e-05\n",
            "Epoch [132/250] ---- Training Loss: 0.0184 ---- Validation Loss: 0.0173    L1Loss  1e-05\n",
            "Epoch [133/250] ---- Training Loss: 0.0184 ---- Validation Loss: 0.0172    L1Loss  1e-05\n",
            "Epoch [134/250] ---- Training Loss: 0.0184 ---- Validation Loss: 0.0172    L1Loss  1e-05\n",
            "Epoch [135/250] ---- Training Loss: 0.0184 ---- Validation Loss: 0.0172    L1Loss  1e-05\n",
            "Epoch [136/250] ---- Training Loss: 0.0183 ---- Validation Loss: 0.0172    L1Loss  1e-05\n",
            "Epoch [137/250] ---- Training Loss: 0.0183 ---- Validation Loss: 0.0172    L1Loss  1e-05\n",
            "Epoch [138/250] ---- Training Loss: 0.0183 ---- Validation Loss: 0.0171    L1Loss  1e-05\n",
            "Epoch [139/250] ---- Training Loss: 0.0183 ---- Validation Loss: 0.0171    L1Loss  1e-05\n",
            "Epoch [140/250] ---- Training Loss: 0.0183 ---- Validation Loss: 0.0171    L1Loss  1e-05\n",
            "Epoch [141/250] ---- Training Loss: 0.0182 ---- Validation Loss: 0.0171    L1Loss  1e-05\n",
            "Epoch [142/250] ---- Training Loss: 0.0182 ---- Validation Loss: 0.0171    L1Loss  1e-05\n",
            "Epoch [143/250] ---- Training Loss: 0.0182 ---- Validation Loss: 0.0170    L1Loss  1e-05\n",
            "Epoch [144/250] ---- Training Loss: 0.0182 ---- Validation Loss: 0.0170    L1Loss  1e-05\n",
            "Epoch [145/250] ---- Training Loss: 0.0182 ---- Validation Loss: 0.0170    L1Loss  1e-05\n",
            "Epoch [146/250] ---- Training Loss: 0.0181 ---- Validation Loss: 0.0170    L1Loss  1e-05\n",
            "Epoch [147/250] ---- Training Loss: 0.0181 ---- Validation Loss: 0.0170    L1Loss  1e-05\n",
            "Epoch [148/250] ---- Training Loss: 0.0181 ---- Validation Loss: 0.0169    L1Loss  1e-05\n",
            "Epoch [149/250] ---- Training Loss: 0.0181 ---- Validation Loss: 0.0169    L1Loss  1e-05\n",
            "Epoch [150/250] ---- Training Loss: 0.0181 ---- Validation Loss: 0.0169    L1Loss  1e-05\n",
            "Epoch [151/250] ---- Training Loss: 0.0181 ---- Validation Loss: 0.0169    L1Loss  1e-05\n",
            "Epoch [152/250] ---- Training Loss: 0.0180 ---- Validation Loss: 0.0169    L1Loss  1e-05\n",
            "Epoch [153/250] ---- Training Loss: 0.0180 ---- Validation Loss: 0.0169    L1Loss  1e-05\n",
            "Epoch [154/250] ---- Training Loss: 0.0180 ---- Validation Loss: 0.0169    L1Loss  1e-05\n",
            "Epoch [155/250] ---- Training Loss: 0.0180 ---- Validation Loss: 0.0168    L1Loss  1e-05\n",
            "Epoch [156/250] ---- Training Loss: 0.0180 ---- Validation Loss: 0.0169    L1Loss  1e-05\n",
            "Epoch [157/250] ---- Training Loss: 0.0180 ---- Validation Loss: 0.0168    L1Loss  1e-05\n",
            "Epoch [158/250] ---- Training Loss: 0.0179 ---- Validation Loss: 0.0168    L1Loss  1e-05\n",
            "Epoch [159/250] ---- Training Loss: 0.0179 ---- Validation Loss: 0.0168    L1Loss  1e-05\n",
            "Epoch [160/250] ---- Training Loss: 0.0179 ---- Validation Loss: 0.0168    L1Loss  1e-05\n",
            "Epoch [161/250] ---- Training Loss: 0.0179 ---- Validation Loss: 0.0168    L1Loss  1e-05\n",
            "Epoch [162/250] ---- Training Loss: 0.0179 ---- Validation Loss: 0.0168    L1Loss  1e-05\n",
            "Epoch [163/250] ---- Training Loss: 0.0179 ---- Validation Loss: 0.0167    L1Loss  1e-05\n",
            "Epoch [164/250] ---- Training Loss: 0.0179 ---- Validation Loss: 0.0167    L1Loss  1e-05\n",
            "Epoch [165/250] ---- Training Loss: 0.0178 ---- Validation Loss: 0.0167    L1Loss  1e-05\n",
            "Epoch [166/250] ---- Training Loss: 0.0178 ---- Validation Loss: 0.0167    L1Loss  1e-05\n",
            "Epoch [167/250] ---- Training Loss: 0.0178 ---- Validation Loss: 0.0167    L1Loss  1e-05\n",
            "Epoch [168/250] ---- Training Loss: 0.0178 ---- Validation Loss: 0.0167    L1Loss  1e-05\n",
            "Epoch [169/250] ---- Training Loss: 0.0178 ---- Validation Loss: 0.0167    L1Loss  1e-05\n",
            "Epoch [170/250] ---- Training Loss: 0.0178 ---- Validation Loss: 0.0167    L1Loss  1e-05\n",
            "Epoch [171/250] ---- Training Loss: 0.0178 ---- Validation Loss: 0.0167    L1Loss  1e-05\n",
            "Epoch [172/250] ---- Training Loss: 0.0178 ---- Validation Loss: 0.0166    L1Loss  1e-05\n",
            "Epoch [173/250] ---- Training Loss: 0.0178 ---- Validation Loss: 0.0166    L1Loss  1e-05\n",
            "Epoch [174/250] ---- Training Loss: 0.0177 ---- Validation Loss: 0.0166    L1Loss  1e-05\n",
            "Epoch [175/250] ---- Training Loss: 0.0177 ---- Validation Loss: 0.0166    L1Loss  1e-05\n",
            "Epoch [176/250] ---- Training Loss: 0.0177 ---- Validation Loss: 0.0166    L1Loss  1e-05\n",
            "Epoch [177/250] ---- Training Loss: 0.0177 ---- Validation Loss: 0.0166    L1Loss  1e-05\n",
            "Epoch [178/250] ---- Training Loss: 0.0177 ---- Validation Loss: 0.0166    L1Loss  1e-05\n",
            "Epoch [179/250] ---- Training Loss: 0.0177 ---- Validation Loss: 0.0166    L1Loss  1e-05\n",
            "Epoch [180/250] ---- Training Loss: 0.0177 ---- Validation Loss: 0.0165    L1Loss  1e-05\n",
            "Epoch [181/250] ---- Training Loss: 0.0177 ---- Validation Loss: 0.0165    L1Loss  1e-05\n",
            "Epoch [182/250] ---- Training Loss: 0.0176 ---- Validation Loss: 0.0165    L1Loss  1e-05\n",
            "Epoch [183/250] ---- Training Loss: 0.0176 ---- Validation Loss: 0.0165    L1Loss  1e-05\n",
            "Epoch [184/250] ---- Training Loss: 0.0176 ---- Validation Loss: 0.0165    L1Loss  1e-05\n",
            "Epoch [185/250] ---- Training Loss: 0.0176 ---- Validation Loss: 0.0165    L1Loss  1e-05\n",
            "Epoch [186/250] ---- Training Loss: 0.0176 ---- Validation Loss: 0.0165    L1Loss  1e-05\n",
            "Epoch [187/250] ---- Training Loss: 0.0176 ---- Validation Loss: 0.0165    L1Loss  1e-05\n",
            "Epoch [188/250] ---- Training Loss: 0.0176 ---- Validation Loss: 0.0165    L1Loss  1e-05\n",
            "Epoch [189/250] ---- Training Loss: 0.0176 ---- Validation Loss: 0.0165    L1Loss  1e-05\n",
            "Epoch [190/250] ---- Training Loss: 0.0176 ---- Validation Loss: 0.0164    L1Loss  1e-05\n",
            "Epoch [191/250] ---- Training Loss: 0.0175 ---- Validation Loss: 0.0164    L1Loss  1e-05\n",
            "Epoch [192/250] ---- Training Loss: 0.0175 ---- Validation Loss: 0.0164    L1Loss  1e-05\n",
            "Epoch [193/250] ---- Training Loss: 0.0175 ---- Validation Loss: 0.0164    L1Loss  1e-05\n",
            "Epoch [194/250] ---- Training Loss: 0.0175 ---- Validation Loss: 0.0164    L1Loss  1e-05\n",
            "Epoch [195/250] ---- Training Loss: 0.0175 ---- Validation Loss: 0.0164    L1Loss  1e-05\n",
            "Epoch [196/250] ---- Training Loss: 0.0175 ---- Validation Loss: 0.0164    L1Loss  1e-05\n",
            "Epoch [197/250] ---- Training Loss: 0.0175 ---- Validation Loss: 0.0164    L1Loss  1e-05\n",
            "Epoch [198/250] ---- Training Loss: 0.0175 ---- Validation Loss: 0.0164    L1Loss  1e-05\n",
            "Epoch [199/250] ---- Training Loss: 0.0175 ---- Validation Loss: 0.0163    L1Loss  1e-05\n",
            "Epoch [200/250] ---- Training Loss: 0.0175 ---- Validation Loss: 0.0163    L1Loss  1e-05\n",
            "Epoch [201/250] ---- Training Loss: 0.0174 ---- Validation Loss: 0.0163    L1Loss  1e-05\n",
            "Epoch [202/250] ---- Training Loss: 0.0174 ---- Validation Loss: 0.0163    L1Loss  1e-05\n",
            "Epoch [203/250] ---- Training Loss: 0.0174 ---- Validation Loss: 0.0163    L1Loss  1e-05\n",
            "Epoch [204/250] ---- Training Loss: 0.0174 ---- Validation Loss: 0.0163    L1Loss  1e-05\n",
            "Epoch [205/250] ---- Training Loss: 0.0174 ---- Validation Loss: 0.0163    L1Loss  1e-05\n",
            "Epoch [206/250] ---- Training Loss: 0.0174 ---- Validation Loss: 0.0163    L1Loss  1e-05\n",
            "Epoch [207/250] ---- Training Loss: 0.0174 ---- Validation Loss: 0.0163    L1Loss  1e-05\n",
            "Epoch [208/250] ---- Training Loss: 0.0174 ---- Validation Loss: 0.0163    L1Loss  1e-05\n",
            "Epoch [209/250] ---- Training Loss: 0.0174 ---- Validation Loss: 0.0163    L1Loss  1e-05\n",
            "Epoch [210/250] ---- Training Loss: 0.0174 ---- Validation Loss: 0.0163    L1Loss  1e-05\n",
            "Epoch [211/250] ---- Training Loss: 0.0173 ---- Validation Loss: 0.0162    L1Loss  1e-05\n",
            "Epoch [212/250] ---- Training Loss: 0.0173 ---- Validation Loss: 0.0162    L1Loss  1e-05\n",
            "Epoch [213/250] ---- Training Loss: 0.0173 ---- Validation Loss: 0.0162    L1Loss  1e-05\n",
            "Epoch [214/250] ---- Training Loss: 0.0173 ---- Validation Loss: 0.0162    L1Loss  1e-05\n",
            "Epoch [215/250] ---- Training Loss: 0.0173 ---- Validation Loss: 0.0162    L1Loss  1e-05\n",
            "Epoch [216/250] ---- Training Loss: 0.0173 ---- Validation Loss: 0.0162    L1Loss  1e-05\n",
            "Epoch [217/250] ---- Training Loss: 0.0173 ---- Validation Loss: 0.0162    L1Loss  1e-05\n",
            "Epoch [218/250] ---- Training Loss: 0.0173 ---- Validation Loss: 0.0162    L1Loss  1e-05\n",
            "Epoch [219/250] ---- Training Loss: 0.0173 ---- Validation Loss: 0.0162    L1Loss  1e-05\n",
            "Epoch [220/250] ---- Training Loss: 0.0173 ---- Validation Loss: 0.0162    L1Loss  1e-05\n",
            "Epoch [221/250] ---- Training Loss: 0.0172 ---- Validation Loss: 0.0162    L1Loss  1e-05\n",
            "Epoch [222/250] ---- Training Loss: 0.0172 ---- Validation Loss: 0.0161    L1Loss  1e-05\n",
            "Epoch [223/250] ---- Training Loss: 0.0172 ---- Validation Loss: 0.0161    L1Loss  1e-05\n",
            "Epoch [224/250] ---- Training Loss: 0.0172 ---- Validation Loss: 0.0161    L1Loss  1e-05\n",
            "Epoch [225/250] ---- Training Loss: 0.0172 ---- Validation Loss: 0.0162    L1Loss  1e-05\n",
            "Epoch [226/250] ---- Training Loss: 0.0172 ---- Validation Loss: 0.0161    L1Loss  1e-05\n",
            "Epoch [227/250] ---- Training Loss: 0.0172 ---- Validation Loss: 0.0161    L1Loss  1e-05\n",
            "Epoch [228/250] ---- Training Loss: 0.0172 ---- Validation Loss: 0.0161    L1Loss  1e-05\n",
            "Epoch [229/250] ---- Training Loss: 0.0172 ---- Validation Loss: 0.0161    L1Loss  1e-05\n",
            "Epoch [230/250] ---- Training Loss: 0.0172 ---- Validation Loss: 0.0161    L1Loss  1e-05\n",
            "Epoch [231/250] ---- Training Loss: 0.0172 ---- Validation Loss: 0.0161    L1Loss  1e-05\n",
            "Epoch [232/250] ---- Training Loss: 0.0172 ---- Validation Loss: 0.0161    L1Loss  1e-05\n",
            "Epoch [233/250] ---- Training Loss: 0.0172 ---- Validation Loss: 0.0161    L1Loss  1e-05\n",
            "Epoch [234/250] ---- Training Loss: 0.0171 ---- Validation Loss: 0.0161    L1Loss  1e-05\n",
            "Epoch [235/250] ---- Training Loss: 0.0171 ---- Validation Loss: 0.0161    L1Loss  1e-05\n",
            "Epoch [236/250] ---- Training Loss: 0.0171 ---- Validation Loss: 0.0160    L1Loss  1e-05\n",
            "Epoch [237/250] ---- Training Loss: 0.0171 ---- Validation Loss: 0.0160    L1Loss  1e-05\n",
            "Epoch [238/250] ---- Training Loss: 0.0171 ---- Validation Loss: 0.0160    L1Loss  1e-05\n",
            "Epoch [239/250] ---- Training Loss: 0.0171 ---- Validation Loss: 0.0160    L1Loss  1e-05\n",
            "Epoch [240/250] ---- Training Loss: 0.0171 ---- Validation Loss: 0.0160    L1Loss  1e-05\n",
            "Epoch [241/250] ---- Training Loss: 0.0171 ---- Validation Loss: 0.0160    L1Loss  1e-05\n",
            "Epoch [242/250] ---- Training Loss: 0.0171 ---- Validation Loss: 0.0160    L1Loss  1e-05\n",
            "Epoch [243/250] ---- Training Loss: 0.0171 ---- Validation Loss: 0.0160    L1Loss  1e-05\n",
            "Epoch [244/250] ---- Training Loss: 0.0171 ---- Validation Loss: 0.0160    L1Loss  1e-05\n",
            "Epoch [245/250] ---- Training Loss: 0.0171 ---- Validation Loss: 0.0160    L1Loss  1e-05\n",
            "Epoch [246/250] ---- Training Loss: 0.0171 ---- Validation Loss: 0.0160    L1Loss  1e-05\n",
            "Epoch [247/250] ---- Training Loss: 0.0170 ---- Validation Loss: 0.0160    L1Loss  1e-05\n",
            "Epoch [248/250] ---- Training Loss: 0.0170 ---- Validation Loss: 0.0160    L1Loss  1e-05\n",
            "Epoch [249/250] ---- Training Loss: 0.0170 ---- Validation Loss: 0.0160    L1Loss  1e-05\n",
            "Epoch [250/250] ---- Training Loss: 0.0170 ---- Validation Loss: 0.0160    L1Loss  1e-05\n",
            "Saved model: model_loss_L1Loss_wd_1e-05.pth\n",
            "Epoch [1/250] ---- Training Loss: 0.0828 ---- Validation Loss: 0.0417    L1Loss  5e-05\n",
            "Epoch [2/250] ---- Training Loss: 0.0486 ---- Validation Loss: 0.0403    L1Loss  5e-05\n",
            "Epoch [3/250] ---- Training Loss: 0.0472 ---- Validation Loss: 0.0392    L1Loss  5e-05\n",
            "Epoch [4/250] ---- Training Loss: 0.0458 ---- Validation Loss: 0.0380    L1Loss  5e-05\n",
            "Epoch [5/250] ---- Training Loss: 0.0445 ---- Validation Loss: 0.0370    L1Loss  5e-05\n",
            "Epoch [6/250] ---- Training Loss: 0.0433 ---- Validation Loss: 0.0363    L1Loss  5e-05\n",
            "Epoch [7/250] ---- Training Loss: 0.0421 ---- Validation Loss: 0.0358    L1Loss  5e-05\n",
            "Epoch [8/250] ---- Training Loss: 0.0411 ---- Validation Loss: 0.0354    L1Loss  5e-05\n",
            "Epoch [9/250] ---- Training Loss: 0.0400 ---- Validation Loss: 0.0351    L1Loss  5e-05\n",
            "Epoch [10/250] ---- Training Loss: 0.0390 ---- Validation Loss: 0.0347    L1Loss  5e-05\n",
            "Epoch [11/250] ---- Training Loss: 0.0382 ---- Validation Loss: 0.0344    L1Loss  5e-05\n",
            "Epoch [12/250] ---- Training Loss: 0.0374 ---- Validation Loss: 0.0341    L1Loss  5e-05\n",
            "Epoch [13/250] ---- Training Loss: 0.0368 ---- Validation Loss: 0.0338    L1Loss  5e-05\n",
            "Epoch [14/250] ---- Training Loss: 0.0362 ---- Validation Loss: 0.0335    L1Loss  5e-05\n",
            "Epoch [15/250] ---- Training Loss: 0.0358 ---- Validation Loss: 0.0333    L1Loss  5e-05\n",
            "Epoch [16/250] ---- Training Loss: 0.0354 ---- Validation Loss: 0.0331    L1Loss  5e-05\n",
            "Epoch [17/250] ---- Training Loss: 0.0350 ---- Validation Loss: 0.0329    L1Loss  5e-05\n",
            "Epoch [18/250] ---- Training Loss: 0.0347 ---- Validation Loss: 0.0327    L1Loss  5e-05\n",
            "Epoch [19/250] ---- Training Loss: 0.0344 ---- Validation Loss: 0.0326    L1Loss  5e-05\n",
            "Epoch [20/250] ---- Training Loss: 0.0341 ---- Validation Loss: 0.0324    L1Loss  5e-05\n",
            "Epoch [21/250] ---- Training Loss: 0.0339 ---- Validation Loss: 0.0322    L1Loss  5e-05\n",
            "Epoch [22/250] ---- Training Loss: 0.0336 ---- Validation Loss: 0.0321    L1Loss  5e-05\n",
            "Epoch [23/250] ---- Training Loss: 0.0334 ---- Validation Loss: 0.0319    L1Loss  5e-05\n",
            "Epoch [24/250] ---- Training Loss: 0.0331 ---- Validation Loss: 0.0317    L1Loss  5e-05\n",
            "Epoch [25/250] ---- Training Loss: 0.0329 ---- Validation Loss: 0.0316    L1Loss  5e-05\n",
            "Epoch [26/250] ---- Training Loss: 0.0327 ---- Validation Loss: 0.0314    L1Loss  5e-05\n",
            "Epoch [27/250] ---- Training Loss: 0.0325 ---- Validation Loss: 0.0312    L1Loss  5e-05\n",
            "Epoch [28/250] ---- Training Loss: 0.0323 ---- Validation Loss: 0.0311    L1Loss  5e-05\n",
            "Epoch [29/250] ---- Training Loss: 0.0321 ---- Validation Loss: 0.0309    L1Loss  5e-05\n",
            "Epoch [30/250] ---- Training Loss: 0.0319 ---- Validation Loss: 0.0308    L1Loss  5e-05\n",
            "Epoch [31/250] ---- Training Loss: 0.0317 ---- Validation Loss: 0.0306    L1Loss  5e-05\n",
            "Epoch [32/250] ---- Training Loss: 0.0315 ---- Validation Loss: 0.0304    L1Loss  5e-05\n",
            "Epoch [33/250] ---- Training Loss: 0.0314 ---- Validation Loss: 0.0303    L1Loss  5e-05\n",
            "Epoch [34/250] ---- Training Loss: 0.0312 ---- Validation Loss: 0.0301    L1Loss  5e-05\n",
            "Epoch [35/250] ---- Training Loss: 0.0310 ---- Validation Loss: 0.0300    L1Loss  5e-05\n",
            "Epoch [36/250] ---- Training Loss: 0.0309 ---- Validation Loss: 0.0298    L1Loss  5e-05\n",
            "Epoch [37/250] ---- Training Loss: 0.0307 ---- Validation Loss: 0.0297    L1Loss  5e-05\n",
            "Epoch [38/250] ---- Training Loss: 0.0305 ---- Validation Loss: 0.0295    L1Loss  5e-05\n",
            "Epoch [39/250] ---- Training Loss: 0.0304 ---- Validation Loss: 0.0294    L1Loss  5e-05\n",
            "Epoch [40/250] ---- Training Loss: 0.0302 ---- Validation Loss: 0.0292    L1Loss  5e-05\n",
            "Epoch [41/250] ---- Training Loss: 0.0301 ---- Validation Loss: 0.0291    L1Loss  5e-05\n",
            "Epoch [42/250] ---- Training Loss: 0.0299 ---- Validation Loss: 0.0289    L1Loss  5e-05\n",
            "Epoch [43/250] ---- Training Loss: 0.0298 ---- Validation Loss: 0.0288    L1Loss  5e-05\n",
            "Epoch [44/250] ---- Training Loss: 0.0296 ---- Validation Loss: 0.0286    L1Loss  5e-05\n",
            "Epoch [45/250] ---- Training Loss: 0.0295 ---- Validation Loss: 0.0284    L1Loss  5e-05\n",
            "Epoch [46/250] ---- Training Loss: 0.0293 ---- Validation Loss: 0.0283    L1Loss  5e-05\n",
            "Epoch [47/250] ---- Training Loss: 0.0292 ---- Validation Loss: 0.0281    L1Loss  5e-05\n",
            "Epoch [48/250] ---- Training Loss: 0.0290 ---- Validation Loss: 0.0279    L1Loss  5e-05\n",
            "Epoch [49/250] ---- Training Loss: 0.0288 ---- Validation Loss: 0.0277    L1Loss  5e-05\n",
            "Epoch [50/250] ---- Training Loss: 0.0287 ---- Validation Loss: 0.0276    L1Loss  5e-05\n",
            "Epoch [51/250] ---- Training Loss: 0.0285 ---- Validation Loss: 0.0273    L1Loss  5e-05\n",
            "Epoch [52/250] ---- Training Loss: 0.0283 ---- Validation Loss: 0.0271    L1Loss  5e-05\n",
            "Epoch [53/250] ---- Training Loss: 0.0281 ---- Validation Loss: 0.0269    L1Loss  5e-05\n",
            "Epoch [54/250] ---- Training Loss: 0.0279 ---- Validation Loss: 0.0267    L1Loss  5e-05\n",
            "Epoch [55/250] ---- Training Loss: 0.0277 ---- Validation Loss: 0.0264    L1Loss  5e-05\n",
            "Epoch [56/250] ---- Training Loss: 0.0275 ---- Validation Loss: 0.0262    L1Loss  5e-05\n",
            "Epoch [57/250] ---- Training Loss: 0.0273 ---- Validation Loss: 0.0260    L1Loss  5e-05\n",
            "Epoch [58/250] ---- Training Loss: 0.0271 ---- Validation Loss: 0.0258    L1Loss  5e-05\n",
            "Epoch [59/250] ---- Training Loss: 0.0269 ---- Validation Loss: 0.0256    L1Loss  5e-05\n",
            "Epoch [60/250] ---- Training Loss: 0.0268 ---- Validation Loss: 0.0254    L1Loss  5e-05\n",
            "Epoch [61/250] ---- Training Loss: 0.0266 ---- Validation Loss: 0.0252    L1Loss  5e-05\n",
            "Epoch [62/250] ---- Training Loss: 0.0264 ---- Validation Loss: 0.0250    L1Loss  5e-05\n",
            "Epoch [63/250] ---- Training Loss: 0.0262 ---- Validation Loss: 0.0249    L1Loss  5e-05\n",
            "Epoch [64/250] ---- Training Loss: 0.0261 ---- Validation Loss: 0.0247    L1Loss  5e-05\n",
            "Epoch [65/250] ---- Training Loss: 0.0259 ---- Validation Loss: 0.0246    L1Loss  5e-05\n",
            "Epoch [66/250] ---- Training Loss: 0.0258 ---- Validation Loss: 0.0244    L1Loss  5e-05\n",
            "Epoch [67/250] ---- Training Loss: 0.0256 ---- Validation Loss: 0.0243    L1Loss  5e-05\n",
            "Epoch [68/250] ---- Training Loss: 0.0255 ---- Validation Loss: 0.0241    L1Loss  5e-05\n",
            "Epoch [69/250] ---- Training Loss: 0.0253 ---- Validation Loss: 0.0240    L1Loss  5e-05\n",
            "Epoch [70/250] ---- Training Loss: 0.0252 ---- Validation Loss: 0.0239    L1Loss  5e-05\n",
            "Epoch [71/250] ---- Training Loss: 0.0251 ---- Validation Loss: 0.0238    L1Loss  5e-05\n",
            "Epoch [72/250] ---- Training Loss: 0.0249 ---- Validation Loss: 0.0237    L1Loss  5e-05\n",
            "Epoch [73/250] ---- Training Loss: 0.0248 ---- Validation Loss: 0.0235    L1Loss  5e-05\n",
            "Epoch [74/250] ---- Training Loss: 0.0247 ---- Validation Loss: 0.0234    L1Loss  5e-05\n",
            "Epoch [75/250] ---- Training Loss: 0.0246 ---- Validation Loss: 0.0233    L1Loss  5e-05\n",
            "Epoch [76/250] ---- Training Loss: 0.0245 ---- Validation Loss: 0.0232    L1Loss  5e-05\n",
            "Epoch [77/250] ---- Training Loss: 0.0244 ---- Validation Loss: 0.0231    L1Loss  5e-05\n",
            "Epoch [78/250] ---- Training Loss: 0.0243 ---- Validation Loss: 0.0230    L1Loss  5e-05\n",
            "Epoch [79/250] ---- Training Loss: 0.0242 ---- Validation Loss: 0.0229    L1Loss  5e-05\n",
            "Epoch [80/250] ---- Training Loss: 0.0241 ---- Validation Loss: 0.0228    L1Loss  5e-05\n",
            "Epoch [81/250] ---- Training Loss: 0.0240 ---- Validation Loss: 0.0227    L1Loss  5e-05\n",
            "Epoch [82/250] ---- Training Loss: 0.0239 ---- Validation Loss: 0.0226    L1Loss  5e-05\n",
            "Epoch [83/250] ---- Training Loss: 0.0237 ---- Validation Loss: 0.0225    L1Loss  5e-05\n",
            "Epoch [84/250] ---- Training Loss: 0.0236 ---- Validation Loss: 0.0224    L1Loss  5e-05\n",
            "Epoch [85/250] ---- Training Loss: 0.0235 ---- Validation Loss: 0.0223    L1Loss  5e-05\n",
            "Epoch [86/250] ---- Training Loss: 0.0234 ---- Validation Loss: 0.0222    L1Loss  5e-05\n",
            "Epoch [87/250] ---- Training Loss: 0.0233 ---- Validation Loss: 0.0221    L1Loss  5e-05\n",
            "Epoch [88/250] ---- Training Loss: 0.0232 ---- Validation Loss: 0.0220    L1Loss  5e-05\n",
            "Epoch [89/250] ---- Training Loss: 0.0231 ---- Validation Loss: 0.0219    L1Loss  5e-05\n",
            "Epoch [90/250] ---- Training Loss: 0.0230 ---- Validation Loss: 0.0218    L1Loss  5e-05\n",
            "Epoch [91/250] ---- Training Loss: 0.0229 ---- Validation Loss: 0.0217    L1Loss  5e-05\n",
            "Epoch [92/250] ---- Training Loss: 0.0228 ---- Validation Loss: 0.0216    L1Loss  5e-05\n",
            "Epoch [93/250] ---- Training Loss: 0.0227 ---- Validation Loss: 0.0215    L1Loss  5e-05\n",
            "Epoch [94/250] ---- Training Loss: 0.0226 ---- Validation Loss: 0.0214    L1Loss  5e-05\n",
            "Epoch [95/250] ---- Training Loss: 0.0225 ---- Validation Loss: 0.0213    L1Loss  5e-05\n",
            "Epoch [96/250] ---- Training Loss: 0.0224 ---- Validation Loss: 0.0212    L1Loss  5e-05\n",
            "Epoch [97/250] ---- Training Loss: 0.0223 ---- Validation Loss: 0.0211    L1Loss  5e-05\n",
            "Epoch [98/250] ---- Training Loss: 0.0222 ---- Validation Loss: 0.0210    L1Loss  5e-05\n",
            "Epoch [99/250] ---- Training Loss: 0.0221 ---- Validation Loss: 0.0208    L1Loss  5e-05\n",
            "Epoch [100/250] ---- Training Loss: 0.0220 ---- Validation Loss: 0.0207    L1Loss  5e-05\n",
            "Epoch [101/250] ---- Training Loss: 0.0219 ---- Validation Loss: 0.0206    L1Loss  5e-05\n",
            "Epoch [102/250] ---- Training Loss: 0.0218 ---- Validation Loss: 0.0205    L1Loss  5e-05\n",
            "Epoch [103/250] ---- Training Loss: 0.0217 ---- Validation Loss: 0.0204    L1Loss  5e-05\n",
            "Epoch [104/250] ---- Training Loss: 0.0217 ---- Validation Loss: 0.0204    L1Loss  5e-05\n",
            "Epoch [105/250] ---- Training Loss: 0.0215 ---- Validation Loss: 0.0202    L1Loss  5e-05\n",
            "Epoch [106/250] ---- Training Loss: 0.0215 ---- Validation Loss: 0.0202    L1Loss  5e-05\n",
            "Epoch [107/250] ---- Training Loss: 0.0214 ---- Validation Loss: 0.0201    L1Loss  5e-05\n",
            "Epoch [108/250] ---- Training Loss: 0.0213 ---- Validation Loss: 0.0200    L1Loss  5e-05\n",
            "Epoch [109/250] ---- Training Loss: 0.0212 ---- Validation Loss: 0.0199    L1Loss  5e-05\n",
            "Epoch [110/250] ---- Training Loss: 0.0211 ---- Validation Loss: 0.0198    L1Loss  5e-05\n",
            "Epoch [111/250] ---- Training Loss: 0.0210 ---- Validation Loss: 0.0197    L1Loss  5e-05\n",
            "Epoch [112/250] ---- Training Loss: 0.0209 ---- Validation Loss: 0.0196    L1Loss  5e-05\n",
            "Epoch [113/250] ---- Training Loss: 0.0209 ---- Validation Loss: 0.0196    L1Loss  5e-05\n",
            "Epoch [114/250] ---- Training Loss: 0.0208 ---- Validation Loss: 0.0195    L1Loss  5e-05\n",
            "Epoch [115/250] ---- Training Loss: 0.0207 ---- Validation Loss: 0.0194    L1Loss  5e-05\n",
            "Epoch [116/250] ---- Training Loss: 0.0206 ---- Validation Loss: 0.0193    L1Loss  5e-05\n",
            "Epoch [117/250] ---- Training Loss: 0.0206 ---- Validation Loss: 0.0193    L1Loss  5e-05\n",
            "Epoch [118/250] ---- Training Loss: 0.0205 ---- Validation Loss: 0.0192    L1Loss  5e-05\n",
            "Epoch [119/250] ---- Training Loss: 0.0205 ---- Validation Loss: 0.0191    L1Loss  5e-05\n",
            "Epoch [120/250] ---- Training Loss: 0.0204 ---- Validation Loss: 0.0191    L1Loss  5e-05\n",
            "Epoch [121/250] ---- Training Loss: 0.0203 ---- Validation Loss: 0.0190    L1Loss  5e-05\n",
            "Epoch [122/250] ---- Training Loss: 0.0203 ---- Validation Loss: 0.0189    L1Loss  5e-05\n",
            "Epoch [123/250] ---- Training Loss: 0.0202 ---- Validation Loss: 0.0189    L1Loss  5e-05\n",
            "Epoch [124/250] ---- Training Loss: 0.0202 ---- Validation Loss: 0.0188    L1Loss  5e-05\n",
            "Epoch [125/250] ---- Training Loss: 0.0201 ---- Validation Loss: 0.0188    L1Loss  5e-05\n",
            "Epoch [126/250] ---- Training Loss: 0.0200 ---- Validation Loss: 0.0187    L1Loss  5e-05\n",
            "Epoch [127/250] ---- Training Loss: 0.0200 ---- Validation Loss: 0.0187    L1Loss  5e-05\n",
            "Epoch [128/250] ---- Training Loss: 0.0200 ---- Validation Loss: 0.0186    L1Loss  5e-05\n",
            "Epoch [129/250] ---- Training Loss: 0.0199 ---- Validation Loss: 0.0186    L1Loss  5e-05\n",
            "Epoch [130/250] ---- Training Loss: 0.0199 ---- Validation Loss: 0.0186    L1Loss  5e-05\n",
            "Epoch [131/250] ---- Training Loss: 0.0198 ---- Validation Loss: 0.0185    L1Loss  5e-05\n",
            "Epoch [132/250] ---- Training Loss: 0.0198 ---- Validation Loss: 0.0185    L1Loss  5e-05\n",
            "Epoch [133/250] ---- Training Loss: 0.0197 ---- Validation Loss: 0.0184    L1Loss  5e-05\n",
            "Epoch [134/250] ---- Training Loss: 0.0197 ---- Validation Loss: 0.0184    L1Loss  5e-05\n",
            "Epoch [135/250] ---- Training Loss: 0.0197 ---- Validation Loss: 0.0184    L1Loss  5e-05\n",
            "Epoch [136/250] ---- Training Loss: 0.0196 ---- Validation Loss: 0.0183    L1Loss  5e-05\n",
            "Epoch [137/250] ---- Training Loss: 0.0196 ---- Validation Loss: 0.0183    L1Loss  5e-05\n",
            "Epoch [138/250] ---- Training Loss: 0.0196 ---- Validation Loss: 0.0183    L1Loss  5e-05\n",
            "Epoch [139/250] ---- Training Loss: 0.0195 ---- Validation Loss: 0.0182    L1Loss  5e-05\n",
            "Epoch [140/250] ---- Training Loss: 0.0195 ---- Validation Loss: 0.0182    L1Loss  5e-05\n",
            "Epoch [141/250] ---- Training Loss: 0.0195 ---- Validation Loss: 0.0182    L1Loss  5e-05\n",
            "Epoch [142/250] ---- Training Loss: 0.0194 ---- Validation Loss: 0.0182    L1Loss  5e-05\n",
            "Epoch [143/250] ---- Training Loss: 0.0194 ---- Validation Loss: 0.0181    L1Loss  5e-05\n",
            "Epoch [144/250] ---- Training Loss: 0.0194 ---- Validation Loss: 0.0181    L1Loss  5e-05\n",
            "Epoch [145/250] ---- Training Loss: 0.0194 ---- Validation Loss: 0.0181    L1Loss  5e-05\n",
            "Epoch [146/250] ---- Training Loss: 0.0193 ---- Validation Loss: 0.0180    L1Loss  5e-05\n",
            "Epoch [147/250] ---- Training Loss: 0.0193 ---- Validation Loss: 0.0180    L1Loss  5e-05\n",
            "Epoch [148/250] ---- Training Loss: 0.0193 ---- Validation Loss: 0.0180    L1Loss  5e-05\n",
            "Epoch [149/250] ---- Training Loss: 0.0193 ---- Validation Loss: 0.0180    L1Loss  5e-05\n",
            "Epoch [150/250] ---- Training Loss: 0.0192 ---- Validation Loss: 0.0180    L1Loss  5e-05\n",
            "Epoch [151/250] ---- Training Loss: 0.0192 ---- Validation Loss: 0.0179    L1Loss  5e-05\n",
            "Epoch [152/250] ---- Training Loss: 0.0192 ---- Validation Loss: 0.0179    L1Loss  5e-05\n",
            "Epoch [153/250] ---- Training Loss: 0.0192 ---- Validation Loss: 0.0179    L1Loss  5e-05\n",
            "Epoch [154/250] ---- Training Loss: 0.0192 ---- Validation Loss: 0.0179    L1Loss  5e-05\n",
            "Epoch [155/250] ---- Training Loss: 0.0191 ---- Validation Loss: 0.0179    L1Loss  5e-05\n",
            "Epoch [156/250] ---- Training Loss: 0.0191 ---- Validation Loss: 0.0178    L1Loss  5e-05\n",
            "Epoch [157/250] ---- Training Loss: 0.0191 ---- Validation Loss: 0.0178    L1Loss  5e-05\n",
            "Epoch [158/250] ---- Training Loss: 0.0191 ---- Validation Loss: 0.0178    L1Loss  5e-05\n",
            "Epoch [159/250] ---- Training Loss: 0.0191 ---- Validation Loss: 0.0178    L1Loss  5e-05\n",
            "Epoch [160/250] ---- Training Loss: 0.0190 ---- Validation Loss: 0.0178    L1Loss  5e-05\n",
            "Epoch [161/250] ---- Training Loss: 0.0190 ---- Validation Loss: 0.0178    L1Loss  5e-05\n",
            "Epoch [162/250] ---- Training Loss: 0.0190 ---- Validation Loss: 0.0177    L1Loss  5e-05\n",
            "Epoch [163/250] ---- Training Loss: 0.0190 ---- Validation Loss: 0.0177    L1Loss  5e-05\n",
            "Epoch [164/250] ---- Training Loss: 0.0190 ---- Validation Loss: 0.0177    L1Loss  5e-05\n",
            "Epoch [165/250] ---- Training Loss: 0.0190 ---- Validation Loss: 0.0177    L1Loss  5e-05\n",
            "Epoch [166/250] ---- Training Loss: 0.0189 ---- Validation Loss: 0.0177    L1Loss  5e-05\n",
            "Epoch [167/250] ---- Training Loss: 0.0189 ---- Validation Loss: 0.0177    L1Loss  5e-05\n",
            "Epoch [168/250] ---- Training Loss: 0.0189 ---- Validation Loss: 0.0177    L1Loss  5e-05\n",
            "Epoch [169/250] ---- Training Loss: 0.0189 ---- Validation Loss: 0.0176    L1Loss  5e-05\n",
            "Epoch [170/250] ---- Training Loss: 0.0189 ---- Validation Loss: 0.0176    L1Loss  5e-05\n",
            "Epoch [171/250] ---- Training Loss: 0.0189 ---- Validation Loss: 0.0176    L1Loss  5e-05\n",
            "Epoch [172/250] ---- Training Loss: 0.0188 ---- Validation Loss: 0.0176    L1Loss  5e-05\n",
            "Epoch [173/250] ---- Training Loss: 0.0188 ---- Validation Loss: 0.0176    L1Loss  5e-05\n",
            "Epoch [174/250] ---- Training Loss: 0.0188 ---- Validation Loss: 0.0176    L1Loss  5e-05\n",
            "Epoch [175/250] ---- Training Loss: 0.0188 ---- Validation Loss: 0.0176    L1Loss  5e-05\n",
            "Epoch [176/250] ---- Training Loss: 0.0188 ---- Validation Loss: 0.0175    L1Loss  5e-05\n",
            "Epoch [177/250] ---- Training Loss: 0.0188 ---- Validation Loss: 0.0175    L1Loss  5e-05\n",
            "Epoch [178/250] ---- Training Loss: 0.0188 ---- Validation Loss: 0.0175    L1Loss  5e-05\n",
            "Epoch [179/250] ---- Training Loss: 0.0187 ---- Validation Loss: 0.0175    L1Loss  5e-05\n",
            "Epoch [180/250] ---- Training Loss: 0.0187 ---- Validation Loss: 0.0175    L1Loss  5e-05\n",
            "Epoch [181/250] ---- Training Loss: 0.0187 ---- Validation Loss: 0.0175    L1Loss  5e-05\n",
            "Epoch [182/250] ---- Training Loss: 0.0187 ---- Validation Loss: 0.0174    L1Loss  5e-05\n",
            "Epoch [183/250] ---- Training Loss: 0.0187 ---- Validation Loss: 0.0174    L1Loss  5e-05\n",
            "Epoch [184/250] ---- Training Loss: 0.0187 ---- Validation Loss: 0.0174    L1Loss  5e-05\n",
            "Epoch [185/250] ---- Training Loss: 0.0187 ---- Validation Loss: 0.0174    L1Loss  5e-05\n",
            "Epoch [186/250] ---- Training Loss: 0.0186 ---- Validation Loss: 0.0174    L1Loss  5e-05\n",
            "Epoch [187/250] ---- Training Loss: 0.0186 ---- Validation Loss: 0.0174    L1Loss  5e-05\n",
            "Epoch [188/250] ---- Training Loss: 0.0186 ---- Validation Loss: 0.0174    L1Loss  5e-05\n",
            "Epoch [189/250] ---- Training Loss: 0.0186 ---- Validation Loss: 0.0174    L1Loss  5e-05\n",
            "Epoch [190/250] ---- Training Loss: 0.0186 ---- Validation Loss: 0.0174    L1Loss  5e-05\n",
            "Epoch [191/250] ---- Training Loss: 0.0186 ---- Validation Loss: 0.0173    L1Loss  5e-05\n",
            "Epoch [192/250] ---- Training Loss: 0.0186 ---- Validation Loss: 0.0173    L1Loss  5e-05\n",
            "Epoch [193/250] ---- Training Loss: 0.0186 ---- Validation Loss: 0.0173    L1Loss  5e-05\n",
            "Epoch [194/250] ---- Training Loss: 0.0185 ---- Validation Loss: 0.0173    L1Loss  5e-05\n",
            "Epoch [195/250] ---- Training Loss: 0.0185 ---- Validation Loss: 0.0173    L1Loss  5e-05\n",
            "Epoch [196/250] ---- Training Loss: 0.0185 ---- Validation Loss: 0.0173    L1Loss  5e-05\n",
            "Epoch [197/250] ---- Training Loss: 0.0185 ---- Validation Loss: 0.0173    L1Loss  5e-05\n",
            "Epoch [198/250] ---- Training Loss: 0.0185 ---- Validation Loss: 0.0173    L1Loss  5e-05\n",
            "Epoch [199/250] ---- Training Loss: 0.0185 ---- Validation Loss: 0.0173    L1Loss  5e-05\n",
            "Epoch [200/250] ---- Training Loss: 0.0185 ---- Validation Loss: 0.0172    L1Loss  5e-05\n",
            "Epoch [201/250] ---- Training Loss: 0.0184 ---- Validation Loss: 0.0172    L1Loss  5e-05\n",
            "Epoch [202/250] ---- Training Loss: 0.0184 ---- Validation Loss: 0.0172    L1Loss  5e-05\n",
            "Epoch [203/250] ---- Training Loss: 0.0184 ---- Validation Loss: 0.0172    L1Loss  5e-05\n",
            "Epoch [204/250] ---- Training Loss: 0.0184 ---- Validation Loss: 0.0172    L1Loss  5e-05\n",
            "Epoch [205/250] ---- Training Loss: 0.0184 ---- Validation Loss: 0.0172    L1Loss  5e-05\n",
            "Epoch [206/250] ---- Training Loss: 0.0184 ---- Validation Loss: 0.0172    L1Loss  5e-05\n",
            "Epoch [207/250] ---- Training Loss: 0.0184 ---- Validation Loss: 0.0172    L1Loss  5e-05\n",
            "Epoch [208/250] ---- Training Loss: 0.0184 ---- Validation Loss: 0.0172    L1Loss  5e-05\n",
            "Epoch [209/250] ---- Training Loss: 0.0184 ---- Validation Loss: 0.0171    L1Loss  5e-05\n",
            "Epoch [210/250] ---- Training Loss: 0.0183 ---- Validation Loss: 0.0171    L1Loss  5e-05\n",
            "Epoch [211/250] ---- Training Loss: 0.0183 ---- Validation Loss: 0.0171    L1Loss  5e-05\n",
            "Epoch [212/250] ---- Training Loss: 0.0183 ---- Validation Loss: 0.0171    L1Loss  5e-05\n",
            "Epoch [213/250] ---- Training Loss: 0.0183 ---- Validation Loss: 0.0171    L1Loss  5e-05\n",
            "Epoch [214/250] ---- Training Loss: 0.0183 ---- Validation Loss: 0.0171    L1Loss  5e-05\n",
            "Epoch [215/250] ---- Training Loss: 0.0183 ---- Validation Loss: 0.0171    L1Loss  5e-05\n",
            "Epoch [216/250] ---- Training Loss: 0.0183 ---- Validation Loss: 0.0171    L1Loss  5e-05\n",
            "Epoch [217/250] ---- Training Loss: 0.0183 ---- Validation Loss: 0.0171    L1Loss  5e-05\n",
            "Epoch [218/250] ---- Training Loss: 0.0183 ---- Validation Loss: 0.0171    L1Loss  5e-05\n",
            "Epoch [219/250] ---- Training Loss: 0.0182 ---- Validation Loss: 0.0170    L1Loss  5e-05\n",
            "Epoch [220/250] ---- Training Loss: 0.0182 ---- Validation Loss: 0.0170    L1Loss  5e-05\n",
            "Epoch [221/250] ---- Training Loss: 0.0182 ---- Validation Loss: 0.0170    L1Loss  5e-05\n",
            "Epoch [222/250] ---- Training Loss: 0.0182 ---- Validation Loss: 0.0170    L1Loss  5e-05\n",
            "Epoch [223/250] ---- Training Loss: 0.0182 ---- Validation Loss: 0.0170    L1Loss  5e-05\n",
            "Epoch [224/250] ---- Training Loss: 0.0182 ---- Validation Loss: 0.0170    L1Loss  5e-05\n",
            "Epoch [225/250] ---- Training Loss: 0.0182 ---- Validation Loss: 0.0170    L1Loss  5e-05\n",
            "Epoch [226/250] ---- Training Loss: 0.0182 ---- Validation Loss: 0.0170    L1Loss  5e-05\n",
            "Epoch [227/250] ---- Training Loss: 0.0182 ---- Validation Loss: 0.0170    L1Loss  5e-05\n",
            "Epoch [228/250] ---- Training Loss: 0.0182 ---- Validation Loss: 0.0170    L1Loss  5e-05\n",
            "Epoch [229/250] ---- Training Loss: 0.0181 ---- Validation Loss: 0.0169    L1Loss  5e-05\n",
            "Epoch [230/250] ---- Training Loss: 0.0181 ---- Validation Loss: 0.0169    L1Loss  5e-05\n",
            "Epoch [231/250] ---- Training Loss: 0.0181 ---- Validation Loss: 0.0169    L1Loss  5e-05\n",
            "Epoch [232/250] ---- Training Loss: 0.0181 ---- Validation Loss: 0.0169    L1Loss  5e-05\n",
            "Epoch [233/250] ---- Training Loss: 0.0181 ---- Validation Loss: 0.0169    L1Loss  5e-05\n",
            "Epoch [234/250] ---- Training Loss: 0.0181 ---- Validation Loss: 0.0169    L1Loss  5e-05\n",
            "Epoch [235/250] ---- Training Loss: 0.0181 ---- Validation Loss: 0.0169    L1Loss  5e-05\n",
            "Epoch [236/250] ---- Training Loss: 0.0181 ---- Validation Loss: 0.0169    L1Loss  5e-05\n",
            "Epoch [237/250] ---- Training Loss: 0.0181 ---- Validation Loss: 0.0169    L1Loss  5e-05\n",
            "Epoch [238/250] ---- Training Loss: 0.0181 ---- Validation Loss: 0.0169    L1Loss  5e-05\n",
            "Epoch [239/250] ---- Training Loss: 0.0181 ---- Validation Loss: 0.0169    L1Loss  5e-05\n",
            "Epoch [240/250] ---- Training Loss: 0.0181 ---- Validation Loss: 0.0168    L1Loss  5e-05\n",
            "Epoch [241/250] ---- Training Loss: 0.0180 ---- Validation Loss: 0.0168    L1Loss  5e-05\n",
            "Epoch [242/250] ---- Training Loss: 0.0180 ---- Validation Loss: 0.0168    L1Loss  5e-05\n",
            "Epoch [243/250] ---- Training Loss: 0.0180 ---- Validation Loss: 0.0168    L1Loss  5e-05\n",
            "Epoch [244/250] ---- Training Loss: 0.0180 ---- Validation Loss: 0.0168    L1Loss  5e-05\n",
            "Epoch [245/250] ---- Training Loss: 0.0180 ---- Validation Loss: 0.0168    L1Loss  5e-05\n",
            "Epoch [246/250] ---- Training Loss: 0.0180 ---- Validation Loss: 0.0168    L1Loss  5e-05\n",
            "Epoch [247/250] ---- Training Loss: 0.0180 ---- Validation Loss: 0.0168    L1Loss  5e-05\n",
            "Epoch [248/250] ---- Training Loss: 0.0180 ---- Validation Loss: 0.0168    L1Loss  5e-05\n",
            "Epoch [249/250] ---- Training Loss: 0.0180 ---- Validation Loss: 0.0168    L1Loss  5e-05\n",
            "Epoch [250/250] ---- Training Loss: 0.0180 ---- Validation Loss: 0.0168    L1Loss  5e-05\n",
            "Saved model: model_loss_L1Loss_wd_5e-05.pth\n",
            "Epoch [1/250] ---- Training Loss: 0.0455 ---- Validation Loss: 0.0406    L1Loss  1e-06\n",
            "Epoch [2/250] ---- Training Loss: 0.0428 ---- Validation Loss: 0.0386    L1Loss  1e-06\n",
            "Epoch [3/250] ---- Training Loss: 0.0407 ---- Validation Loss: 0.0371    L1Loss  1e-06\n",
            "Epoch [4/250] ---- Training Loss: 0.0392 ---- Validation Loss: 0.0363    L1Loss  1e-06\n",
            "Epoch [5/250] ---- Training Loss: 0.0380 ---- Validation Loss: 0.0356    L1Loss  1e-06\n",
            "Epoch [6/250] ---- Training Loss: 0.0370 ---- Validation Loss: 0.0349    L1Loss  1e-06\n",
            "Epoch [7/250] ---- Training Loss: 0.0361 ---- Validation Loss: 0.0343    L1Loss  1e-06\n",
            "Epoch [8/250] ---- Training Loss: 0.0353 ---- Validation Loss: 0.0338    L1Loss  1e-06\n",
            "Epoch [9/250] ---- Training Loss: 0.0347 ---- Validation Loss: 0.0333    L1Loss  1e-06\n",
            "Epoch [10/250] ---- Training Loss: 0.0343 ---- Validation Loss: 0.0330    L1Loss  1e-06\n",
            "Epoch [11/250] ---- Training Loss: 0.0339 ---- Validation Loss: 0.0327    L1Loss  1e-06\n",
            "Epoch [12/250] ---- Training Loss: 0.0336 ---- Validation Loss: 0.0325    L1Loss  1e-06\n",
            "Epoch [13/250] ---- Training Loss: 0.0333 ---- Validation Loss: 0.0323    L1Loss  1e-06\n",
            "Epoch [14/250] ---- Training Loss: 0.0330 ---- Validation Loss: 0.0320    L1Loss  1e-06\n",
            "Epoch [15/250] ---- Training Loss: 0.0327 ---- Validation Loss: 0.0318    L1Loss  1e-06\n",
            "Epoch [16/250] ---- Training Loss: 0.0324 ---- Validation Loss: 0.0315    L1Loss  1e-06\n",
            "Epoch [17/250] ---- Training Loss: 0.0321 ---- Validation Loss: 0.0313    L1Loss  1e-06\n",
            "Epoch [18/250] ---- Training Loss: 0.0319 ---- Validation Loss: 0.0310    L1Loss  1e-06\n",
            "Epoch [19/250] ---- Training Loss: 0.0316 ---- Validation Loss: 0.0307    L1Loss  1e-06\n",
            "Epoch [20/250] ---- Training Loss: 0.0313 ---- Validation Loss: 0.0305    L1Loss  1e-06\n",
            "Epoch [21/250] ---- Training Loss: 0.0311 ---- Validation Loss: 0.0302    L1Loss  1e-06\n",
            "Epoch [22/250] ---- Training Loss: 0.0308 ---- Validation Loss: 0.0300    L1Loss  1e-06\n",
            "Epoch [23/250] ---- Training Loss: 0.0306 ---- Validation Loss: 0.0297    L1Loss  1e-06\n",
            "Epoch [24/250] ---- Training Loss: 0.0303 ---- Validation Loss: 0.0294    L1Loss  1e-06\n",
            "Epoch [25/250] ---- Training Loss: 0.0301 ---- Validation Loss: 0.0291    L1Loss  1e-06\n",
            "Epoch [26/250] ---- Training Loss: 0.0298 ---- Validation Loss: 0.0288    L1Loss  1e-06\n",
            "Epoch [27/250] ---- Training Loss: 0.0296 ---- Validation Loss: 0.0285    L1Loss  1e-06\n",
            "Epoch [28/250] ---- Training Loss: 0.0293 ---- Validation Loss: 0.0282    L1Loss  1e-06\n",
            "Epoch [29/250] ---- Training Loss: 0.0290 ---- Validation Loss: 0.0279    L1Loss  1e-06\n",
            "Epoch [30/250] ---- Training Loss: 0.0287 ---- Validation Loss: 0.0276    L1Loss  1e-06\n",
            "Epoch [31/250] ---- Training Loss: 0.0285 ---- Validation Loss: 0.0272    L1Loss  1e-06\n",
            "Epoch [32/250] ---- Training Loss: 0.0282 ---- Validation Loss: 0.0269    L1Loss  1e-06\n",
            "Epoch [33/250] ---- Training Loss: 0.0279 ---- Validation Loss: 0.0266    L1Loss  1e-06\n",
            "Epoch [34/250] ---- Training Loss: 0.0276 ---- Validation Loss: 0.0264    L1Loss  1e-06\n",
            "Epoch [35/250] ---- Training Loss: 0.0274 ---- Validation Loss: 0.0261    L1Loss  1e-06\n",
            "Epoch [36/250] ---- Training Loss: 0.0271 ---- Validation Loss: 0.0258    L1Loss  1e-06\n",
            "Epoch [37/250] ---- Training Loss: 0.0269 ---- Validation Loss: 0.0255    L1Loss  1e-06\n",
            "Epoch [38/250] ---- Training Loss: 0.0266 ---- Validation Loss: 0.0253    L1Loss  1e-06\n",
            "Epoch [39/250] ---- Training Loss: 0.0264 ---- Validation Loss: 0.0250    L1Loss  1e-06\n",
            "Epoch [40/250] ---- Training Loss: 0.0261 ---- Validation Loss: 0.0247    L1Loss  1e-06\n",
            "Epoch [41/250] ---- Training Loss: 0.0259 ---- Validation Loss: 0.0245    L1Loss  1e-06\n",
            "Epoch [42/250] ---- Training Loss: 0.0256 ---- Validation Loss: 0.0242    L1Loss  1e-06\n",
            "Epoch [43/250] ---- Training Loss: 0.0254 ---- Validation Loss: 0.0240    L1Loss  1e-06\n",
            "Epoch [44/250] ---- Training Loss: 0.0252 ---- Validation Loss: 0.0238    L1Loss  1e-06\n",
            "Epoch [45/250] ---- Training Loss: 0.0249 ---- Validation Loss: 0.0235    L1Loss  1e-06\n",
            "Epoch [46/250] ---- Training Loss: 0.0247 ---- Validation Loss: 0.0233    L1Loss  1e-06\n",
            "Epoch [47/250] ---- Training Loss: 0.0245 ---- Validation Loss: 0.0231    L1Loss  1e-06\n",
            "Epoch [48/250] ---- Training Loss: 0.0243 ---- Validation Loss: 0.0228    L1Loss  1e-06\n",
            "Epoch [49/250] ---- Training Loss: 0.0241 ---- Validation Loss: 0.0226    L1Loss  1e-06\n",
            "Epoch [50/250] ---- Training Loss: 0.0239 ---- Validation Loss: 0.0224    L1Loss  1e-06\n",
            "Epoch [51/250] ---- Training Loss: 0.0237 ---- Validation Loss: 0.0222    L1Loss  1e-06\n",
            "Epoch [52/250] ---- Training Loss: 0.0235 ---- Validation Loss: 0.0220    L1Loss  1e-06\n",
            "Epoch [53/250] ---- Training Loss: 0.0233 ---- Validation Loss: 0.0218    L1Loss  1e-06\n",
            "Epoch [54/250] ---- Training Loss: 0.0231 ---- Validation Loss: 0.0217    L1Loss  1e-06\n",
            "Epoch [55/250] ---- Training Loss: 0.0229 ---- Validation Loss: 0.0215    L1Loss  1e-06\n",
            "Epoch [56/250] ---- Training Loss: 0.0227 ---- Validation Loss: 0.0213    L1Loss  1e-06\n",
            "Epoch [57/250] ---- Training Loss: 0.0225 ---- Validation Loss: 0.0211    L1Loss  1e-06\n",
            "Epoch [58/250] ---- Training Loss: 0.0223 ---- Validation Loss: 0.0209    L1Loss  1e-06\n",
            "Epoch [59/250] ---- Training Loss: 0.0222 ---- Validation Loss: 0.0207    L1Loss  1e-06\n",
            "Epoch [60/250] ---- Training Loss: 0.0220 ---- Validation Loss: 0.0206    L1Loss  1e-06\n",
            "Epoch [61/250] ---- Training Loss: 0.0218 ---- Validation Loss: 0.0204    L1Loss  1e-06\n",
            "Epoch [62/250] ---- Training Loss: 0.0217 ---- Validation Loss: 0.0203    L1Loss  1e-06\n",
            "Epoch [63/250] ---- Training Loss: 0.0215 ---- Validation Loss: 0.0201    L1Loss  1e-06\n",
            "Epoch [64/250] ---- Training Loss: 0.0214 ---- Validation Loss: 0.0200    L1Loss  1e-06\n",
            "Epoch [65/250] ---- Training Loss: 0.0212 ---- Validation Loss: 0.0199    L1Loss  1e-06\n",
            "Epoch [66/250] ---- Training Loss: 0.0211 ---- Validation Loss: 0.0197    L1Loss  1e-06\n",
            "Epoch [67/250] ---- Training Loss: 0.0210 ---- Validation Loss: 0.0196    L1Loss  1e-06\n",
            "Epoch [68/250] ---- Training Loss: 0.0209 ---- Validation Loss: 0.0195    L1Loss  1e-06\n",
            "Epoch [69/250] ---- Training Loss: 0.0207 ---- Validation Loss: 0.0194    L1Loss  1e-06\n",
            "Epoch [70/250] ---- Training Loss: 0.0206 ---- Validation Loss: 0.0193    L1Loss  1e-06\n",
            "Epoch [71/250] ---- Training Loss: 0.0205 ---- Validation Loss: 0.0192    L1Loss  1e-06\n",
            "Epoch [72/250] ---- Training Loss: 0.0204 ---- Validation Loss: 0.0191    L1Loss  1e-06\n",
            "Epoch [73/250] ---- Training Loss: 0.0203 ---- Validation Loss: 0.0190    L1Loss  1e-06\n",
            "Epoch [74/250] ---- Training Loss: 0.0202 ---- Validation Loss: 0.0190    L1Loss  1e-06\n",
            "Epoch [75/250] ---- Training Loss: 0.0202 ---- Validation Loss: 0.0189    L1Loss  1e-06\n",
            "Epoch [76/250] ---- Training Loss: 0.0201 ---- Validation Loss: 0.0188    L1Loss  1e-06\n",
            "Epoch [77/250] ---- Training Loss: 0.0200 ---- Validation Loss: 0.0187    L1Loss  1e-06\n",
            "Epoch [78/250] ---- Training Loss: 0.0199 ---- Validation Loss: 0.0187    L1Loss  1e-06\n",
            "Epoch [79/250] ---- Training Loss: 0.0198 ---- Validation Loss: 0.0186    L1Loss  1e-06\n",
            "Epoch [80/250] ---- Training Loss: 0.0197 ---- Validation Loss: 0.0185    L1Loss  1e-06\n",
            "Epoch [81/250] ---- Training Loss: 0.0197 ---- Validation Loss: 0.0185    L1Loss  1e-06\n",
            "Epoch [82/250] ---- Training Loss: 0.0196 ---- Validation Loss: 0.0184    L1Loss  1e-06\n",
            "Epoch [83/250] ---- Training Loss: 0.0195 ---- Validation Loss: 0.0183    L1Loss  1e-06\n",
            "Epoch [84/250] ---- Training Loss: 0.0195 ---- Validation Loss: 0.0183    L1Loss  1e-06\n",
            "Epoch [85/250] ---- Training Loss: 0.0194 ---- Validation Loss: 0.0182    L1Loss  1e-06\n",
            "Epoch [86/250] ---- Training Loss: 0.0193 ---- Validation Loss: 0.0182    L1Loss  1e-06\n",
            "Epoch [87/250] ---- Training Loss: 0.0193 ---- Validation Loss: 0.0181    L1Loss  1e-06\n",
            "Epoch [88/250] ---- Training Loss: 0.0192 ---- Validation Loss: 0.0181    L1Loss  1e-06\n",
            "Epoch [89/250] ---- Training Loss: 0.0192 ---- Validation Loss: 0.0180    L1Loss  1e-06\n",
            "Epoch [90/250] ---- Training Loss: 0.0191 ---- Validation Loss: 0.0180    L1Loss  1e-06\n",
            "Epoch [91/250] ---- Training Loss: 0.0190 ---- Validation Loss: 0.0179    L1Loss  1e-06\n",
            "Epoch [92/250] ---- Training Loss: 0.0190 ---- Validation Loss: 0.0179    L1Loss  1e-06\n",
            "Epoch [93/250] ---- Training Loss: 0.0189 ---- Validation Loss: 0.0178    L1Loss  1e-06\n",
            "Epoch [94/250] ---- Training Loss: 0.0189 ---- Validation Loss: 0.0177    L1Loss  1e-06\n",
            "Epoch [95/250] ---- Training Loss: 0.0188 ---- Validation Loss: 0.0177    L1Loss  1e-06\n",
            "Epoch [96/250] ---- Training Loss: 0.0188 ---- Validation Loss: 0.0177    L1Loss  1e-06\n",
            "Epoch [97/250] ---- Training Loss: 0.0187 ---- Validation Loss: 0.0176    L1Loss  1e-06\n",
            "Epoch [98/250] ---- Training Loss: 0.0187 ---- Validation Loss: 0.0175    L1Loss  1e-06\n",
            "Epoch [99/250] ---- Training Loss: 0.0186 ---- Validation Loss: 0.0175    L1Loss  1e-06\n",
            "Epoch [100/250] ---- Training Loss: 0.0186 ---- Validation Loss: 0.0175    L1Loss  1e-06\n",
            "Epoch [101/250] ---- Training Loss: 0.0185 ---- Validation Loss: 0.0174    L1Loss  1e-06\n",
            "Epoch [102/250] ---- Training Loss: 0.0185 ---- Validation Loss: 0.0174    L1Loss  1e-06\n",
            "Epoch [103/250] ---- Training Loss: 0.0184 ---- Validation Loss: 0.0173    L1Loss  1e-06\n",
            "Epoch [104/250] ---- Training Loss: 0.0184 ---- Validation Loss: 0.0173    L1Loss  1e-06\n",
            "Epoch [105/250] ---- Training Loss: 0.0183 ---- Validation Loss: 0.0172    L1Loss  1e-06\n",
            "Epoch [106/250] ---- Training Loss: 0.0183 ---- Validation Loss: 0.0172    L1Loss  1e-06\n",
            "Epoch [107/250] ---- Training Loss: 0.0182 ---- Validation Loss: 0.0171    L1Loss  1e-06\n",
            "Epoch [108/250] ---- Training Loss: 0.0182 ---- Validation Loss: 0.0171    L1Loss  1e-06\n",
            "Epoch [109/250] ---- Training Loss: 0.0181 ---- Validation Loss: 0.0171    L1Loss  1e-06\n",
            "Epoch [110/250] ---- Training Loss: 0.0181 ---- Validation Loss: 0.0170    L1Loss  1e-06\n",
            "Epoch [111/250] ---- Training Loss: 0.0181 ---- Validation Loss: 0.0170    L1Loss  1e-06\n",
            "Epoch [112/250] ---- Training Loss: 0.0180 ---- Validation Loss: 0.0170    L1Loss  1e-06\n",
            "Epoch [113/250] ---- Training Loss: 0.0180 ---- Validation Loss: 0.0169    L1Loss  1e-06\n",
            "Epoch [114/250] ---- Training Loss: 0.0179 ---- Validation Loss: 0.0169    L1Loss  1e-06\n",
            "Epoch [115/250] ---- Training Loss: 0.0179 ---- Validation Loss: 0.0169    L1Loss  1e-06\n",
            "Epoch [116/250] ---- Training Loss: 0.0179 ---- Validation Loss: 0.0168    L1Loss  1e-06\n",
            "Epoch [117/250] ---- Training Loss: 0.0178 ---- Validation Loss: 0.0168    L1Loss  1e-06\n",
            "Epoch [118/250] ---- Training Loss: 0.0178 ---- Validation Loss: 0.0168    L1Loss  1e-06\n",
            "Epoch [119/250] ---- Training Loss: 0.0178 ---- Validation Loss: 0.0167    L1Loss  1e-06\n",
            "Epoch [120/250] ---- Training Loss: 0.0177 ---- Validation Loss: 0.0167    L1Loss  1e-06\n",
            "Epoch [121/250] ---- Training Loss: 0.0177 ---- Validation Loss: 0.0167    L1Loss  1e-06\n",
            "Epoch [122/250] ---- Training Loss: 0.0177 ---- Validation Loss: 0.0167    L1Loss  1e-06\n",
            "Epoch [123/250] ---- Training Loss: 0.0176 ---- Validation Loss: 0.0166    L1Loss  1e-06\n",
            "Epoch [124/250] ---- Training Loss: 0.0176 ---- Validation Loss: 0.0166    L1Loss  1e-06\n",
            "Epoch [125/250] ---- Training Loss: 0.0176 ---- Validation Loss: 0.0166    L1Loss  1e-06\n",
            "Epoch [126/250] ---- Training Loss: 0.0176 ---- Validation Loss: 0.0166    L1Loss  1e-06\n",
            "Epoch [127/250] ---- Training Loss: 0.0175 ---- Validation Loss: 0.0165    L1Loss  1e-06\n",
            "Epoch [128/250] ---- Training Loss: 0.0175 ---- Validation Loss: 0.0165    L1Loss  1e-06\n",
            "Epoch [129/250] ---- Training Loss: 0.0175 ---- Validation Loss: 0.0165    L1Loss  1e-06\n",
            "Epoch [130/250] ---- Training Loss: 0.0175 ---- Validation Loss: 0.0165    L1Loss  1e-06\n",
            "Epoch [131/250] ---- Training Loss: 0.0174 ---- Validation Loss: 0.0164    L1Loss  1e-06\n",
            "Epoch [132/250] ---- Training Loss: 0.0174 ---- Validation Loss: 0.0164    L1Loss  1e-06\n",
            "Epoch [133/250] ---- Training Loss: 0.0174 ---- Validation Loss: 0.0164    L1Loss  1e-06\n",
            "Epoch [134/250] ---- Training Loss: 0.0174 ---- Validation Loss: 0.0164    L1Loss  1e-06\n",
            "Epoch [135/250] ---- Training Loss: 0.0173 ---- Validation Loss: 0.0164    L1Loss  1e-06\n",
            "Epoch [136/250] ---- Training Loss: 0.0173 ---- Validation Loss: 0.0163    L1Loss  1e-06\n",
            "Epoch [137/250] ---- Training Loss: 0.0173 ---- Validation Loss: 0.0163    L1Loss  1e-06\n",
            "Epoch [138/250] ---- Training Loss: 0.0173 ---- Validation Loss: 0.0163    L1Loss  1e-06\n",
            "Epoch [139/250] ---- Training Loss: 0.0172 ---- Validation Loss: 0.0163    L1Loss  1e-06\n",
            "Epoch [140/250] ---- Training Loss: 0.0172 ---- Validation Loss: 0.0162    L1Loss  1e-06\n",
            "Epoch [141/250] ---- Training Loss: 0.0172 ---- Validation Loss: 0.0162    L1Loss  1e-06\n",
            "Epoch [142/250] ---- Training Loss: 0.0172 ---- Validation Loss: 0.0162    L1Loss  1e-06\n",
            "Epoch [143/250] ---- Training Loss: 0.0171 ---- Validation Loss: 0.0162    L1Loss  1e-06\n",
            "Epoch [144/250] ---- Training Loss: 0.0171 ---- Validation Loss: 0.0162    L1Loss  1e-06\n",
            "Epoch [145/250] ---- Training Loss: 0.0171 ---- Validation Loss: 0.0162    L1Loss  1e-06\n",
            "Epoch [146/250] ---- Training Loss: 0.0171 ---- Validation Loss: 0.0161    L1Loss  1e-06\n",
            "Epoch [147/250] ---- Training Loss: 0.0171 ---- Validation Loss: 0.0161    L1Loss  1e-06\n",
            "Epoch [148/250] ---- Training Loss: 0.0170 ---- Validation Loss: 0.0161    L1Loss  1e-06\n",
            "Epoch [149/250] ---- Training Loss: 0.0170 ---- Validation Loss: 0.0161    L1Loss  1e-06\n",
            "Epoch [150/250] ---- Training Loss: 0.0170 ---- Validation Loss: 0.0161    L1Loss  1e-06\n",
            "Epoch [151/250] ---- Training Loss: 0.0170 ---- Validation Loss: 0.0161    L1Loss  1e-06\n",
            "Epoch [152/250] ---- Training Loss: 0.0170 ---- Validation Loss: 0.0160    L1Loss  1e-06\n",
            "Epoch [153/250] ---- Training Loss: 0.0169 ---- Validation Loss: 0.0160    L1Loss  1e-06\n",
            "Epoch [154/250] ---- Training Loss: 0.0169 ---- Validation Loss: 0.0160    L1Loss  1e-06\n",
            "Epoch [155/250] ---- Training Loss: 0.0169 ---- Validation Loss: 0.0160    L1Loss  1e-06\n",
            "Epoch [156/250] ---- Training Loss: 0.0169 ---- Validation Loss: 0.0160    L1Loss  1e-06\n",
            "Epoch [157/250] ---- Training Loss: 0.0169 ---- Validation Loss: 0.0160    L1Loss  1e-06\n",
            "Epoch [158/250] ---- Training Loss: 0.0169 ---- Validation Loss: 0.0159    L1Loss  1e-06\n",
            "Epoch [159/250] ---- Training Loss: 0.0168 ---- Validation Loss: 0.0159    L1Loss  1e-06\n",
            "Epoch [160/250] ---- Training Loss: 0.0168 ---- Validation Loss: 0.0159    L1Loss  1e-06\n",
            "Epoch [161/250] ---- Training Loss: 0.0168 ---- Validation Loss: 0.0159    L1Loss  1e-06\n",
            "Epoch [162/250] ---- Training Loss: 0.0168 ---- Validation Loss: 0.0159    L1Loss  1e-06\n",
            "Epoch [163/250] ---- Training Loss: 0.0168 ---- Validation Loss: 0.0159    L1Loss  1e-06\n",
            "Epoch [164/250] ---- Training Loss: 0.0168 ---- Validation Loss: 0.0158    L1Loss  1e-06\n",
            "Epoch [165/250] ---- Training Loss: 0.0168 ---- Validation Loss: 0.0158    L1Loss  1e-06\n",
            "Epoch [166/250] ---- Training Loss: 0.0167 ---- Validation Loss: 0.0158    L1Loss  1e-06\n",
            "Epoch [167/250] ---- Training Loss: 0.0167 ---- Validation Loss: 0.0158    L1Loss  1e-06\n",
            "Epoch [168/250] ---- Training Loss: 0.0167 ---- Validation Loss: 0.0158    L1Loss  1e-06\n",
            "Epoch [169/250] ---- Training Loss: 0.0167 ---- Validation Loss: 0.0158    L1Loss  1e-06\n",
            "Epoch [170/250] ---- Training Loss: 0.0167 ---- Validation Loss: 0.0157    L1Loss  1e-06\n",
            "Epoch [171/250] ---- Training Loss: 0.0167 ---- Validation Loss: 0.0157    L1Loss  1e-06\n",
            "Epoch [172/250] ---- Training Loss: 0.0166 ---- Validation Loss: 0.0157    L1Loss  1e-06\n",
            "Epoch [173/250] ---- Training Loss: 0.0166 ---- Validation Loss: 0.0157    L1Loss  1e-06\n",
            "Epoch [174/250] ---- Training Loss: 0.0166 ---- Validation Loss: 0.0157    L1Loss  1e-06\n",
            "Epoch [175/250] ---- Training Loss: 0.0166 ---- Validation Loss: 0.0157    L1Loss  1e-06\n",
            "Epoch [176/250] ---- Training Loss: 0.0166 ---- Validation Loss: 0.0157    L1Loss  1e-06\n",
            "Epoch [177/250] ---- Training Loss: 0.0166 ---- Validation Loss: 0.0157    L1Loss  1e-06\n",
            "Epoch [178/250] ---- Training Loss: 0.0166 ---- Validation Loss: 0.0156    L1Loss  1e-06\n",
            "Epoch [179/250] ---- Training Loss: 0.0165 ---- Validation Loss: 0.0156    L1Loss  1e-06\n",
            "Epoch [180/250] ---- Training Loss: 0.0165 ---- Validation Loss: 0.0156    L1Loss  1e-06\n",
            "Epoch [181/250] ---- Training Loss: 0.0165 ---- Validation Loss: 0.0156    L1Loss  1e-06\n",
            "Epoch [182/250] ---- Training Loss: 0.0165 ---- Validation Loss: 0.0156    L1Loss  1e-06\n",
            "Epoch [183/250] ---- Training Loss: 0.0165 ---- Validation Loss: 0.0156    L1Loss  1e-06\n",
            "Epoch [184/250] ---- Training Loss: 0.0165 ---- Validation Loss: 0.0155    L1Loss  1e-06\n",
            "Epoch [185/250] ---- Training Loss: 0.0165 ---- Validation Loss: 0.0155    L1Loss  1e-06\n",
            "Epoch [186/250] ---- Training Loss: 0.0164 ---- Validation Loss: 0.0155    L1Loss  1e-06\n",
            "Epoch [187/250] ---- Training Loss: 0.0164 ---- Validation Loss: 0.0155    L1Loss  1e-06\n",
            "Epoch [188/250] ---- Training Loss: 0.0164 ---- Validation Loss: 0.0155    L1Loss  1e-06\n",
            "Epoch [189/250] ---- Training Loss: 0.0164 ---- Validation Loss: 0.0155    L1Loss  1e-06\n",
            "Epoch [190/250] ---- Training Loss: 0.0164 ---- Validation Loss: 0.0155    L1Loss  1e-06\n",
            "Epoch [191/250] ---- Training Loss: 0.0164 ---- Validation Loss: 0.0155    L1Loss  1e-06\n",
            "Epoch [192/250] ---- Training Loss: 0.0164 ---- Validation Loss: 0.0155    L1Loss  1e-06\n",
            "Epoch [193/250] ---- Training Loss: 0.0164 ---- Validation Loss: 0.0155    L1Loss  1e-06\n",
            "Epoch [194/250] ---- Training Loss: 0.0164 ---- Validation Loss: 0.0154    L1Loss  1e-06\n",
            "Epoch [195/250] ---- Training Loss: 0.0163 ---- Validation Loss: 0.0154    L1Loss  1e-06\n",
            "Epoch [196/250] ---- Training Loss: 0.0163 ---- Validation Loss: 0.0154    L1Loss  1e-06\n",
            "Epoch [197/250] ---- Training Loss: 0.0163 ---- Validation Loss: 0.0154    L1Loss  1e-06\n",
            "Epoch [198/250] ---- Training Loss: 0.0163 ---- Validation Loss: 0.0154    L1Loss  1e-06\n",
            "Epoch [199/250] ---- Training Loss: 0.0163 ---- Validation Loss: 0.0154    L1Loss  1e-06\n",
            "Epoch [200/250] ---- Training Loss: 0.0163 ---- Validation Loss: 0.0154    L1Loss  1e-06\n",
            "Epoch [201/250] ---- Training Loss: 0.0163 ---- Validation Loss: 0.0154    L1Loss  1e-06\n",
            "Epoch [202/250] ---- Training Loss: 0.0163 ---- Validation Loss: 0.0153    L1Loss  1e-06\n",
            "Epoch [203/250] ---- Training Loss: 0.0163 ---- Validation Loss: 0.0153    L1Loss  1e-06\n",
            "Epoch [204/250] ---- Training Loss: 0.0162 ---- Validation Loss: 0.0153    L1Loss  1e-06\n",
            "Epoch [205/250] ---- Training Loss: 0.0162 ---- Validation Loss: 0.0153    L1Loss  1e-06\n",
            "Epoch [206/250] ---- Training Loss: 0.0162 ---- Validation Loss: 0.0153    L1Loss  1e-06\n",
            "Epoch [207/250] ---- Training Loss: 0.0162 ---- Validation Loss: 0.0153    L1Loss  1e-06\n",
            "Epoch [208/250] ---- Training Loss: 0.0162 ---- Validation Loss: 0.0153    L1Loss  1e-06\n",
            "Epoch [209/250] ---- Training Loss: 0.0162 ---- Validation Loss: 0.0153    L1Loss  1e-06\n",
            "Epoch [210/250] ---- Training Loss: 0.0162 ---- Validation Loss: 0.0153    L1Loss  1e-06\n",
            "Epoch [211/250] ---- Training Loss: 0.0162 ---- Validation Loss: 0.0153    L1Loss  1e-06\n",
            "Epoch [212/250] ---- Training Loss: 0.0162 ---- Validation Loss: 0.0152    L1Loss  1e-06\n",
            "Epoch [213/250] ---- Training Loss: 0.0162 ---- Validation Loss: 0.0152    L1Loss  1e-06\n",
            "Epoch [214/250] ---- Training Loss: 0.0161 ---- Validation Loss: 0.0152    L1Loss  1e-06\n",
            "Epoch [215/250] ---- Training Loss: 0.0161 ---- Validation Loss: 0.0152    L1Loss  1e-06\n",
            "Epoch [216/250] ---- Training Loss: 0.0161 ---- Validation Loss: 0.0152    L1Loss  1e-06\n",
            "Epoch [217/250] ---- Training Loss: 0.0161 ---- Validation Loss: 0.0152    L1Loss  1e-06\n",
            "Epoch [218/250] ---- Training Loss: 0.0161 ---- Validation Loss: 0.0152    L1Loss  1e-06\n",
            "Epoch [219/250] ---- Training Loss: 0.0161 ---- Validation Loss: 0.0152    L1Loss  1e-06\n",
            "Epoch [220/250] ---- Training Loss: 0.0161 ---- Validation Loss: 0.0152    L1Loss  1e-06\n",
            "Epoch [221/250] ---- Training Loss: 0.0161 ---- Validation Loss: 0.0152    L1Loss  1e-06\n",
            "Epoch [222/250] ---- Training Loss: 0.0161 ---- Validation Loss: 0.0151    L1Loss  1e-06\n",
            "Epoch [223/250] ---- Training Loss: 0.0161 ---- Validation Loss: 0.0151    L1Loss  1e-06\n",
            "Epoch [224/250] ---- Training Loss: 0.0161 ---- Validation Loss: 0.0151    L1Loss  1e-06\n",
            "Epoch [225/250] ---- Training Loss: 0.0160 ---- Validation Loss: 0.0151    L1Loss  1e-06\n",
            "Epoch [226/250] ---- Training Loss: 0.0160 ---- Validation Loss: 0.0151    L1Loss  1e-06\n",
            "Epoch [227/250] ---- Training Loss: 0.0160 ---- Validation Loss: 0.0151    L1Loss  1e-06\n",
            "Epoch [228/250] ---- Training Loss: 0.0160 ---- Validation Loss: 0.0151    L1Loss  1e-06\n",
            "Epoch [229/250] ---- Training Loss: 0.0160 ---- Validation Loss: 0.0151    L1Loss  1e-06\n",
            "Epoch [230/250] ---- Training Loss: 0.0160 ---- Validation Loss: 0.0151    L1Loss  1e-06\n",
            "Epoch [231/250] ---- Training Loss: 0.0160 ---- Validation Loss: 0.0151    L1Loss  1e-06\n",
            "Epoch [232/250] ---- Training Loss: 0.0160 ---- Validation Loss: 0.0151    L1Loss  1e-06\n",
            "Epoch [233/250] ---- Training Loss: 0.0160 ---- Validation Loss: 0.0151    L1Loss  1e-06\n",
            "Epoch [234/250] ---- Training Loss: 0.0160 ---- Validation Loss: 0.0150    L1Loss  1e-06\n",
            "Epoch [235/250] ---- Training Loss: 0.0160 ---- Validation Loss: 0.0150    L1Loss  1e-06\n",
            "Epoch [236/250] ---- Training Loss: 0.0160 ---- Validation Loss: 0.0151    L1Loss  1e-06\n",
            "Epoch [237/250] ---- Training Loss: 0.0159 ---- Validation Loss: 0.0150    L1Loss  1e-06\n",
            "Epoch [238/250] ---- Training Loss: 0.0159 ---- Validation Loss: 0.0150    L1Loss  1e-06\n",
            "Epoch [239/250] ---- Training Loss: 0.0159 ---- Validation Loss: 0.0150    L1Loss  1e-06\n",
            "Epoch [240/250] ---- Training Loss: 0.0159 ---- Validation Loss: 0.0150    L1Loss  1e-06\n",
            "Epoch [241/250] ---- Training Loss: 0.0159 ---- Validation Loss: 0.0150    L1Loss  1e-06\n",
            "Epoch [242/250] ---- Training Loss: 0.0159 ---- Validation Loss: 0.0150    L1Loss  1e-06\n",
            "Epoch [243/250] ---- Training Loss: 0.0159 ---- Validation Loss: 0.0150    L1Loss  1e-06\n",
            "Epoch [244/250] ---- Training Loss: 0.0159 ---- Validation Loss: 0.0150    L1Loss  1e-06\n",
            "Epoch [245/250] ---- Training Loss: 0.0159 ---- Validation Loss: 0.0150    L1Loss  1e-06\n",
            "Epoch [246/250] ---- Training Loss: 0.0159 ---- Validation Loss: 0.0149    L1Loss  1e-06\n",
            "Epoch [247/250] ---- Training Loss: 0.0159 ---- Validation Loss: 0.0149    L1Loss  1e-06\n",
            "Epoch [248/250] ---- Training Loss: 0.0159 ---- Validation Loss: 0.0149    L1Loss  1e-06\n",
            "Epoch [249/250] ---- Training Loss: 0.0158 ---- Validation Loss: 0.0149    L1Loss  1e-06\n",
            "Epoch [250/250] ---- Training Loss: 0.0158 ---- Validation Loss: 0.0149    L1Loss  1e-06\n",
            "Saved model: model_loss_L1Loss_wd_1e-06.pth\n",
            "Epoch [1/250] ---- Training Loss: 0.0076 ---- Validation Loss: 0.0049    MSELoss  1e-04\n",
            "Epoch [2/250] ---- Training Loss: 0.0053 ---- Validation Loss: 0.0046    MSELoss  1e-04\n",
            "Epoch [3/250] ---- Training Loss: 0.0051 ---- Validation Loss: 0.0045    MSELoss  1e-04\n",
            "Epoch [4/250] ---- Training Loss: 0.0049 ---- Validation Loss: 0.0043    MSELoss  1e-04\n",
            "Epoch [5/250] ---- Training Loss: 0.0047 ---- Validation Loss: 0.0042    MSELoss  1e-04\n",
            "Epoch [6/250] ---- Training Loss: 0.0045 ---- Validation Loss: 0.0041    MSELoss  1e-04\n",
            "Epoch [7/250] ---- Training Loss: 0.0044 ---- Validation Loss: 0.0039    MSELoss  1e-04\n",
            "Epoch [8/250] ---- Training Loss: 0.0042 ---- Validation Loss: 0.0038    MSELoss  1e-04\n",
            "Epoch [9/250] ---- Training Loss: 0.0041 ---- Validation Loss: 0.0038    MSELoss  1e-04\n",
            "Epoch [10/250] ---- Training Loss: 0.0040 ---- Validation Loss: 0.0037    MSELoss  1e-04\n",
            "Epoch [11/250] ---- Training Loss: 0.0039 ---- Validation Loss: 0.0037    MSELoss  1e-04\n",
            "Epoch [12/250] ---- Training Loss: 0.0038 ---- Validation Loss: 0.0036    MSELoss  1e-04\n",
            "Epoch [13/250] ---- Training Loss: 0.0038 ---- Validation Loss: 0.0035    MSELoss  1e-04\n",
            "Epoch [14/250] ---- Training Loss: 0.0037 ---- Validation Loss: 0.0035    MSELoss  1e-04\n",
            "Epoch [15/250] ---- Training Loss: 0.0036 ---- Validation Loss: 0.0034    MSELoss  1e-04\n",
            "Epoch [16/250] ---- Training Loss: 0.0035 ---- Validation Loss: 0.0034    MSELoss  1e-04\n",
            "Epoch [17/250] ---- Training Loss: 0.0035 ---- Validation Loss: 0.0033    MSELoss  1e-04\n",
            "Epoch [18/250] ---- Training Loss: 0.0034 ---- Validation Loss: 0.0033    MSELoss  1e-04\n",
            "Epoch [19/250] ---- Training Loss: 0.0034 ---- Validation Loss: 0.0033    MSELoss  1e-04\n",
            "Epoch [20/250] ---- Training Loss: 0.0033 ---- Validation Loss: 0.0032    MSELoss  1e-04\n",
            "Epoch [21/250] ---- Training Loss: 0.0033 ---- Validation Loss: 0.0032    MSELoss  1e-04\n",
            "Epoch [22/250] ---- Training Loss: 0.0033 ---- Validation Loss: 0.0032    MSELoss  1e-04\n",
            "Epoch [23/250] ---- Training Loss: 0.0032 ---- Validation Loss: 0.0032    MSELoss  1e-04\n",
            "Epoch [24/250] ---- Training Loss: 0.0032 ---- Validation Loss: 0.0031    MSELoss  1e-04\n",
            "Epoch [25/250] ---- Training Loss: 0.0032 ---- Validation Loss: 0.0031    MSELoss  1e-04\n",
            "Epoch [26/250] ---- Training Loss: 0.0032 ---- Validation Loss: 0.0031    MSELoss  1e-04\n",
            "Epoch [27/250] ---- Training Loss: 0.0032 ---- Validation Loss: 0.0031    MSELoss  1e-04\n",
            "Epoch [28/250] ---- Training Loss: 0.0031 ---- Validation Loss: 0.0031    MSELoss  1e-04\n",
            "Epoch [29/250] ---- Training Loss: 0.0031 ---- Validation Loss: 0.0031    MSELoss  1e-04\n",
            "Epoch [30/250] ---- Training Loss: 0.0031 ---- Validation Loss: 0.0031    MSELoss  1e-04\n",
            "Epoch [31/250] ---- Training Loss: 0.0031 ---- Validation Loss: 0.0031    MSELoss  1e-04\n",
            "Epoch [32/250] ---- Training Loss: 0.0031 ---- Validation Loss: 0.0031    MSELoss  1e-04\n",
            "Epoch [33/250] ---- Training Loss: 0.0031 ---- Validation Loss: 0.0031    MSELoss  1e-04\n",
            "Epoch [34/250] ---- Training Loss: 0.0031 ---- Validation Loss: 0.0030    MSELoss  1e-04\n",
            "Epoch [35/250] ---- Training Loss: 0.0031 ---- Validation Loss: 0.0030    MSELoss  1e-04\n",
            "Epoch [36/250] ---- Training Loss: 0.0031 ---- Validation Loss: 0.0030    MSELoss  1e-04\n",
            "Epoch [37/250] ---- Training Loss: 0.0031 ---- Validation Loss: 0.0030    MSELoss  1e-04\n",
            "Epoch [38/250] ---- Training Loss: 0.0031 ---- Validation Loss: 0.0030    MSELoss  1e-04\n",
            "Epoch [39/250] ---- Training Loss: 0.0031 ---- Validation Loss: 0.0030    MSELoss  1e-04\n",
            "Epoch [40/250] ---- Training Loss: 0.0030 ---- Validation Loss: 0.0030    MSELoss  1e-04\n",
            "Epoch [41/250] ---- Training Loss: 0.0030 ---- Validation Loss: 0.0030    MSELoss  1e-04\n",
            "Epoch [42/250] ---- Training Loss: 0.0030 ---- Validation Loss: 0.0030    MSELoss  1e-04\n",
            "Epoch [43/250] ---- Training Loss: 0.0030 ---- Validation Loss: 0.0030    MSELoss  1e-04\n",
            "Epoch [44/250] ---- Training Loss: 0.0030 ---- Validation Loss: 0.0030    MSELoss  1e-04\n",
            "Epoch [45/250] ---- Training Loss: 0.0030 ---- Validation Loss: 0.0030    MSELoss  1e-04\n",
            "Epoch [46/250] ---- Training Loss: 0.0030 ---- Validation Loss: 0.0030    MSELoss  1e-04\n",
            "Epoch [47/250] ---- Training Loss: 0.0030 ---- Validation Loss: 0.0030    MSELoss  1e-04\n",
            "Epoch [48/250] ---- Training Loss: 0.0030 ---- Validation Loss: 0.0030    MSELoss  1e-04\n",
            "Epoch [49/250] ---- Training Loss: 0.0030 ---- Validation Loss: 0.0030    MSELoss  1e-04\n",
            "Epoch [50/250] ---- Training Loss: 0.0030 ---- Validation Loss: 0.0030    MSELoss  1e-04\n",
            "Epoch [51/250] ---- Training Loss: 0.0030 ---- Validation Loss: 0.0030    MSELoss  1e-04\n",
            "Epoch [52/250] ---- Training Loss: 0.0030 ---- Validation Loss: 0.0030    MSELoss  1e-04\n",
            "Epoch [53/250] ---- Training Loss: 0.0030 ---- Validation Loss: 0.0030    MSELoss  1e-04\n",
            "Epoch [54/250] ---- Training Loss: 0.0030 ---- Validation Loss: 0.0030    MSELoss  1e-04\n",
            "Epoch [55/250] ---- Training Loss: 0.0030 ---- Validation Loss: 0.0030    MSELoss  1e-04\n",
            "Epoch [56/250] ---- Training Loss: 0.0030 ---- Validation Loss: 0.0030    MSELoss  1e-04\n",
            "Epoch [57/250] ---- Training Loss: 0.0030 ---- Validation Loss: 0.0030    MSELoss  1e-04\n",
            "Epoch [58/250] ---- Training Loss: 0.0030 ---- Validation Loss: 0.0030    MSELoss  1e-04\n",
            "Epoch [59/250] ---- Training Loss: 0.0030 ---- Validation Loss: 0.0030    MSELoss  1e-04\n",
            "Epoch [60/250] ---- Training Loss: 0.0030 ---- Validation Loss: 0.0030    MSELoss  1e-04\n",
            "Epoch [61/250] ---- Training Loss: 0.0030 ---- Validation Loss: 0.0030    MSELoss  1e-04\n",
            "Epoch [62/250] ---- Training Loss: 0.0030 ---- Validation Loss: 0.0030    MSELoss  1e-04\n",
            "Epoch [63/250] ---- Training Loss: 0.0030 ---- Validation Loss: 0.0030    MSELoss  1e-04\n",
            "Epoch [64/250] ---- Training Loss: 0.0030 ---- Validation Loss: 0.0030    MSELoss  1e-04\n",
            "Epoch [65/250] ---- Training Loss: 0.0030 ---- Validation Loss: 0.0030    MSELoss  1e-04\n",
            "Epoch [66/250] ---- Training Loss: 0.0030 ---- Validation Loss: 0.0030    MSELoss  1e-04\n",
            "Epoch [67/250] ---- Training Loss: 0.0030 ---- Validation Loss: 0.0030    MSELoss  1e-04\n",
            "Epoch [68/250] ---- Training Loss: 0.0030 ---- Validation Loss: 0.0030    MSELoss  1e-04\n",
            "Epoch [69/250] ---- Training Loss: 0.0030 ---- Validation Loss: 0.0029    MSELoss  1e-04\n",
            "Epoch [70/250] ---- Training Loss: 0.0030 ---- Validation Loss: 0.0029    MSELoss  1e-04\n",
            "Epoch [71/250] ---- Training Loss: 0.0030 ---- Validation Loss: 0.0029    MSELoss  1e-04\n",
            "Epoch [72/250] ---- Training Loss: 0.0030 ---- Validation Loss: 0.0029    MSELoss  1e-04\n",
            "Epoch [73/250] ---- Training Loss: 0.0030 ---- Validation Loss: 0.0029    MSELoss  1e-04\n",
            "Epoch [74/250] ---- Training Loss: 0.0030 ---- Validation Loss: 0.0029    MSELoss  1e-04\n",
            "Epoch [75/250] ---- Training Loss: 0.0030 ---- Validation Loss: 0.0029    MSELoss  1e-04\n",
            "Epoch [76/250] ---- Training Loss: 0.0030 ---- Validation Loss: 0.0029    MSELoss  1e-04\n",
            "Epoch [77/250] ---- Training Loss: 0.0030 ---- Validation Loss: 0.0029    MSELoss  1e-04\n",
            "Epoch [78/250] ---- Training Loss: 0.0029 ---- Validation Loss: 0.0029    MSELoss  1e-04\n",
            "Epoch [79/250] ---- Training Loss: 0.0029 ---- Validation Loss: 0.0029    MSELoss  1e-04\n",
            "Epoch [80/250] ---- Training Loss: 0.0029 ---- Validation Loss: 0.0029    MSELoss  1e-04\n",
            "Epoch [81/250] ---- Training Loss: 0.0029 ---- Validation Loss: 0.0029    MSELoss  1e-04\n",
            "Epoch [82/250] ---- Training Loss: 0.0029 ---- Validation Loss: 0.0029    MSELoss  1e-04\n",
            "Epoch [83/250] ---- Training Loss: 0.0029 ---- Validation Loss: 0.0029    MSELoss  1e-04\n",
            "Epoch [84/250] ---- Training Loss: 0.0029 ---- Validation Loss: 0.0029    MSELoss  1e-04\n",
            "Epoch [85/250] ---- Training Loss: 0.0029 ---- Validation Loss: 0.0029    MSELoss  1e-04\n",
            "Epoch [86/250] ---- Training Loss: 0.0029 ---- Validation Loss: 0.0029    MSELoss  1e-04\n",
            "Epoch [87/250] ---- Training Loss: 0.0029 ---- Validation Loss: 0.0029    MSELoss  1e-04\n",
            "Epoch [88/250] ---- Training Loss: 0.0029 ---- Validation Loss: 0.0029    MSELoss  1e-04\n",
            "Epoch [89/250] ---- Training Loss: 0.0029 ---- Validation Loss: 0.0029    MSELoss  1e-04\n",
            "Epoch [90/250] ---- Training Loss: 0.0029 ---- Validation Loss: 0.0029    MSELoss  1e-04\n",
            "Epoch [91/250] ---- Training Loss: 0.0029 ---- Validation Loss: 0.0029    MSELoss  1e-04\n",
            "Epoch [92/250] ---- Training Loss: 0.0029 ---- Validation Loss: 0.0029    MSELoss  1e-04\n",
            "Epoch [93/250] ---- Training Loss: 0.0029 ---- Validation Loss: 0.0029    MSELoss  1e-04\n",
            "Epoch [94/250] ---- Training Loss: 0.0029 ---- Validation Loss: 0.0029    MSELoss  1e-04\n",
            "Epoch [95/250] ---- Training Loss: 0.0029 ---- Validation Loss: 0.0029    MSELoss  1e-04\n",
            "Epoch [96/250] ---- Training Loss: 0.0029 ---- Validation Loss: 0.0029    MSELoss  1e-04\n",
            "Epoch [97/250] ---- Training Loss: 0.0029 ---- Validation Loss: 0.0029    MSELoss  1e-04\n",
            "Epoch [98/250] ---- Training Loss: 0.0029 ---- Validation Loss: 0.0029    MSELoss  1e-04\n",
            "Epoch [99/250] ---- Training Loss: 0.0029 ---- Validation Loss: 0.0029    MSELoss  1e-04\n",
            "Epoch [100/250] ---- Training Loss: 0.0029 ---- Validation Loss: 0.0029    MSELoss  1e-04\n",
            "Epoch [101/250] ---- Training Loss: 0.0029 ---- Validation Loss: 0.0029    MSELoss  1e-04\n",
            "Epoch [102/250] ---- Training Loss: 0.0029 ---- Validation Loss: 0.0029    MSELoss  1e-04\n",
            "Epoch [103/250] ---- Training Loss: 0.0029 ---- Validation Loss: 0.0029    MSELoss  1e-04\n",
            "Epoch [104/250] ---- Training Loss: 0.0029 ---- Validation Loss: 0.0029    MSELoss  1e-04\n",
            "Epoch [105/250] ---- Training Loss: 0.0029 ---- Validation Loss: 0.0029    MSELoss  1e-04\n",
            "Epoch [106/250] ---- Training Loss: 0.0029 ---- Validation Loss: 0.0029    MSELoss  1e-04\n",
            "Epoch [107/250] ---- Training Loss: 0.0029 ---- Validation Loss: 0.0029    MSELoss  1e-04\n",
            "Epoch [108/250] ---- Training Loss: 0.0029 ---- Validation Loss: 0.0029    MSELoss  1e-04\n",
            "Epoch [109/250] ---- Training Loss: 0.0029 ---- Validation Loss: 0.0029    MSELoss  1e-04\n",
            "Epoch [110/250] ---- Training Loss: 0.0029 ---- Validation Loss: 0.0029    MSELoss  1e-04\n",
            "Epoch [111/250] ---- Training Loss: 0.0029 ---- Validation Loss: 0.0029    MSELoss  1e-04\n",
            "Epoch [112/250] ---- Training Loss: 0.0029 ---- Validation Loss: 0.0029    MSELoss  1e-04\n",
            "Epoch [113/250] ---- Training Loss: 0.0029 ---- Validation Loss: 0.0029    MSELoss  1e-04\n",
            "Epoch [114/250] ---- Training Loss: 0.0029 ---- Validation Loss: 0.0029    MSELoss  1e-04\n",
            "Epoch [115/250] ---- Training Loss: 0.0029 ---- Validation Loss: 0.0029    MSELoss  1e-04\n",
            "Epoch [116/250] ---- Training Loss: 0.0029 ---- Validation Loss: 0.0029    MSELoss  1e-04\n",
            "Epoch [117/250] ---- Training Loss: 0.0029 ---- Validation Loss: 0.0028    MSELoss  1e-04\n",
            "Epoch [118/250] ---- Training Loss: 0.0029 ---- Validation Loss: 0.0028    MSELoss  1e-04\n",
            "Epoch [119/250] ---- Training Loss: 0.0029 ---- Validation Loss: 0.0028    MSELoss  1e-04\n",
            "Epoch [120/250] ---- Training Loss: 0.0029 ---- Validation Loss: 0.0028    MSELoss  1e-04\n",
            "Epoch [121/250] ---- Training Loss: 0.0029 ---- Validation Loss: 0.0028    MSELoss  1e-04\n",
            "Epoch [122/250] ---- Training Loss: 0.0029 ---- Validation Loss: 0.0028    MSELoss  1e-04\n",
            "Epoch [123/250] ---- Training Loss: 0.0029 ---- Validation Loss: 0.0028    MSELoss  1e-04\n",
            "Epoch [124/250] ---- Training Loss: 0.0029 ---- Validation Loss: 0.0028    MSELoss  1e-04\n",
            "Epoch [125/250] ---- Training Loss: 0.0029 ---- Validation Loss: 0.0028    MSELoss  1e-04\n",
            "Epoch [126/250] ---- Training Loss: 0.0029 ---- Validation Loss: 0.0028    MSELoss  1e-04\n",
            "Epoch [127/250] ---- Training Loss: 0.0028 ---- Validation Loss: 0.0028    MSELoss  1e-04\n",
            "Epoch [128/250] ---- Training Loss: 0.0028 ---- Validation Loss: 0.0028    MSELoss  1e-04\n",
            "Epoch [129/250] ---- Training Loss: 0.0028 ---- Validation Loss: 0.0028    MSELoss  1e-04\n",
            "Epoch [130/250] ---- Training Loss: 0.0028 ---- Validation Loss: 0.0028    MSELoss  1e-04\n",
            "Epoch [131/250] ---- Training Loss: 0.0028 ---- Validation Loss: 0.0028    MSELoss  1e-04\n",
            "Epoch [132/250] ---- Training Loss: 0.0028 ---- Validation Loss: 0.0028    MSELoss  1e-04\n",
            "Epoch [133/250] ---- Training Loss: 0.0028 ---- Validation Loss: 0.0028    MSELoss  1e-04\n",
            "Epoch [134/250] ---- Training Loss: 0.0028 ---- Validation Loss: 0.0028    MSELoss  1e-04\n",
            "Epoch [135/250] ---- Training Loss: 0.0028 ---- Validation Loss: 0.0028    MSELoss  1e-04\n",
            "Epoch [136/250] ---- Training Loss: 0.0028 ---- Validation Loss: 0.0028    MSELoss  1e-04\n",
            "Epoch [137/250] ---- Training Loss: 0.0028 ---- Validation Loss: 0.0028    MSELoss  1e-04\n",
            "Epoch [138/250] ---- Training Loss: 0.0028 ---- Validation Loss: 0.0028    MSELoss  1e-04\n",
            "Epoch [139/250] ---- Training Loss: 0.0028 ---- Validation Loss: 0.0028    MSELoss  1e-04\n",
            "Epoch [140/250] ---- Training Loss: 0.0028 ---- Validation Loss: 0.0028    MSELoss  1e-04\n",
            "Epoch [141/250] ---- Training Loss: 0.0028 ---- Validation Loss: 0.0028    MSELoss  1e-04\n",
            "Epoch [142/250] ---- Training Loss: 0.0028 ---- Validation Loss: 0.0028    MSELoss  1e-04\n",
            "Epoch [143/250] ---- Training Loss: 0.0028 ---- Validation Loss: 0.0028    MSELoss  1e-04\n",
            "Epoch [144/250] ---- Training Loss: 0.0028 ---- Validation Loss: 0.0028    MSELoss  1e-04\n",
            "Epoch [145/250] ---- Training Loss: 0.0028 ---- Validation Loss: 0.0028    MSELoss  1e-04\n",
            "Epoch [146/250] ---- Training Loss: 0.0028 ---- Validation Loss: 0.0027    MSELoss  1e-04\n",
            "Epoch [147/250] ---- Training Loss: 0.0028 ---- Validation Loss: 0.0027    MSELoss  1e-04\n",
            "Epoch [148/250] ---- Training Loss: 0.0028 ---- Validation Loss: 0.0027    MSELoss  1e-04\n",
            "Epoch [149/250] ---- Training Loss: 0.0028 ---- Validation Loss: 0.0027    MSELoss  1e-04\n",
            "Epoch [150/250] ---- Training Loss: 0.0028 ---- Validation Loss: 0.0027    MSELoss  1e-04\n",
            "Epoch [151/250] ---- Training Loss: 0.0028 ---- Validation Loss: 0.0027    MSELoss  1e-04\n",
            "Epoch [152/250] ---- Training Loss: 0.0028 ---- Validation Loss: 0.0027    MSELoss  1e-04\n",
            "Epoch [153/250] ---- Training Loss: 0.0028 ---- Validation Loss: 0.0027    MSELoss  1e-04\n",
            "Epoch [154/250] ---- Training Loss: 0.0028 ---- Validation Loss: 0.0027    MSELoss  1e-04\n",
            "Epoch [155/250] ---- Training Loss: 0.0028 ---- Validation Loss: 0.0027    MSELoss  1e-04\n",
            "Epoch [156/250] ---- Training Loss: 0.0028 ---- Validation Loss: 0.0027    MSELoss  1e-04\n",
            "Epoch [157/250] ---- Training Loss: 0.0028 ---- Validation Loss: 0.0027    MSELoss  1e-04\n",
            "Epoch [158/250] ---- Training Loss: 0.0027 ---- Validation Loss: 0.0027    MSELoss  1e-04\n",
            "Epoch [159/250] ---- Training Loss: 0.0027 ---- Validation Loss: 0.0027    MSELoss  1e-04\n",
            "Epoch [160/250] ---- Training Loss: 0.0027 ---- Validation Loss: 0.0027    MSELoss  1e-04\n",
            "Epoch [161/250] ---- Training Loss: 0.0027 ---- Validation Loss: 0.0027    MSELoss  1e-04\n",
            "Epoch [162/250] ---- Training Loss: 0.0027 ---- Validation Loss: 0.0027    MSELoss  1e-04\n",
            "Epoch [163/250] ---- Training Loss: 0.0027 ---- Validation Loss: 0.0027    MSELoss  1e-04\n",
            "Epoch [164/250] ---- Training Loss: 0.0027 ---- Validation Loss: 0.0027    MSELoss  1e-04\n",
            "Epoch [165/250] ---- Training Loss: 0.0027 ---- Validation Loss: 0.0027    MSELoss  1e-04\n",
            "Epoch [166/250] ---- Training Loss: 0.0027 ---- Validation Loss: 0.0027    MSELoss  1e-04\n",
            "Epoch [167/250] ---- Training Loss: 0.0027 ---- Validation Loss: 0.0027    MSELoss  1e-04\n",
            "Epoch [168/250] ---- Training Loss: 0.0027 ---- Validation Loss: 0.0027    MSELoss  1e-04\n",
            "Epoch [169/250] ---- Training Loss: 0.0027 ---- Validation Loss: 0.0026    MSELoss  1e-04\n",
            "Epoch [170/250] ---- Training Loss: 0.0027 ---- Validation Loss: 0.0026    MSELoss  1e-04\n",
            "Epoch [171/250] ---- Training Loss: 0.0027 ---- Validation Loss: 0.0026    MSELoss  1e-04\n",
            "Epoch [172/250] ---- Training Loss: 0.0027 ---- Validation Loss: 0.0026    MSELoss  1e-04\n",
            "Epoch [173/250] ---- Training Loss: 0.0027 ---- Validation Loss: 0.0026    MSELoss  1e-04\n",
            "Epoch [174/250] ---- Training Loss: 0.0027 ---- Validation Loss: 0.0026    MSELoss  1e-04\n",
            "Epoch [175/250] ---- Training Loss: 0.0027 ---- Validation Loss: 0.0026    MSELoss  1e-04\n",
            "Epoch [176/250] ---- Training Loss: 0.0027 ---- Validation Loss: 0.0026    MSELoss  1e-04\n",
            "Epoch [177/250] ---- Training Loss: 0.0027 ---- Validation Loss: 0.0026    MSELoss  1e-04\n",
            "Epoch [178/250] ---- Training Loss: 0.0027 ---- Validation Loss: 0.0026    MSELoss  1e-04\n",
            "Epoch [179/250] ---- Training Loss: 0.0027 ---- Validation Loss: 0.0026    MSELoss  1e-04\n",
            "Epoch [180/250] ---- Training Loss: 0.0027 ---- Validation Loss: 0.0026    MSELoss  1e-04\n",
            "Epoch [181/250] ---- Training Loss: 0.0027 ---- Validation Loss: 0.0026    MSELoss  1e-04\n",
            "Epoch [182/250] ---- Training Loss: 0.0027 ---- Validation Loss: 0.0026    MSELoss  1e-04\n",
            "Epoch [183/250] ---- Training Loss: 0.0027 ---- Validation Loss: 0.0026    MSELoss  1e-04\n",
            "Epoch [184/250] ---- Training Loss: 0.0027 ---- Validation Loss: 0.0026    MSELoss  1e-04\n",
            "Epoch [185/250] ---- Training Loss: 0.0026 ---- Validation Loss: 0.0026    MSELoss  1e-04\n",
            "Epoch [186/250] ---- Training Loss: 0.0026 ---- Validation Loss: 0.0026    MSELoss  1e-04\n",
            "Epoch [187/250] ---- Training Loss: 0.0026 ---- Validation Loss: 0.0026    MSELoss  1e-04\n",
            "Epoch [188/250] ---- Training Loss: 0.0026 ---- Validation Loss: 0.0026    MSELoss  1e-04\n",
            "Epoch [189/250] ---- Training Loss: 0.0026 ---- Validation Loss: 0.0026    MSELoss  1e-04\n",
            "Epoch [190/250] ---- Training Loss: 0.0026 ---- Validation Loss: 0.0026    MSELoss  1e-04\n",
            "Epoch [191/250] ---- Training Loss: 0.0026 ---- Validation Loss: 0.0026    MSELoss  1e-04\n",
            "Epoch [192/250] ---- Training Loss: 0.0026 ---- Validation Loss: 0.0025    MSELoss  1e-04\n",
            "Epoch [193/250] ---- Training Loss: 0.0026 ---- Validation Loss: 0.0025    MSELoss  1e-04\n",
            "Epoch [194/250] ---- Training Loss: 0.0026 ---- Validation Loss: 0.0025    MSELoss  1e-04\n",
            "Epoch [195/250] ---- Training Loss: 0.0026 ---- Validation Loss: 0.0025    MSELoss  1e-04\n",
            "Epoch [196/250] ---- Training Loss: 0.0026 ---- Validation Loss: 0.0025    MSELoss  1e-04\n",
            "Epoch [197/250] ---- Training Loss: 0.0026 ---- Validation Loss: 0.0025    MSELoss  1e-04\n",
            "Epoch [198/250] ---- Training Loss: 0.0026 ---- Validation Loss: 0.0025    MSELoss  1e-04\n",
            "Epoch [199/250] ---- Training Loss: 0.0026 ---- Validation Loss: 0.0025    MSELoss  1e-04\n",
            "Epoch [200/250] ---- Training Loss: 0.0026 ---- Validation Loss: 0.0025    MSELoss  1e-04\n",
            "Epoch [201/250] ---- Training Loss: 0.0026 ---- Validation Loss: 0.0025    MSELoss  1e-04\n",
            "Epoch [202/250] ---- Training Loss: 0.0026 ---- Validation Loss: 0.0025    MSELoss  1e-04\n",
            "Epoch [203/250] ---- Training Loss: 0.0026 ---- Validation Loss: 0.0025    MSELoss  1e-04\n",
            "Epoch [204/250] ---- Training Loss: 0.0026 ---- Validation Loss: 0.0025    MSELoss  1e-04\n",
            "Epoch [205/250] ---- Training Loss: 0.0026 ---- Validation Loss: 0.0025    MSELoss  1e-04\n",
            "Epoch [206/250] ---- Training Loss: 0.0026 ---- Validation Loss: 0.0025    MSELoss  1e-04\n",
            "Epoch [207/250] ---- Training Loss: 0.0026 ---- Validation Loss: 0.0025    MSELoss  1e-04\n",
            "Epoch [208/250] ---- Training Loss: 0.0026 ---- Validation Loss: 0.0025    MSELoss  1e-04\n",
            "Epoch [209/250] ---- Training Loss: 0.0026 ---- Validation Loss: 0.0025    MSELoss  1e-04\n",
            "Epoch [210/250] ---- Training Loss: 0.0026 ---- Validation Loss: 0.0025    MSELoss  1e-04\n",
            "Epoch [211/250] ---- Training Loss: 0.0026 ---- Validation Loss: 0.0025    MSELoss  1e-04\n",
            "Epoch [212/250] ---- Training Loss: 0.0026 ---- Validation Loss: 0.0025    MSELoss  1e-04\n",
            "Epoch [213/250] ---- Training Loss: 0.0026 ---- Validation Loss: 0.0025    MSELoss  1e-04\n",
            "Epoch [214/250] ---- Training Loss: 0.0025 ---- Validation Loss: 0.0025    MSELoss  1e-04\n",
            "Epoch [215/250] ---- Training Loss: 0.0025 ---- Validation Loss: 0.0025    MSELoss  1e-04\n",
            "Epoch [216/250] ---- Training Loss: 0.0025 ---- Validation Loss: 0.0025    MSELoss  1e-04\n",
            "Epoch [217/250] ---- Training Loss: 0.0025 ---- Validation Loss: 0.0025    MSELoss  1e-04\n",
            "Epoch [218/250] ---- Training Loss: 0.0025 ---- Validation Loss: 0.0025    MSELoss  1e-04\n",
            "Epoch [219/250] ---- Training Loss: 0.0025 ---- Validation Loss: 0.0024    MSELoss  1e-04\n",
            "Epoch [220/250] ---- Training Loss: 0.0025 ---- Validation Loss: 0.0024    MSELoss  1e-04\n",
            "Epoch [221/250] ---- Training Loss: 0.0025 ---- Validation Loss: 0.0024    MSELoss  1e-04\n",
            "Epoch [222/250] ---- Training Loss: 0.0025 ---- Validation Loss: 0.0024    MSELoss  1e-04\n",
            "Epoch [223/250] ---- Training Loss: 0.0025 ---- Validation Loss: 0.0024    MSELoss  1e-04\n",
            "Epoch [224/250] ---- Training Loss: 0.0025 ---- Validation Loss: 0.0024    MSELoss  1e-04\n",
            "Epoch [225/250] ---- Training Loss: 0.0025 ---- Validation Loss: 0.0024    MSELoss  1e-04\n",
            "Epoch [226/250] ---- Training Loss: 0.0025 ---- Validation Loss: 0.0024    MSELoss  1e-04\n",
            "Epoch [227/250] ---- Training Loss: 0.0025 ---- Validation Loss: 0.0024    MSELoss  1e-04\n",
            "Epoch [228/250] ---- Training Loss: 0.0025 ---- Validation Loss: 0.0024    MSELoss  1e-04\n",
            "Epoch [229/250] ---- Training Loss: 0.0025 ---- Validation Loss: 0.0024    MSELoss  1e-04\n",
            "Epoch [230/250] ---- Training Loss: 0.0025 ---- Validation Loss: 0.0024    MSELoss  1e-04\n",
            "Epoch [231/250] ---- Training Loss: 0.0025 ---- Validation Loss: 0.0024    MSELoss  1e-04\n",
            "Epoch [232/250] ---- Training Loss: 0.0025 ---- Validation Loss: 0.0024    MSELoss  1e-04\n",
            "Epoch [233/250] ---- Training Loss: 0.0025 ---- Validation Loss: 0.0024    MSELoss  1e-04\n",
            "Epoch [234/250] ---- Training Loss: 0.0025 ---- Validation Loss: 0.0024    MSELoss  1e-04\n",
            "Epoch [235/250] ---- Training Loss: 0.0025 ---- Validation Loss: 0.0024    MSELoss  1e-04\n",
            "Epoch [236/250] ---- Training Loss: 0.0025 ---- Validation Loss: 0.0024    MSELoss  1e-04\n",
            "Epoch [237/250] ---- Training Loss: 0.0025 ---- Validation Loss: 0.0024    MSELoss  1e-04\n",
            "Epoch [238/250] ---- Training Loss: 0.0025 ---- Validation Loss: 0.0024    MSELoss  1e-04\n",
            "Epoch [239/250] ---- Training Loss: 0.0025 ---- Validation Loss: 0.0024    MSELoss  1e-04\n",
            "Epoch [240/250] ---- Training Loss: 0.0025 ---- Validation Loss: 0.0024    MSELoss  1e-04\n",
            "Epoch [241/250] ---- Training Loss: 0.0025 ---- Validation Loss: 0.0024    MSELoss  1e-04\n",
            "Epoch [242/250] ---- Training Loss: 0.0025 ---- Validation Loss: 0.0024    MSELoss  1e-04\n",
            "Epoch [243/250] ---- Training Loss: 0.0025 ---- Validation Loss: 0.0024    MSELoss  1e-04\n",
            "Epoch [244/250] ---- Training Loss: 0.0025 ---- Validation Loss: 0.0024    MSELoss  1e-04\n",
            "Epoch [245/250] ---- Training Loss: 0.0025 ---- Validation Loss: 0.0024    MSELoss  1e-04\n",
            "Epoch [246/250] ---- Training Loss: 0.0025 ---- Validation Loss: 0.0023    MSELoss  1e-04\n",
            "Epoch [247/250] ---- Training Loss: 0.0024 ---- Validation Loss: 0.0023    MSELoss  1e-04\n",
            "Epoch [248/250] ---- Training Loss: 0.0024 ---- Validation Loss: 0.0023    MSELoss  1e-04\n",
            "Epoch [249/250] ---- Training Loss: 0.0024 ---- Validation Loss: 0.0023    MSELoss  1e-04\n",
            "Epoch [250/250] ---- Training Loss: 0.0024 ---- Validation Loss: 0.0023    MSELoss  1e-04\n",
            "Saved model: model_loss_MSELoss_wd_1e-04.pth\n",
            "Epoch [1/250] ---- Training Loss: 0.0055 ---- Validation Loss: 0.0045    MSELoss  1e-05\n",
            "Epoch [2/250] ---- Training Loss: 0.0049 ---- Validation Loss: 0.0043    MSELoss  1e-05\n",
            "Epoch [3/250] ---- Training Loss: 0.0047 ---- Validation Loss: 0.0041    MSELoss  1e-05\n",
            "Epoch [4/250] ---- Training Loss: 0.0045 ---- Validation Loss: 0.0040    MSELoss  1e-05\n",
            "Epoch [5/250] ---- Training Loss: 0.0043 ---- Validation Loss: 0.0039    MSELoss  1e-05\n",
            "Epoch [6/250] ---- Training Loss: 0.0041 ---- Validation Loss: 0.0038    MSELoss  1e-05\n",
            "Epoch [7/250] ---- Training Loss: 0.0040 ---- Validation Loss: 0.0037    MSELoss  1e-05\n",
            "Epoch [8/250] ---- Training Loss: 0.0039 ---- Validation Loss: 0.0036    MSELoss  1e-05\n",
            "Epoch [9/250] ---- Training Loss: 0.0037 ---- Validation Loss: 0.0035    MSELoss  1e-05\n",
            "Epoch [10/250] ---- Training Loss: 0.0036 ---- Validation Loss: 0.0034    MSELoss  1e-05\n",
            "Epoch [11/250] ---- Training Loss: 0.0035 ---- Validation Loss: 0.0033    MSELoss  1e-05\n",
            "Epoch [12/250] ---- Training Loss: 0.0034 ---- Validation Loss: 0.0032    MSELoss  1e-05\n",
            "Epoch [13/250] ---- Training Loss: 0.0034 ---- Validation Loss: 0.0032    MSELoss  1e-05\n",
            "Epoch [14/250] ---- Training Loss: 0.0033 ---- Validation Loss: 0.0031    MSELoss  1e-05\n",
            "Epoch [15/250] ---- Training Loss: 0.0033 ---- Validation Loss: 0.0031    MSELoss  1e-05\n",
            "Epoch [16/250] ---- Training Loss: 0.0032 ---- Validation Loss: 0.0031    MSELoss  1e-05\n",
            "Epoch [17/250] ---- Training Loss: 0.0032 ---- Validation Loss: 0.0031    MSELoss  1e-05\n",
            "Epoch [18/250] ---- Training Loss: 0.0032 ---- Validation Loss: 0.0030    MSELoss  1e-05\n",
            "Epoch [19/250] ---- Training Loss: 0.0031 ---- Validation Loss: 0.0030    MSELoss  1e-05\n",
            "Epoch [20/250] ---- Training Loss: 0.0031 ---- Validation Loss: 0.0030    MSELoss  1e-05\n",
            "Epoch [21/250] ---- Training Loss: 0.0031 ---- Validation Loss: 0.0030    MSELoss  1e-05\n",
            "Epoch [22/250] ---- Training Loss: 0.0030 ---- Validation Loss: 0.0029    MSELoss  1e-05\n",
            "Epoch [23/250] ---- Training Loss: 0.0030 ---- Validation Loss: 0.0029    MSELoss  1e-05\n",
            "Epoch [24/250] ---- Training Loss: 0.0030 ---- Validation Loss: 0.0029    MSELoss  1e-05\n",
            "Epoch [25/250] ---- Training Loss: 0.0030 ---- Validation Loss: 0.0029    MSELoss  1e-05\n",
            "Epoch [26/250] ---- Training Loss: 0.0030 ---- Validation Loss: 0.0029    MSELoss  1e-05\n",
            "Epoch [27/250] ---- Training Loss: 0.0029 ---- Validation Loss: 0.0029    MSELoss  1e-05\n",
            "Epoch [28/250] ---- Training Loss: 0.0029 ---- Validation Loss: 0.0028    MSELoss  1e-05\n",
            "Epoch [29/250] ---- Training Loss: 0.0029 ---- Validation Loss: 0.0028    MSELoss  1e-05\n",
            "Epoch [30/250] ---- Training Loss: 0.0029 ---- Validation Loss: 0.0028    MSELoss  1e-05\n",
            "Epoch [31/250] ---- Training Loss: 0.0029 ---- Validation Loss: 0.0028    MSELoss  1e-05\n",
            "Epoch [32/250] ---- Training Loss: 0.0029 ---- Validation Loss: 0.0028    MSELoss  1e-05\n",
            "Epoch [33/250] ---- Training Loss: 0.0028 ---- Validation Loss: 0.0028    MSELoss  1e-05\n",
            "Epoch [34/250] ---- Training Loss: 0.0028 ---- Validation Loss: 0.0027    MSELoss  1e-05\n",
            "Epoch [35/250] ---- Training Loss: 0.0028 ---- Validation Loss: 0.0027    MSELoss  1e-05\n",
            "Epoch [36/250] ---- Training Loss: 0.0028 ---- Validation Loss: 0.0027    MSELoss  1e-05\n",
            "Epoch [37/250] ---- Training Loss: 0.0028 ---- Validation Loss: 0.0027    MSELoss  1e-05\n",
            "Epoch [38/250] ---- Training Loss: 0.0027 ---- Validation Loss: 0.0026    MSELoss  1e-05\n",
            "Epoch [39/250] ---- Training Loss: 0.0027 ---- Validation Loss: 0.0026    MSELoss  1e-05\n",
            "Epoch [40/250] ---- Training Loss: 0.0027 ---- Validation Loss: 0.0026    MSELoss  1e-05\n",
            "Epoch [41/250] ---- Training Loss: 0.0026 ---- Validation Loss: 0.0025    MSELoss  1e-05\n",
            "Epoch [42/250] ---- Training Loss: 0.0026 ---- Validation Loss: 0.0025    MSELoss  1e-05\n",
            "Epoch [43/250] ---- Training Loss: 0.0026 ---- Validation Loss: 0.0024    MSELoss  1e-05\n",
            "Epoch [44/250] ---- Training Loss: 0.0025 ---- Validation Loss: 0.0024    MSELoss  1e-05\n",
            "Epoch [45/250] ---- Training Loss: 0.0025 ---- Validation Loss: 0.0024    MSELoss  1e-05\n",
            "Epoch [46/250] ---- Training Loss: 0.0025 ---- Validation Loss: 0.0023    MSELoss  1e-05\n",
            "Epoch [47/250] ---- Training Loss: 0.0024 ---- Validation Loss: 0.0023    MSELoss  1e-05\n",
            "Epoch [48/250] ---- Training Loss: 0.0024 ---- Validation Loss: 0.0023    MSELoss  1e-05\n",
            "Epoch [49/250] ---- Training Loss: 0.0024 ---- Validation Loss: 0.0023    MSELoss  1e-05\n",
            "Epoch [50/250] ---- Training Loss: 0.0024 ---- Validation Loss: 0.0022    MSELoss  1e-05\n",
            "Epoch [51/250] ---- Training Loss: 0.0023 ---- Validation Loss: 0.0022    MSELoss  1e-05\n",
            "Epoch [52/250] ---- Training Loss: 0.0023 ---- Validation Loss: 0.0022    MSELoss  1e-05\n",
            "Epoch [53/250] ---- Training Loss: 0.0023 ---- Validation Loss: 0.0022    MSELoss  1e-05\n",
            "Epoch [54/250] ---- Training Loss: 0.0023 ---- Validation Loss: 0.0021    MSELoss  1e-05\n",
            "Epoch [55/250] ---- Training Loss: 0.0022 ---- Validation Loss: 0.0021    MSELoss  1e-05\n",
            "Epoch [56/250] ---- Training Loss: 0.0022 ---- Validation Loss: 0.0021    MSELoss  1e-05\n",
            "Epoch [57/250] ---- Training Loss: 0.0022 ---- Validation Loss: 0.0021    MSELoss  1e-05\n",
            "Epoch [58/250] ---- Training Loss: 0.0022 ---- Validation Loss: 0.0021    MSELoss  1e-05\n",
            "Epoch [59/250] ---- Training Loss: 0.0022 ---- Validation Loss: 0.0020    MSELoss  1e-05\n",
            "Epoch [60/250] ---- Training Loss: 0.0021 ---- Validation Loss: 0.0020    MSELoss  1e-05\n",
            "Epoch [61/250] ---- Training Loss: 0.0021 ---- Validation Loss: 0.0020    MSELoss  1e-05\n",
            "Epoch [62/250] ---- Training Loss: 0.0021 ---- Validation Loss: 0.0020    MSELoss  1e-05\n",
            "Epoch [63/250] ---- Training Loss: 0.0021 ---- Validation Loss: 0.0019    MSELoss  1e-05\n",
            "Epoch [64/250] ---- Training Loss: 0.0021 ---- Validation Loss: 0.0019    MSELoss  1e-05\n",
            "Epoch [65/250] ---- Training Loss: 0.0020 ---- Validation Loss: 0.0019    MSELoss  1e-05\n",
            "Epoch [66/250] ---- Training Loss: 0.0020 ---- Validation Loss: 0.0019    MSELoss  1e-05\n",
            "Epoch [67/250] ---- Training Loss: 0.0020 ---- Validation Loss: 0.0019    MSELoss  1e-05\n",
            "Epoch [68/250] ---- Training Loss: 0.0020 ---- Validation Loss: 0.0018    MSELoss  1e-05\n",
            "Epoch [69/250] ---- Training Loss: 0.0020 ---- Validation Loss: 0.0018    MSELoss  1e-05\n",
            "Epoch [70/250] ---- Training Loss: 0.0020 ---- Validation Loss: 0.0018    MSELoss  1e-05\n",
            "Epoch [71/250] ---- Training Loss: 0.0019 ---- Validation Loss: 0.0018    MSELoss  1e-05\n",
            "Epoch [72/250] ---- Training Loss: 0.0019 ---- Validation Loss: 0.0018    MSELoss  1e-05\n",
            "Epoch [73/250] ---- Training Loss: 0.0019 ---- Validation Loss: 0.0018    MSELoss  1e-05\n",
            "Epoch [74/250] ---- Training Loss: 0.0019 ---- Validation Loss: 0.0017    MSELoss  1e-05\n",
            "Epoch [75/250] ---- Training Loss: 0.0019 ---- Validation Loss: 0.0017    MSELoss  1e-05\n",
            "Epoch [76/250] ---- Training Loss: 0.0019 ---- Validation Loss: 0.0017    MSELoss  1e-05\n",
            "Epoch [77/250] ---- Training Loss: 0.0018 ---- Validation Loss: 0.0017    MSELoss  1e-05\n",
            "Epoch [78/250] ---- Training Loss: 0.0018 ---- Validation Loss: 0.0017    MSELoss  1e-05\n",
            "Epoch [79/250] ---- Training Loss: 0.0018 ---- Validation Loss: 0.0017    MSELoss  1e-05\n",
            "Epoch [80/250] ---- Training Loss: 0.0018 ---- Validation Loss: 0.0016    MSELoss  1e-05\n",
            "Epoch [81/250] ---- Training Loss: 0.0018 ---- Validation Loss: 0.0016    MSELoss  1e-05\n",
            "Epoch [82/250] ---- Training Loss: 0.0018 ---- Validation Loss: 0.0016    MSELoss  1e-05\n",
            "Epoch [83/250] ---- Training Loss: 0.0017 ---- Validation Loss: 0.0016    MSELoss  1e-05\n",
            "Epoch [84/250] ---- Training Loss: 0.0017 ---- Validation Loss: 0.0016    MSELoss  1e-05\n",
            "Epoch [85/250] ---- Training Loss: 0.0017 ---- Validation Loss: 0.0016    MSELoss  1e-05\n",
            "Epoch [86/250] ---- Training Loss: 0.0017 ---- Validation Loss: 0.0016    MSELoss  1e-05\n",
            "Epoch [87/250] ---- Training Loss: 0.0017 ---- Validation Loss: 0.0015    MSELoss  1e-05\n",
            "Epoch [88/250] ---- Training Loss: 0.0017 ---- Validation Loss: 0.0015    MSELoss  1e-05\n",
            "Epoch [89/250] ---- Training Loss: 0.0017 ---- Validation Loss: 0.0015    MSELoss  1e-05\n",
            "Epoch [90/250] ---- Training Loss: 0.0017 ---- Validation Loss: 0.0015    MSELoss  1e-05\n",
            "Epoch [91/250] ---- Training Loss: 0.0016 ---- Validation Loss: 0.0015    MSELoss  1e-05\n",
            "Epoch [92/250] ---- Training Loss: 0.0016 ---- Validation Loss: 0.0015    MSELoss  1e-05\n",
            "Epoch [93/250] ---- Training Loss: 0.0016 ---- Validation Loss: 0.0015    MSELoss  1e-05\n",
            "Epoch [94/250] ---- Training Loss: 0.0016 ---- Validation Loss: 0.0015    MSELoss  1e-05\n",
            "Epoch [95/250] ---- Training Loss: 0.0016 ---- Validation Loss: 0.0015    MSELoss  1e-05\n",
            "Epoch [96/250] ---- Training Loss: 0.0016 ---- Validation Loss: 0.0014    MSELoss  1e-05\n",
            "Epoch [97/250] ---- Training Loss: 0.0016 ---- Validation Loss: 0.0014    MSELoss  1e-05\n",
            "Epoch [98/250] ---- Training Loss: 0.0016 ---- Validation Loss: 0.0014    MSELoss  1e-05\n",
            "Epoch [99/250] ---- Training Loss: 0.0016 ---- Validation Loss: 0.0014    MSELoss  1e-05\n",
            "Epoch [100/250] ---- Training Loss: 0.0016 ---- Validation Loss: 0.0014    MSELoss  1e-05\n",
            "Epoch [101/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0014    MSELoss  1e-05\n",
            "Epoch [102/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0014    MSELoss  1e-05\n",
            "Epoch [103/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0014    MSELoss  1e-05\n",
            "Epoch [104/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0014    MSELoss  1e-05\n",
            "Epoch [105/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0014    MSELoss  1e-05\n",
            "Epoch [106/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0014    MSELoss  1e-05\n",
            "Epoch [107/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0014    MSELoss  1e-05\n",
            "Epoch [108/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0013    MSELoss  1e-05\n",
            "Epoch [109/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0013    MSELoss  1e-05\n",
            "Epoch [110/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0013    MSELoss  1e-05\n",
            "Epoch [111/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0013    MSELoss  1e-05\n",
            "Epoch [112/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0013    MSELoss  1e-05\n",
            "Epoch [113/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0013    MSELoss  1e-05\n",
            "Epoch [114/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0013    MSELoss  1e-05\n",
            "Epoch [115/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0013    MSELoss  1e-05\n",
            "Epoch [116/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0013    MSELoss  1e-05\n",
            "Epoch [117/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0013    MSELoss  1e-05\n",
            "Epoch [118/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0013    MSELoss  1e-05\n",
            "Epoch [119/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0013    MSELoss  1e-05\n",
            "Epoch [120/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0013    MSELoss  1e-05\n",
            "Epoch [121/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0013    MSELoss  1e-05\n",
            "Epoch [122/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0012    MSELoss  1e-05\n",
            "Epoch [123/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0012    MSELoss  1e-05\n",
            "Epoch [124/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0012    MSELoss  1e-05\n",
            "Epoch [125/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0012    MSELoss  1e-05\n",
            "Epoch [126/250] ---- Training Loss: 0.0013 ---- Validation Loss: 0.0012    MSELoss  1e-05\n",
            "Epoch [127/250] ---- Training Loss: 0.0013 ---- Validation Loss: 0.0012    MSELoss  1e-05\n",
            "Epoch [128/250] ---- Training Loss: 0.0013 ---- Validation Loss: 0.0012    MSELoss  1e-05\n",
            "Epoch [129/250] ---- Training Loss: 0.0013 ---- Validation Loss: 0.0012    MSELoss  1e-05\n",
            "Epoch [130/250] ---- Training Loss: 0.0013 ---- Validation Loss: 0.0012    MSELoss  1e-05\n",
            "Epoch [131/250] ---- Training Loss: 0.0013 ---- Validation Loss: 0.0012    MSELoss  1e-05\n",
            "Epoch [132/250] ---- Training Loss: 0.0013 ---- Validation Loss: 0.0012    MSELoss  1e-05\n",
            "Epoch [133/250] ---- Training Loss: 0.0013 ---- Validation Loss: 0.0012    MSELoss  1e-05\n",
            "Epoch [134/250] ---- Training Loss: 0.0013 ---- Validation Loss: 0.0012    MSELoss  1e-05\n",
            "Epoch [135/250] ---- Training Loss: 0.0013 ---- Validation Loss: 0.0012    MSELoss  1e-05\n",
            "Epoch [136/250] ---- Training Loss: 0.0013 ---- Validation Loss: 0.0012    MSELoss  1e-05\n",
            "Epoch [137/250] ---- Training Loss: 0.0013 ---- Validation Loss: 0.0012    MSELoss  1e-05\n",
            "Epoch [138/250] ---- Training Loss: 0.0013 ---- Validation Loss: 0.0012    MSELoss  1e-05\n",
            "Epoch [139/250] ---- Training Loss: 0.0013 ---- Validation Loss: 0.0012    MSELoss  1e-05\n",
            "Epoch [140/250] ---- Training Loss: 0.0013 ---- Validation Loss: 0.0011    MSELoss  1e-05\n",
            "Epoch [141/250] ---- Training Loss: 0.0013 ---- Validation Loss: 0.0011    MSELoss  1e-05\n",
            "Epoch [142/250] ---- Training Loss: 0.0013 ---- Validation Loss: 0.0011    MSELoss  1e-05\n",
            "Epoch [143/250] ---- Training Loss: 0.0013 ---- Validation Loss: 0.0011    MSELoss  1e-05\n",
            "Epoch [144/250] ---- Training Loss: 0.0013 ---- Validation Loss: 0.0011    MSELoss  1e-05\n",
            "Epoch [145/250] ---- Training Loss: 0.0012 ---- Validation Loss: 0.0011    MSELoss  1e-05\n",
            "Epoch [146/250] ---- Training Loss: 0.0012 ---- Validation Loss: 0.0011    MSELoss  1e-05\n",
            "Epoch [147/250] ---- Training Loss: 0.0012 ---- Validation Loss: 0.0011    MSELoss  1e-05\n",
            "Epoch [148/250] ---- Training Loss: 0.0012 ---- Validation Loss: 0.0011    MSELoss  1e-05\n",
            "Epoch [149/250] ---- Training Loss: 0.0012 ---- Validation Loss: 0.0011    MSELoss  1e-05\n",
            "Epoch [150/250] ---- Training Loss: 0.0012 ---- Validation Loss: 0.0011    MSELoss  1e-05\n",
            "Epoch [151/250] ---- Training Loss: 0.0012 ---- Validation Loss: 0.0011    MSELoss  1e-05\n",
            "Epoch [152/250] ---- Training Loss: 0.0012 ---- Validation Loss: 0.0011    MSELoss  1e-05\n",
            "Epoch [153/250] ---- Training Loss: 0.0012 ---- Validation Loss: 0.0011    MSELoss  1e-05\n",
            "Epoch [154/250] ---- Training Loss: 0.0012 ---- Validation Loss: 0.0011    MSELoss  1e-05\n",
            "Epoch [155/250] ---- Training Loss: 0.0012 ---- Validation Loss: 0.0011    MSELoss  1e-05\n",
            "Epoch [156/250] ---- Training Loss: 0.0012 ---- Validation Loss: 0.0011    MSELoss  1e-05\n",
            "Epoch [157/250] ---- Training Loss: 0.0012 ---- Validation Loss: 0.0011    MSELoss  1e-05\n",
            "Epoch [158/250] ---- Training Loss: 0.0012 ---- Validation Loss: 0.0011    MSELoss  1e-05\n",
            "Epoch [159/250] ---- Training Loss: 0.0012 ---- Validation Loss: 0.0011    MSELoss  1e-05\n",
            "Epoch [160/250] ---- Training Loss: 0.0012 ---- Validation Loss: 0.0011    MSELoss  1e-05\n",
            "Epoch [161/250] ---- Training Loss: 0.0012 ---- Validation Loss: 0.0011    MSELoss  1e-05\n",
            "Epoch [162/250] ---- Training Loss: 0.0012 ---- Validation Loss: 0.0011    MSELoss  1e-05\n",
            "Epoch [163/250] ---- Training Loss: 0.0012 ---- Validation Loss: 0.0011    MSELoss  1e-05\n",
            "Epoch [164/250] ---- Training Loss: 0.0012 ---- Validation Loss: 0.0011    MSELoss  1e-05\n",
            "Epoch [165/250] ---- Training Loss: 0.0012 ---- Validation Loss: 0.0011    MSELoss  1e-05\n",
            "Epoch [166/250] ---- Training Loss: 0.0012 ---- Validation Loss: 0.0010    MSELoss  1e-05\n",
            "Epoch [167/250] ---- Training Loss: 0.0012 ---- Validation Loss: 0.0010    MSELoss  1e-05\n",
            "Epoch [168/250] ---- Training Loss: 0.0012 ---- Validation Loss: 0.0010    MSELoss  1e-05\n",
            "Epoch [169/250] ---- Training Loss: 0.0012 ---- Validation Loss: 0.0010    MSELoss  1e-05\n",
            "Epoch [170/250] ---- Training Loss: 0.0012 ---- Validation Loss: 0.0010    MSELoss  1e-05\n",
            "Epoch [171/250] ---- Training Loss: 0.0012 ---- Validation Loss: 0.0010    MSELoss  1e-05\n",
            "Epoch [172/250] ---- Training Loss: 0.0011 ---- Validation Loss: 0.0010    MSELoss  1e-05\n",
            "Epoch [173/250] ---- Training Loss: 0.0011 ---- Validation Loss: 0.0010    MSELoss  1e-05\n",
            "Epoch [174/250] ---- Training Loss: 0.0011 ---- Validation Loss: 0.0010    MSELoss  1e-05\n",
            "Epoch [175/250] ---- Training Loss: 0.0011 ---- Validation Loss: 0.0010    MSELoss  1e-05\n",
            "Epoch [176/250] ---- Training Loss: 0.0011 ---- Validation Loss: 0.0010    MSELoss  1e-05\n",
            "Epoch [177/250] ---- Training Loss: 0.0011 ---- Validation Loss: 0.0010    MSELoss  1e-05\n",
            "Epoch [178/250] ---- Training Loss: 0.0011 ---- Validation Loss: 0.0010    MSELoss  1e-05\n",
            "Epoch [179/250] ---- Training Loss: 0.0011 ---- Validation Loss: 0.0010    MSELoss  1e-05\n",
            "Epoch [180/250] ---- Training Loss: 0.0011 ---- Validation Loss: 0.0010    MSELoss  1e-05\n",
            "Epoch [181/250] ---- Training Loss: 0.0011 ---- Validation Loss: 0.0010    MSELoss  1e-05\n",
            "Epoch [182/250] ---- Training Loss: 0.0011 ---- Validation Loss: 0.0010    MSELoss  1e-05\n",
            "Epoch [183/250] ---- Training Loss: 0.0011 ---- Validation Loss: 0.0010    MSELoss  1e-05\n",
            "Epoch [184/250] ---- Training Loss: 0.0011 ---- Validation Loss: 0.0010    MSELoss  1e-05\n",
            "Epoch [185/250] ---- Training Loss: 0.0011 ---- Validation Loss: 0.0010    MSELoss  1e-05\n",
            "Epoch [186/250] ---- Training Loss: 0.0011 ---- Validation Loss: 0.0010    MSELoss  1e-05\n",
            "Epoch [187/250] ---- Training Loss: 0.0011 ---- Validation Loss: 0.0010    MSELoss  1e-05\n",
            "Epoch [188/250] ---- Training Loss: 0.0011 ---- Validation Loss: 0.0010    MSELoss  1e-05\n",
            "Epoch [189/250] ---- Training Loss: 0.0011 ---- Validation Loss: 0.0010    MSELoss  1e-05\n",
            "Epoch [190/250] ---- Training Loss: 0.0011 ---- Validation Loss: 0.0010    MSELoss  1e-05\n",
            "Epoch [191/250] ---- Training Loss: 0.0011 ---- Validation Loss: 0.0010    MSELoss  1e-05\n",
            "Epoch [192/250] ---- Training Loss: 0.0011 ---- Validation Loss: 0.0010    MSELoss  1e-05\n",
            "Epoch [193/250] ---- Training Loss: 0.0011 ---- Validation Loss: 0.0010    MSELoss  1e-05\n",
            "Epoch [194/250] ---- Training Loss: 0.0011 ---- Validation Loss: 0.0010    MSELoss  1e-05\n",
            "Epoch [195/250] ---- Training Loss: 0.0011 ---- Validation Loss: 0.0010    MSELoss  1e-05\n",
            "Epoch [196/250] ---- Training Loss: 0.0011 ---- Validation Loss: 0.0010    MSELoss  1e-05\n",
            "Epoch [197/250] ---- Training Loss: 0.0011 ---- Validation Loss: 0.0010    MSELoss  1e-05\n",
            "Epoch [198/250] ---- Training Loss: 0.0011 ---- Validation Loss: 0.0010    MSELoss  1e-05\n",
            "Epoch [199/250] ---- Training Loss: 0.0011 ---- Validation Loss: 0.0010    MSELoss  1e-05\n",
            "Epoch [200/250] ---- Training Loss: 0.0011 ---- Validation Loss: 0.0010    MSELoss  1e-05\n",
            "Epoch [201/250] ---- Training Loss: 0.0011 ---- Validation Loss: 0.0010    MSELoss  1e-05\n",
            "Epoch [202/250] ---- Training Loss: 0.0011 ---- Validation Loss: 0.0010    MSELoss  1e-05\n",
            "Epoch [203/250] ---- Training Loss: 0.0011 ---- Validation Loss: 0.0010    MSELoss  1e-05\n",
            "Epoch [204/250] ---- Training Loss: 0.0011 ---- Validation Loss: 0.0010    MSELoss  1e-05\n",
            "Epoch [205/250] ---- Training Loss: 0.0011 ---- Validation Loss: 0.0009    MSELoss  1e-05\n",
            "Epoch [206/250] ---- Training Loss: 0.0011 ---- Validation Loss: 0.0009    MSELoss  1e-05\n",
            "Epoch [207/250] ---- Training Loss: 0.0011 ---- Validation Loss: 0.0009    MSELoss  1e-05\n",
            "Epoch [208/250] ---- Training Loss: 0.0011 ---- Validation Loss: 0.0009    MSELoss  1e-05\n",
            "Epoch [209/250] ---- Training Loss: 0.0011 ---- Validation Loss: 0.0009    MSELoss  1e-05\n",
            "Epoch [210/250] ---- Training Loss: 0.0011 ---- Validation Loss: 0.0009    MSELoss  1e-05\n",
            "Epoch [211/250] ---- Training Loss: 0.0011 ---- Validation Loss: 0.0009    MSELoss  1e-05\n",
            "Epoch [212/250] ---- Training Loss: 0.0011 ---- Validation Loss: 0.0009    MSELoss  1e-05\n",
            "Epoch [213/250] ---- Training Loss: 0.0011 ---- Validation Loss: 0.0009    MSELoss  1e-05\n",
            "Epoch [214/250] ---- Training Loss: 0.0010 ---- Validation Loss: 0.0009    MSELoss  1e-05\n",
            "Epoch [215/250] ---- Training Loss: 0.0010 ---- Validation Loss: 0.0009    MSELoss  1e-05\n",
            "Epoch [216/250] ---- Training Loss: 0.0010 ---- Validation Loss: 0.0009    MSELoss  1e-05\n",
            "Epoch [217/250] ---- Training Loss: 0.0010 ---- Validation Loss: 0.0009    MSELoss  1e-05\n",
            "Epoch [218/250] ---- Training Loss: 0.0010 ---- Validation Loss: 0.0009    MSELoss  1e-05\n",
            "Epoch [219/250] ---- Training Loss: 0.0010 ---- Validation Loss: 0.0009    MSELoss  1e-05\n",
            "Epoch [220/250] ---- Training Loss: 0.0010 ---- Validation Loss: 0.0009    MSELoss  1e-05\n",
            "Epoch [221/250] ---- Training Loss: 0.0010 ---- Validation Loss: 0.0009    MSELoss  1e-05\n",
            "Epoch [222/250] ---- Training Loss: 0.0010 ---- Validation Loss: 0.0009    MSELoss  1e-05\n",
            "Epoch [223/250] ---- Training Loss: 0.0010 ---- Validation Loss: 0.0009    MSELoss  1e-05\n",
            "Epoch [224/250] ---- Training Loss: 0.0010 ---- Validation Loss: 0.0009    MSELoss  1e-05\n",
            "Epoch [225/250] ---- Training Loss: 0.0010 ---- Validation Loss: 0.0009    MSELoss  1e-05\n",
            "Epoch [226/250] ---- Training Loss: 0.0010 ---- Validation Loss: 0.0009    MSELoss  1e-05\n",
            "Epoch [227/250] ---- Training Loss: 0.0010 ---- Validation Loss: 0.0009    MSELoss  1e-05\n",
            "Epoch [228/250] ---- Training Loss: 0.0010 ---- Validation Loss: 0.0009    MSELoss  1e-05\n",
            "Epoch [229/250] ---- Training Loss: 0.0010 ---- Validation Loss: 0.0009    MSELoss  1e-05\n",
            "Epoch [230/250] ---- Training Loss: 0.0010 ---- Validation Loss: 0.0009    MSELoss  1e-05\n",
            "Epoch [231/250] ---- Training Loss: 0.0010 ---- Validation Loss: 0.0009    MSELoss  1e-05\n",
            "Epoch [232/250] ---- Training Loss: 0.0010 ---- Validation Loss: 0.0009    MSELoss  1e-05\n",
            "Epoch [233/250] ---- Training Loss: 0.0010 ---- Validation Loss: 0.0009    MSELoss  1e-05\n",
            "Epoch [234/250] ---- Training Loss: 0.0010 ---- Validation Loss: 0.0009    MSELoss  1e-05\n",
            "Epoch [235/250] ---- Training Loss: 0.0010 ---- Validation Loss: 0.0009    MSELoss  1e-05\n",
            "Epoch [236/250] ---- Training Loss: 0.0010 ---- Validation Loss: 0.0009    MSELoss  1e-05\n",
            "Epoch [237/250] ---- Training Loss: 0.0010 ---- Validation Loss: 0.0009    MSELoss  1e-05\n",
            "Epoch [238/250] ---- Training Loss: 0.0010 ---- Validation Loss: 0.0009    MSELoss  1e-05\n",
            "Epoch [239/250] ---- Training Loss: 0.0010 ---- Validation Loss: 0.0009    MSELoss  1e-05\n",
            "Epoch [240/250] ---- Training Loss: 0.0010 ---- Validation Loss: 0.0009    MSELoss  1e-05\n",
            "Epoch [241/250] ---- Training Loss: 0.0010 ---- Validation Loss: 0.0009    MSELoss  1e-05\n",
            "Epoch [242/250] ---- Training Loss: 0.0010 ---- Validation Loss: 0.0009    MSELoss  1e-05\n",
            "Epoch [243/250] ---- Training Loss: 0.0010 ---- Validation Loss: 0.0009    MSELoss  1e-05\n",
            "Epoch [244/250] ---- Training Loss: 0.0010 ---- Validation Loss: 0.0009    MSELoss  1e-05\n",
            "Epoch [245/250] ---- Training Loss: 0.0010 ---- Validation Loss: 0.0009    MSELoss  1e-05\n",
            "Epoch [246/250] ---- Training Loss: 0.0010 ---- Validation Loss: 0.0009    MSELoss  1e-05\n",
            "Epoch [247/250] ---- Training Loss: 0.0010 ---- Validation Loss: 0.0009    MSELoss  1e-05\n",
            "Epoch [248/250] ---- Training Loss: 0.0010 ---- Validation Loss: 0.0009    MSELoss  1e-05\n",
            "Epoch [249/250] ---- Training Loss: 0.0010 ---- Validation Loss: 0.0009    MSELoss  1e-05\n",
            "Epoch [250/250] ---- Training Loss: 0.0010 ---- Validation Loss: 0.0009    MSELoss  1e-05\n",
            "Saved model: model_loss_MSELoss_wd_1e-05.pth\n",
            "Epoch [1/250] ---- Training Loss: 0.0058 ---- Validation Loss: 0.0043    MSELoss  5e-05\n",
            "Epoch [2/250] ---- Training Loss: 0.0049 ---- Validation Loss: 0.0042    MSELoss  5e-05\n",
            "Epoch [3/250] ---- Training Loss: 0.0048 ---- Validation Loss: 0.0041    MSELoss  5e-05\n",
            "Epoch [4/250] ---- Training Loss: 0.0046 ---- Validation Loss: 0.0040    MSELoss  5e-05\n",
            "Epoch [5/250] ---- Training Loss: 0.0044 ---- Validation Loss: 0.0039    MSELoss  5e-05\n",
            "Epoch [6/250] ---- Training Loss: 0.0043 ---- Validation Loss: 0.0038    MSELoss  5e-05\n",
            "Epoch [7/250] ---- Training Loss: 0.0041 ---- Validation Loss: 0.0038    MSELoss  5e-05\n",
            "Epoch [8/250] ---- Training Loss: 0.0040 ---- Validation Loss: 0.0037    MSELoss  5e-05\n",
            "Epoch [9/250] ---- Training Loss: 0.0039 ---- Validation Loss: 0.0036    MSELoss  5e-05\n",
            "Epoch [10/250] ---- Training Loss: 0.0038 ---- Validation Loss: 0.0036    MSELoss  5e-05\n",
            "Epoch [11/250] ---- Training Loss: 0.0037 ---- Validation Loss: 0.0035    MSELoss  5e-05\n",
            "Epoch [12/250] ---- Training Loss: 0.0036 ---- Validation Loss: 0.0034    MSELoss  5e-05\n",
            "Epoch [13/250] ---- Training Loss: 0.0036 ---- Validation Loss: 0.0034    MSELoss  5e-05\n",
            "Epoch [14/250] ---- Training Loss: 0.0035 ---- Validation Loss: 0.0033    MSELoss  5e-05\n",
            "Epoch [15/250] ---- Training Loss: 0.0034 ---- Validation Loss: 0.0033    MSELoss  5e-05\n",
            "Epoch [16/250] ---- Training Loss: 0.0034 ---- Validation Loss: 0.0032    MSELoss  5e-05\n",
            "Epoch [17/250] ---- Training Loss: 0.0033 ---- Validation Loss: 0.0032    MSELoss  5e-05\n",
            "Epoch [18/250] ---- Training Loss: 0.0033 ---- Validation Loss: 0.0032    MSELoss  5e-05\n",
            "Epoch [19/250] ---- Training Loss: 0.0033 ---- Validation Loss: 0.0031    MSELoss  5e-05\n",
            "Epoch [20/250] ---- Training Loss: 0.0032 ---- Validation Loss: 0.0031    MSELoss  5e-05\n",
            "Epoch [21/250] ---- Training Loss: 0.0032 ---- Validation Loss: 0.0031    MSELoss  5e-05\n",
            "Epoch [22/250] ---- Training Loss: 0.0032 ---- Validation Loss: 0.0031    MSELoss  5e-05\n",
            "Epoch [23/250] ---- Training Loss: 0.0032 ---- Validation Loss: 0.0031    MSELoss  5e-05\n",
            "Epoch [24/250] ---- Training Loss: 0.0032 ---- Validation Loss: 0.0031    MSELoss  5e-05\n",
            "Epoch [25/250] ---- Training Loss: 0.0031 ---- Validation Loss: 0.0030    MSELoss  5e-05\n",
            "Epoch [26/250] ---- Training Loss: 0.0031 ---- Validation Loss: 0.0030    MSELoss  5e-05\n",
            "Epoch [27/250] ---- Training Loss: 0.0031 ---- Validation Loss: 0.0030    MSELoss  5e-05\n",
            "Epoch [28/250] ---- Training Loss: 0.0031 ---- Validation Loss: 0.0030    MSELoss  5e-05\n",
            "Epoch [29/250] ---- Training Loss: 0.0031 ---- Validation Loss: 0.0030    MSELoss  5e-05\n",
            "Epoch [30/250] ---- Training Loss: 0.0031 ---- Validation Loss: 0.0030    MSELoss  5e-05\n",
            "Epoch [31/250] ---- Training Loss: 0.0031 ---- Validation Loss: 0.0030    MSELoss  5e-05\n",
            "Epoch [32/250] ---- Training Loss: 0.0031 ---- Validation Loss: 0.0030    MSELoss  5e-05\n",
            "Epoch [33/250] ---- Training Loss: 0.0030 ---- Validation Loss: 0.0030    MSELoss  5e-05\n",
            "Epoch [34/250] ---- Training Loss: 0.0030 ---- Validation Loss: 0.0030    MSELoss  5e-05\n",
            "Epoch [35/250] ---- Training Loss: 0.0030 ---- Validation Loss: 0.0030    MSELoss  5e-05\n",
            "Epoch [36/250] ---- Training Loss: 0.0030 ---- Validation Loss: 0.0030    MSELoss  5e-05\n",
            "Epoch [37/250] ---- Training Loss: 0.0030 ---- Validation Loss: 0.0030    MSELoss  5e-05\n",
            "Epoch [38/250] ---- Training Loss: 0.0030 ---- Validation Loss: 0.0030    MSELoss  5e-05\n",
            "Epoch [39/250] ---- Training Loss: 0.0030 ---- Validation Loss: 0.0029    MSELoss  5e-05\n",
            "Epoch [40/250] ---- Training Loss: 0.0030 ---- Validation Loss: 0.0029    MSELoss  5e-05\n",
            "Epoch [41/250] ---- Training Loss: 0.0030 ---- Validation Loss: 0.0029    MSELoss  5e-05\n",
            "Epoch [42/250] ---- Training Loss: 0.0030 ---- Validation Loss: 0.0029    MSELoss  5e-05\n",
            "Epoch [43/250] ---- Training Loss: 0.0030 ---- Validation Loss: 0.0029    MSELoss  5e-05\n",
            "Epoch [44/250] ---- Training Loss: 0.0030 ---- Validation Loss: 0.0029    MSELoss  5e-05\n",
            "Epoch [45/250] ---- Training Loss: 0.0030 ---- Validation Loss: 0.0029    MSELoss  5e-05\n",
            "Epoch [46/250] ---- Training Loss: 0.0030 ---- Validation Loss: 0.0029    MSELoss  5e-05\n",
            "Epoch [47/250] ---- Training Loss: 0.0030 ---- Validation Loss: 0.0029    MSELoss  5e-05\n",
            "Epoch [48/250] ---- Training Loss: 0.0030 ---- Validation Loss: 0.0029    MSELoss  5e-05\n",
            "Epoch [49/250] ---- Training Loss: 0.0029 ---- Validation Loss: 0.0029    MSELoss  5e-05\n",
            "Epoch [50/250] ---- Training Loss: 0.0029 ---- Validation Loss: 0.0029    MSELoss  5e-05\n",
            "Epoch [51/250] ---- Training Loss: 0.0029 ---- Validation Loss: 0.0029    MSELoss  5e-05\n",
            "Epoch [52/250] ---- Training Loss: 0.0029 ---- Validation Loss: 0.0029    MSELoss  5e-05\n",
            "Epoch [53/250] ---- Training Loss: 0.0029 ---- Validation Loss: 0.0029    MSELoss  5e-05\n",
            "Epoch [54/250] ---- Training Loss: 0.0029 ---- Validation Loss: 0.0029    MSELoss  5e-05\n",
            "Epoch [55/250] ---- Training Loss: 0.0029 ---- Validation Loss: 0.0029    MSELoss  5e-05\n",
            "Epoch [56/250] ---- Training Loss: 0.0029 ---- Validation Loss: 0.0029    MSELoss  5e-05\n",
            "Epoch [57/250] ---- Training Loss: 0.0029 ---- Validation Loss: 0.0029    MSELoss  5e-05\n",
            "Epoch [58/250] ---- Training Loss: 0.0029 ---- Validation Loss: 0.0029    MSELoss  5e-05\n",
            "Epoch [59/250] ---- Training Loss: 0.0029 ---- Validation Loss: 0.0029    MSELoss  5e-05\n",
            "Epoch [60/250] ---- Training Loss: 0.0029 ---- Validation Loss: 0.0029    MSELoss  5e-05\n",
            "Epoch [61/250] ---- Training Loss: 0.0029 ---- Validation Loss: 0.0029    MSELoss  5e-05\n",
            "Epoch [62/250] ---- Training Loss: 0.0029 ---- Validation Loss: 0.0029    MSELoss  5e-05\n",
            "Epoch [63/250] ---- Training Loss: 0.0029 ---- Validation Loss: 0.0029    MSELoss  5e-05\n",
            "Epoch [64/250] ---- Training Loss: 0.0029 ---- Validation Loss: 0.0029    MSELoss  5e-05\n",
            "Epoch [65/250] ---- Training Loss: 0.0029 ---- Validation Loss: 0.0028    MSELoss  5e-05\n",
            "Epoch [66/250] ---- Training Loss: 0.0029 ---- Validation Loss: 0.0028    MSELoss  5e-05\n",
            "Epoch [67/250] ---- Training Loss: 0.0029 ---- Validation Loss: 0.0028    MSELoss  5e-05\n",
            "Epoch [68/250] ---- Training Loss: 0.0029 ---- Validation Loss: 0.0028    MSELoss  5e-05\n",
            "Epoch [69/250] ---- Training Loss: 0.0029 ---- Validation Loss: 0.0028    MSELoss  5e-05\n",
            "Epoch [70/250] ---- Training Loss: 0.0029 ---- Validation Loss: 0.0028    MSELoss  5e-05\n",
            "Epoch [71/250] ---- Training Loss: 0.0029 ---- Validation Loss: 0.0028    MSELoss  5e-05\n",
            "Epoch [72/250] ---- Training Loss: 0.0029 ---- Validation Loss: 0.0028    MSELoss  5e-05\n",
            "Epoch [73/250] ---- Training Loss: 0.0029 ---- Validation Loss: 0.0028    MSELoss  5e-05\n",
            "Epoch [74/250] ---- Training Loss: 0.0029 ---- Validation Loss: 0.0028    MSELoss  5e-05\n",
            "Epoch [75/250] ---- Training Loss: 0.0028 ---- Validation Loss: 0.0028    MSELoss  5e-05\n",
            "Epoch [76/250] ---- Training Loss: 0.0028 ---- Validation Loss: 0.0028    MSELoss  5e-05\n",
            "Epoch [77/250] ---- Training Loss: 0.0028 ---- Validation Loss: 0.0028    MSELoss  5e-05\n",
            "Epoch [78/250] ---- Training Loss: 0.0028 ---- Validation Loss: 0.0028    MSELoss  5e-05\n",
            "Epoch [79/250] ---- Training Loss: 0.0028 ---- Validation Loss: 0.0028    MSELoss  5e-05\n",
            "Epoch [80/250] ---- Training Loss: 0.0028 ---- Validation Loss: 0.0028    MSELoss  5e-05\n",
            "Epoch [81/250] ---- Training Loss: 0.0028 ---- Validation Loss: 0.0028    MSELoss  5e-05\n",
            "Epoch [82/250] ---- Training Loss: 0.0028 ---- Validation Loss: 0.0028    MSELoss  5e-05\n",
            "Epoch [83/250] ---- Training Loss: 0.0028 ---- Validation Loss: 0.0028    MSELoss  5e-05\n",
            "Epoch [84/250] ---- Training Loss: 0.0028 ---- Validation Loss: 0.0028    MSELoss  5e-05\n",
            "Epoch [85/250] ---- Training Loss: 0.0028 ---- Validation Loss: 0.0028    MSELoss  5e-05\n",
            "Epoch [86/250] ---- Training Loss: 0.0028 ---- Validation Loss: 0.0028    MSELoss  5e-05\n",
            "Epoch [87/250] ---- Training Loss: 0.0028 ---- Validation Loss: 0.0028    MSELoss  5e-05\n",
            "Epoch [88/250] ---- Training Loss: 0.0028 ---- Validation Loss: 0.0028    MSELoss  5e-05\n",
            "Epoch [89/250] ---- Training Loss: 0.0028 ---- Validation Loss: 0.0028    MSELoss  5e-05\n",
            "Epoch [90/250] ---- Training Loss: 0.0028 ---- Validation Loss: 0.0027    MSELoss  5e-05\n",
            "Epoch [91/250] ---- Training Loss: 0.0028 ---- Validation Loss: 0.0027    MSELoss  5e-05\n",
            "Epoch [92/250] ---- Training Loss: 0.0028 ---- Validation Loss: 0.0027    MSELoss  5e-05\n",
            "Epoch [93/250] ---- Training Loss: 0.0028 ---- Validation Loss: 0.0027    MSELoss  5e-05\n",
            "Epoch [94/250] ---- Training Loss: 0.0028 ---- Validation Loss: 0.0027    MSELoss  5e-05\n",
            "Epoch [95/250] ---- Training Loss: 0.0028 ---- Validation Loss: 0.0027    MSELoss  5e-05\n",
            "Epoch [96/250] ---- Training Loss: 0.0028 ---- Validation Loss: 0.0027    MSELoss  5e-05\n",
            "Epoch [97/250] ---- Training Loss: 0.0028 ---- Validation Loss: 0.0027    MSELoss  5e-05\n",
            "Epoch [98/250] ---- Training Loss: 0.0027 ---- Validation Loss: 0.0027    MSELoss  5e-05\n",
            "Epoch [99/250] ---- Training Loss: 0.0027 ---- Validation Loss: 0.0027    MSELoss  5e-05\n",
            "Epoch [100/250] ---- Training Loss: 0.0027 ---- Validation Loss: 0.0027    MSELoss  5e-05\n",
            "Epoch [101/250] ---- Training Loss: 0.0027 ---- Validation Loss: 0.0027    MSELoss  5e-05\n",
            "Epoch [102/250] ---- Training Loss: 0.0027 ---- Validation Loss: 0.0027    MSELoss  5e-05\n",
            "Epoch [103/250] ---- Training Loss: 0.0027 ---- Validation Loss: 0.0027    MSELoss  5e-05\n",
            "Epoch [104/250] ---- Training Loss: 0.0027 ---- Validation Loss: 0.0027    MSELoss  5e-05\n",
            "Epoch [105/250] ---- Training Loss: 0.0027 ---- Validation Loss: 0.0027    MSELoss  5e-05\n",
            "Epoch [106/250] ---- Training Loss: 0.0027 ---- Validation Loss: 0.0027    MSELoss  5e-05\n",
            "Epoch [107/250] ---- Training Loss: 0.0027 ---- Validation Loss: 0.0026    MSELoss  5e-05\n",
            "Epoch [108/250] ---- Training Loss: 0.0027 ---- Validation Loss: 0.0026    MSELoss  5e-05\n",
            "Epoch [109/250] ---- Training Loss: 0.0027 ---- Validation Loss: 0.0026    MSELoss  5e-05\n",
            "Epoch [110/250] ---- Training Loss: 0.0027 ---- Validation Loss: 0.0026    MSELoss  5e-05\n",
            "Epoch [111/250] ---- Training Loss: 0.0027 ---- Validation Loss: 0.0026    MSELoss  5e-05\n",
            "Epoch [112/250] ---- Training Loss: 0.0027 ---- Validation Loss: 0.0026    MSELoss  5e-05\n",
            "Epoch [113/250] ---- Training Loss: 0.0027 ---- Validation Loss: 0.0026    MSELoss  5e-05\n",
            "Epoch [114/250] ---- Training Loss: 0.0027 ---- Validation Loss: 0.0026    MSELoss  5e-05\n",
            "Epoch [115/250] ---- Training Loss: 0.0026 ---- Validation Loss: 0.0026    MSELoss  5e-05\n",
            "Epoch [116/250] ---- Training Loss: 0.0026 ---- Validation Loss: 0.0026    MSELoss  5e-05\n",
            "Epoch [117/250] ---- Training Loss: 0.0026 ---- Validation Loss: 0.0026    MSELoss  5e-05\n",
            "Epoch [118/250] ---- Training Loss: 0.0026 ---- Validation Loss: 0.0026    MSELoss  5e-05\n",
            "Epoch [119/250] ---- Training Loss: 0.0026 ---- Validation Loss: 0.0026    MSELoss  5e-05\n",
            "Epoch [120/250] ---- Training Loss: 0.0026 ---- Validation Loss: 0.0025    MSELoss  5e-05\n",
            "Epoch [121/250] ---- Training Loss: 0.0026 ---- Validation Loss: 0.0025    MSELoss  5e-05\n",
            "Epoch [122/250] ---- Training Loss: 0.0026 ---- Validation Loss: 0.0025    MSELoss  5e-05\n",
            "Epoch [123/250] ---- Training Loss: 0.0026 ---- Validation Loss: 0.0025    MSELoss  5e-05\n",
            "Epoch [124/250] ---- Training Loss: 0.0026 ---- Validation Loss: 0.0025    MSELoss  5e-05\n",
            "Epoch [125/250] ---- Training Loss: 0.0026 ---- Validation Loss: 0.0025    MSELoss  5e-05\n",
            "Epoch [126/250] ---- Training Loss: 0.0026 ---- Validation Loss: 0.0025    MSELoss  5e-05\n",
            "Epoch [127/250] ---- Training Loss: 0.0026 ---- Validation Loss: 0.0025    MSELoss  5e-05\n",
            "Epoch [128/250] ---- Training Loss: 0.0026 ---- Validation Loss: 0.0025    MSELoss  5e-05\n",
            "Epoch [129/250] ---- Training Loss: 0.0026 ---- Validation Loss: 0.0025    MSELoss  5e-05\n",
            "Epoch [130/250] ---- Training Loss: 0.0026 ---- Validation Loss: 0.0025    MSELoss  5e-05\n",
            "Epoch [131/250] ---- Training Loss: 0.0025 ---- Validation Loss: 0.0025    MSELoss  5e-05\n",
            "Epoch [132/250] ---- Training Loss: 0.0025 ---- Validation Loss: 0.0025    MSELoss  5e-05\n",
            "Epoch [133/250] ---- Training Loss: 0.0025 ---- Validation Loss: 0.0024    MSELoss  5e-05\n",
            "Epoch [134/250] ---- Training Loss: 0.0025 ---- Validation Loss: 0.0024    MSELoss  5e-05\n",
            "Epoch [135/250] ---- Training Loss: 0.0025 ---- Validation Loss: 0.0024    MSELoss  5e-05\n",
            "Epoch [136/250] ---- Training Loss: 0.0025 ---- Validation Loss: 0.0024    MSELoss  5e-05\n",
            "Epoch [137/250] ---- Training Loss: 0.0025 ---- Validation Loss: 0.0024    MSELoss  5e-05\n",
            "Epoch [138/250] ---- Training Loss: 0.0025 ---- Validation Loss: 0.0024    MSELoss  5e-05\n",
            "Epoch [139/250] ---- Training Loss: 0.0025 ---- Validation Loss: 0.0024    MSELoss  5e-05\n",
            "Epoch [140/250] ---- Training Loss: 0.0025 ---- Validation Loss: 0.0024    MSELoss  5e-05\n",
            "Epoch [141/250] ---- Training Loss: 0.0025 ---- Validation Loss: 0.0024    MSELoss  5e-05\n",
            "Epoch [142/250] ---- Training Loss: 0.0025 ---- Validation Loss: 0.0024    MSELoss  5e-05\n",
            "Epoch [143/250] ---- Training Loss: 0.0025 ---- Validation Loss: 0.0024    MSELoss  5e-05\n",
            "Epoch [144/250] ---- Training Loss: 0.0025 ---- Validation Loss: 0.0024    MSELoss  5e-05\n",
            "Epoch [145/250] ---- Training Loss: 0.0025 ---- Validation Loss: 0.0024    MSELoss  5e-05\n",
            "Epoch [146/250] ---- Training Loss: 0.0024 ---- Validation Loss: 0.0023    MSELoss  5e-05\n",
            "Epoch [147/250] ---- Training Loss: 0.0024 ---- Validation Loss: 0.0023    MSELoss  5e-05\n",
            "Epoch [148/250] ---- Training Loss: 0.0024 ---- Validation Loss: 0.0023    MSELoss  5e-05\n",
            "Epoch [149/250] ---- Training Loss: 0.0024 ---- Validation Loss: 0.0023    MSELoss  5e-05\n",
            "Epoch [150/250] ---- Training Loss: 0.0024 ---- Validation Loss: 0.0023    MSELoss  5e-05\n",
            "Epoch [151/250] ---- Training Loss: 0.0024 ---- Validation Loss: 0.0023    MSELoss  5e-05\n",
            "Epoch [152/250] ---- Training Loss: 0.0024 ---- Validation Loss: 0.0023    MSELoss  5e-05\n",
            "Epoch [153/250] ---- Training Loss: 0.0024 ---- Validation Loss: 0.0023    MSELoss  5e-05\n",
            "Epoch [154/250] ---- Training Loss: 0.0024 ---- Validation Loss: 0.0023    MSELoss  5e-05\n",
            "Epoch [155/250] ---- Training Loss: 0.0024 ---- Validation Loss: 0.0023    MSELoss  5e-05\n",
            "Epoch [156/250] ---- Training Loss: 0.0024 ---- Validation Loss: 0.0023    MSELoss  5e-05\n",
            "Epoch [157/250] ---- Training Loss: 0.0024 ---- Validation Loss: 0.0023    MSELoss  5e-05\n",
            "Epoch [158/250] ---- Training Loss: 0.0024 ---- Validation Loss: 0.0023    MSELoss  5e-05\n",
            "Epoch [159/250] ---- Training Loss: 0.0024 ---- Validation Loss: 0.0022    MSELoss  5e-05\n",
            "Epoch [160/250] ---- Training Loss: 0.0024 ---- Validation Loss: 0.0022    MSELoss  5e-05\n",
            "Epoch [161/250] ---- Training Loss: 0.0024 ---- Validation Loss: 0.0022    MSELoss  5e-05\n",
            "Epoch [162/250] ---- Training Loss: 0.0023 ---- Validation Loss: 0.0022    MSELoss  5e-05\n",
            "Epoch [163/250] ---- Training Loss: 0.0023 ---- Validation Loss: 0.0022    MSELoss  5e-05\n",
            "Epoch [164/250] ---- Training Loss: 0.0023 ---- Validation Loss: 0.0022    MSELoss  5e-05\n",
            "Epoch [165/250] ---- Training Loss: 0.0023 ---- Validation Loss: 0.0022    MSELoss  5e-05\n",
            "Epoch [166/250] ---- Training Loss: 0.0023 ---- Validation Loss: 0.0022    MSELoss  5e-05\n",
            "Epoch [167/250] ---- Training Loss: 0.0023 ---- Validation Loss: 0.0022    MSELoss  5e-05\n",
            "Epoch [168/250] ---- Training Loss: 0.0023 ---- Validation Loss: 0.0022    MSELoss  5e-05\n",
            "Epoch [169/250] ---- Training Loss: 0.0023 ---- Validation Loss: 0.0022    MSELoss  5e-05\n",
            "Epoch [170/250] ---- Training Loss: 0.0023 ---- Validation Loss: 0.0022    MSELoss  5e-05\n",
            "Epoch [171/250] ---- Training Loss: 0.0023 ---- Validation Loss: 0.0022    MSELoss  5e-05\n",
            "Epoch [172/250] ---- Training Loss: 0.0023 ---- Validation Loss: 0.0022    MSELoss  5e-05\n",
            "Epoch [173/250] ---- Training Loss: 0.0023 ---- Validation Loss: 0.0022    MSELoss  5e-05\n",
            "Epoch [174/250] ---- Training Loss: 0.0023 ---- Validation Loss: 0.0021    MSELoss  5e-05\n",
            "Epoch [175/250] ---- Training Loss: 0.0023 ---- Validation Loss: 0.0021    MSELoss  5e-05\n",
            "Epoch [176/250] ---- Training Loss: 0.0023 ---- Validation Loss: 0.0021    MSELoss  5e-05\n",
            "Epoch [177/250] ---- Training Loss: 0.0023 ---- Validation Loss: 0.0021    MSELoss  5e-05\n",
            "Epoch [178/250] ---- Training Loss: 0.0023 ---- Validation Loss: 0.0021    MSELoss  5e-05\n",
            "Epoch [179/250] ---- Training Loss: 0.0023 ---- Validation Loss: 0.0021    MSELoss  5e-05\n",
            "Epoch [180/250] ---- Training Loss: 0.0023 ---- Validation Loss: 0.0021    MSELoss  5e-05\n",
            "Epoch [181/250] ---- Training Loss: 0.0022 ---- Validation Loss: 0.0021    MSELoss  5e-05\n",
            "Epoch [182/250] ---- Training Loss: 0.0022 ---- Validation Loss: 0.0021    MSELoss  5e-05\n",
            "Epoch [183/250] ---- Training Loss: 0.0022 ---- Validation Loss: 0.0021    MSELoss  5e-05\n",
            "Epoch [184/250] ---- Training Loss: 0.0022 ---- Validation Loss: 0.0021    MSELoss  5e-05\n",
            "Epoch [185/250] ---- Training Loss: 0.0022 ---- Validation Loss: 0.0021    MSELoss  5e-05\n",
            "Epoch [186/250] ---- Training Loss: 0.0022 ---- Validation Loss: 0.0021    MSELoss  5e-05\n",
            "Epoch [187/250] ---- Training Loss: 0.0022 ---- Validation Loss: 0.0021    MSELoss  5e-05\n",
            "Epoch [188/250] ---- Training Loss: 0.0022 ---- Validation Loss: 0.0021    MSELoss  5e-05\n",
            "Epoch [189/250] ---- Training Loss: 0.0022 ---- Validation Loss: 0.0021    MSELoss  5e-05\n",
            "Epoch [190/250] ---- Training Loss: 0.0022 ---- Validation Loss: 0.0021    MSELoss  5e-05\n",
            "Epoch [191/250] ---- Training Loss: 0.0022 ---- Validation Loss: 0.0021    MSELoss  5e-05\n",
            "Epoch [192/250] ---- Training Loss: 0.0022 ---- Validation Loss: 0.0021    MSELoss  5e-05\n",
            "Epoch [193/250] ---- Training Loss: 0.0022 ---- Validation Loss: 0.0021    MSELoss  5e-05\n",
            "Epoch [194/250] ---- Training Loss: 0.0022 ---- Validation Loss: 0.0021    MSELoss  5e-05\n",
            "Epoch [195/250] ---- Training Loss: 0.0022 ---- Validation Loss: 0.0021    MSELoss  5e-05\n",
            "Epoch [196/250] ---- Training Loss: 0.0022 ---- Validation Loss: 0.0021    MSELoss  5e-05\n",
            "Epoch [197/250] ---- Training Loss: 0.0022 ---- Validation Loss: 0.0020    MSELoss  5e-05\n",
            "Epoch [198/250] ---- Training Loss: 0.0022 ---- Validation Loss: 0.0020    MSELoss  5e-05\n",
            "Epoch [199/250] ---- Training Loss: 0.0022 ---- Validation Loss: 0.0020    MSELoss  5e-05\n",
            "Epoch [200/250] ---- Training Loss: 0.0022 ---- Validation Loss: 0.0020    MSELoss  5e-05\n",
            "Epoch [201/250] ---- Training Loss: 0.0022 ---- Validation Loss: 0.0020    MSELoss  5e-05\n",
            "Epoch [202/250] ---- Training Loss: 0.0022 ---- Validation Loss: 0.0020    MSELoss  5e-05\n",
            "Epoch [203/250] ---- Training Loss: 0.0022 ---- Validation Loss: 0.0020    MSELoss  5e-05\n",
            "Epoch [204/250] ---- Training Loss: 0.0022 ---- Validation Loss: 0.0020    MSELoss  5e-05\n",
            "Epoch [205/250] ---- Training Loss: 0.0022 ---- Validation Loss: 0.0020    MSELoss  5e-05\n",
            "Epoch [206/250] ---- Training Loss: 0.0022 ---- Validation Loss: 0.0020    MSELoss  5e-05\n",
            "Epoch [207/250] ---- Training Loss: 0.0022 ---- Validation Loss: 0.0020    MSELoss  5e-05\n",
            "Epoch [208/250] ---- Training Loss: 0.0022 ---- Validation Loss: 0.0020    MSELoss  5e-05\n",
            "Epoch [209/250] ---- Training Loss: 0.0021 ---- Validation Loss: 0.0020    MSELoss  5e-05\n",
            "Epoch [210/250] ---- Training Loss: 0.0021 ---- Validation Loss: 0.0020    MSELoss  5e-05\n",
            "Epoch [211/250] ---- Training Loss: 0.0021 ---- Validation Loss: 0.0020    MSELoss  5e-05\n",
            "Epoch [212/250] ---- Training Loss: 0.0021 ---- Validation Loss: 0.0020    MSELoss  5e-05\n",
            "Epoch [213/250] ---- Training Loss: 0.0021 ---- Validation Loss: 0.0020    MSELoss  5e-05\n",
            "Epoch [214/250] ---- Training Loss: 0.0021 ---- Validation Loss: 0.0020    MSELoss  5e-05\n",
            "Epoch [215/250] ---- Training Loss: 0.0021 ---- Validation Loss: 0.0020    MSELoss  5e-05\n",
            "Epoch [216/250] ---- Training Loss: 0.0021 ---- Validation Loss: 0.0020    MSELoss  5e-05\n",
            "Epoch [217/250] ---- Training Loss: 0.0021 ---- Validation Loss: 0.0020    MSELoss  5e-05\n",
            "Epoch [218/250] ---- Training Loss: 0.0021 ---- Validation Loss: 0.0020    MSELoss  5e-05\n",
            "Epoch [219/250] ---- Training Loss: 0.0021 ---- Validation Loss: 0.0020    MSELoss  5e-05\n",
            "Epoch [220/250] ---- Training Loss: 0.0021 ---- Validation Loss: 0.0020    MSELoss  5e-05\n",
            "Epoch [221/250] ---- Training Loss: 0.0021 ---- Validation Loss: 0.0020    MSELoss  5e-05\n",
            "Epoch [222/250] ---- Training Loss: 0.0021 ---- Validation Loss: 0.0020    MSELoss  5e-05\n",
            "Epoch [223/250] ---- Training Loss: 0.0021 ---- Validation Loss: 0.0020    MSELoss  5e-05\n",
            "Epoch [224/250] ---- Training Loss: 0.0021 ---- Validation Loss: 0.0020    MSELoss  5e-05\n",
            "Epoch [225/250] ---- Training Loss: 0.0021 ---- Validation Loss: 0.0020    MSELoss  5e-05\n",
            "Epoch [226/250] ---- Training Loss: 0.0021 ---- Validation Loss: 0.0020    MSELoss  5e-05\n",
            "Epoch [227/250] ---- Training Loss: 0.0021 ---- Validation Loss: 0.0020    MSELoss  5e-05\n",
            "Epoch [228/250] ---- Training Loss: 0.0021 ---- Validation Loss: 0.0020    MSELoss  5e-05\n",
            "Epoch [229/250] ---- Training Loss: 0.0021 ---- Validation Loss: 0.0020    MSELoss  5e-05\n",
            "Epoch [230/250] ---- Training Loss: 0.0021 ---- Validation Loss: 0.0020    MSELoss  5e-05\n",
            "Epoch [231/250] ---- Training Loss: 0.0021 ---- Validation Loss: 0.0020    MSELoss  5e-05\n",
            "Epoch [232/250] ---- Training Loss: 0.0021 ---- Validation Loss: 0.0020    MSELoss  5e-05\n",
            "Epoch [233/250] ---- Training Loss: 0.0021 ---- Validation Loss: 0.0020    MSELoss  5e-05\n",
            "Epoch [234/250] ---- Training Loss: 0.0021 ---- Validation Loss: 0.0020    MSELoss  5e-05\n",
            "Epoch [235/250] ---- Training Loss: 0.0021 ---- Validation Loss: 0.0019    MSELoss  5e-05\n",
            "Epoch [236/250] ---- Training Loss: 0.0021 ---- Validation Loss: 0.0019    MSELoss  5e-05\n",
            "Epoch [237/250] ---- Training Loss: 0.0021 ---- Validation Loss: 0.0019    MSELoss  5e-05\n",
            "Epoch [238/250] ---- Training Loss: 0.0021 ---- Validation Loss: 0.0019    MSELoss  5e-05\n",
            "Epoch [239/250] ---- Training Loss: 0.0021 ---- Validation Loss: 0.0019    MSELoss  5e-05\n",
            "Epoch [240/250] ---- Training Loss: 0.0021 ---- Validation Loss: 0.0019    MSELoss  5e-05\n",
            "Epoch [241/250] ---- Training Loss: 0.0021 ---- Validation Loss: 0.0019    MSELoss  5e-05\n",
            "Epoch [242/250] ---- Training Loss: 0.0021 ---- Validation Loss: 0.0019    MSELoss  5e-05\n",
            "Epoch [243/250] ---- Training Loss: 0.0021 ---- Validation Loss: 0.0019    MSELoss  5e-05\n",
            "Epoch [244/250] ---- Training Loss: 0.0021 ---- Validation Loss: 0.0019    MSELoss  5e-05\n",
            "Epoch [245/250] ---- Training Loss: 0.0021 ---- Validation Loss: 0.0019    MSELoss  5e-05\n",
            "Epoch [246/250] ---- Training Loss: 0.0021 ---- Validation Loss: 0.0019    MSELoss  5e-05\n",
            "Epoch [247/250] ---- Training Loss: 0.0021 ---- Validation Loss: 0.0019    MSELoss  5e-05\n",
            "Epoch [248/250] ---- Training Loss: 0.0021 ---- Validation Loss: 0.0019    MSELoss  5e-05\n",
            "Epoch [249/250] ---- Training Loss: 0.0021 ---- Validation Loss: 0.0019    MSELoss  5e-05\n",
            "Epoch [250/250] ---- Training Loss: 0.0021 ---- Validation Loss: 0.0019    MSELoss  5e-05\n",
            "Saved model: model_loss_MSELoss_wd_5e-05.pth\n",
            "Epoch [1/250] ---- Training Loss: 0.0056 ---- Validation Loss: 0.0045    MSELoss  1e-06\n",
            "Epoch [2/250] ---- Training Loss: 0.0048 ---- Validation Loss: 0.0043    MSELoss  1e-06\n",
            "Epoch [3/250] ---- Training Loss: 0.0045 ---- Validation Loss: 0.0041    MSELoss  1e-06\n",
            "Epoch [4/250] ---- Training Loss: 0.0043 ---- Validation Loss: 0.0039    MSELoss  1e-06\n",
            "Epoch [5/250] ---- Training Loss: 0.0041 ---- Validation Loss: 0.0038    MSELoss  1e-06\n",
            "Epoch [6/250] ---- Training Loss: 0.0040 ---- Validation Loss: 0.0037    MSELoss  1e-06\n",
            "Epoch [7/250] ---- Training Loss: 0.0039 ---- Validation Loss: 0.0036    MSELoss  1e-06\n",
            "Epoch [8/250] ---- Training Loss: 0.0038 ---- Validation Loss: 0.0035    MSELoss  1e-06\n",
            "Epoch [9/250] ---- Training Loss: 0.0037 ---- Validation Loss: 0.0035    MSELoss  1e-06\n",
            "Epoch [10/250] ---- Training Loss: 0.0036 ---- Validation Loss: 0.0034    MSELoss  1e-06\n",
            "Epoch [11/250] ---- Training Loss: 0.0035 ---- Validation Loss: 0.0033    MSELoss  1e-06\n",
            "Epoch [12/250] ---- Training Loss: 0.0034 ---- Validation Loss: 0.0032    MSELoss  1e-06\n",
            "Epoch [13/250] ---- Training Loss: 0.0034 ---- Validation Loss: 0.0032    MSELoss  1e-06\n",
            "Epoch [14/250] ---- Training Loss: 0.0033 ---- Validation Loss: 0.0031    MSELoss  1e-06\n",
            "Epoch [15/250] ---- Training Loss: 0.0032 ---- Validation Loss: 0.0031    MSELoss  1e-06\n",
            "Epoch [16/250] ---- Training Loss: 0.0032 ---- Validation Loss: 0.0031    MSELoss  1e-06\n",
            "Epoch [17/250] ---- Training Loss: 0.0032 ---- Validation Loss: 0.0030    MSELoss  1e-06\n",
            "Epoch [18/250] ---- Training Loss: 0.0031 ---- Validation Loss: 0.0030    MSELoss  1e-06\n",
            "Epoch [19/250] ---- Training Loss: 0.0031 ---- Validation Loss: 0.0030    MSELoss  1e-06\n",
            "Epoch [20/250] ---- Training Loss: 0.0030 ---- Validation Loss: 0.0029    MSELoss  1e-06\n",
            "Epoch [21/250] ---- Training Loss: 0.0030 ---- Validation Loss: 0.0029    MSELoss  1e-06\n",
            "Epoch [22/250] ---- Training Loss: 0.0030 ---- Validation Loss: 0.0029    MSELoss  1e-06\n",
            "Epoch [23/250] ---- Training Loss: 0.0029 ---- Validation Loss: 0.0028    MSELoss  1e-06\n",
            "Epoch [24/250] ---- Training Loss: 0.0029 ---- Validation Loss: 0.0028    MSELoss  1e-06\n",
            "Epoch [25/250] ---- Training Loss: 0.0029 ---- Validation Loss: 0.0028    MSELoss  1e-06\n",
            "Epoch [26/250] ---- Training Loss: 0.0028 ---- Validation Loss: 0.0027    MSELoss  1e-06\n",
            "Epoch [27/250] ---- Training Loss: 0.0028 ---- Validation Loss: 0.0027    MSELoss  1e-06\n",
            "Epoch [28/250] ---- Training Loss: 0.0028 ---- Validation Loss: 0.0027    MSELoss  1e-06\n",
            "Epoch [29/250] ---- Training Loss: 0.0027 ---- Validation Loss: 0.0026    MSELoss  1e-06\n",
            "Epoch [30/250] ---- Training Loss: 0.0027 ---- Validation Loss: 0.0026    MSELoss  1e-06\n",
            "Epoch [31/250] ---- Training Loss: 0.0027 ---- Validation Loss: 0.0026    MSELoss  1e-06\n",
            "Epoch [32/250] ---- Training Loss: 0.0026 ---- Validation Loss: 0.0025    MSELoss  1e-06\n",
            "Epoch [33/250] ---- Training Loss: 0.0026 ---- Validation Loss: 0.0025    MSELoss  1e-06\n",
            "Epoch [34/250] ---- Training Loss: 0.0026 ---- Validation Loss: 0.0024    MSELoss  1e-06\n",
            "Epoch [35/250] ---- Training Loss: 0.0025 ---- Validation Loss: 0.0024    MSELoss  1e-06\n",
            "Epoch [36/250] ---- Training Loss: 0.0025 ---- Validation Loss: 0.0024    MSELoss  1e-06\n",
            "Epoch [37/250] ---- Training Loss: 0.0025 ---- Validation Loss: 0.0023    MSELoss  1e-06\n",
            "Epoch [38/250] ---- Training Loss: 0.0024 ---- Validation Loss: 0.0023    MSELoss  1e-06\n",
            "Epoch [39/250] ---- Training Loss: 0.0024 ---- Validation Loss: 0.0023    MSELoss  1e-06\n",
            "Epoch [40/250] ---- Training Loss: 0.0024 ---- Validation Loss: 0.0022    MSELoss  1e-06\n",
            "Epoch [41/250] ---- Training Loss: 0.0023 ---- Validation Loss: 0.0022    MSELoss  1e-06\n",
            "Epoch [42/250] ---- Training Loss: 0.0023 ---- Validation Loss: 0.0022    MSELoss  1e-06\n",
            "Epoch [43/250] ---- Training Loss: 0.0023 ---- Validation Loss: 0.0021    MSELoss  1e-06\n",
            "Epoch [44/250] ---- Training Loss: 0.0022 ---- Validation Loss: 0.0021    MSELoss  1e-06\n",
            "Epoch [45/250] ---- Training Loss: 0.0022 ---- Validation Loss: 0.0020    MSELoss  1e-06\n",
            "Epoch [46/250] ---- Training Loss: 0.0022 ---- Validation Loss: 0.0020    MSELoss  1e-06\n",
            "Epoch [47/250] ---- Training Loss: 0.0021 ---- Validation Loss: 0.0020    MSELoss  1e-06\n",
            "Epoch [48/250] ---- Training Loss: 0.0021 ---- Validation Loss: 0.0019    MSELoss  1e-06\n",
            "Epoch [49/250] ---- Training Loss: 0.0021 ---- Validation Loss: 0.0019    MSELoss  1e-06\n",
            "Epoch [50/250] ---- Training Loss: 0.0020 ---- Validation Loss: 0.0019    MSELoss  1e-06\n",
            "Epoch [51/250] ---- Training Loss: 0.0020 ---- Validation Loss: 0.0018    MSELoss  1e-06\n",
            "Epoch [52/250] ---- Training Loss: 0.0020 ---- Validation Loss: 0.0018    MSELoss  1e-06\n",
            "Epoch [53/250] ---- Training Loss: 0.0019 ---- Validation Loss: 0.0017    MSELoss  1e-06\n",
            "Epoch [54/250] ---- Training Loss: 0.0019 ---- Validation Loss: 0.0017    MSELoss  1e-06\n",
            "Epoch [55/250] ---- Training Loss: 0.0018 ---- Validation Loss: 0.0017    MSELoss  1e-06\n",
            "Epoch [56/250] ---- Training Loss: 0.0018 ---- Validation Loss: 0.0016    MSELoss  1e-06\n",
            "Epoch [57/250] ---- Training Loss: 0.0018 ---- Validation Loss: 0.0016    MSELoss  1e-06\n",
            "Epoch [58/250] ---- Training Loss: 0.0017 ---- Validation Loss: 0.0016    MSELoss  1e-06\n",
            "Epoch [59/250] ---- Training Loss: 0.0017 ---- Validation Loss: 0.0015    MSELoss  1e-06\n",
            "Epoch [60/250] ---- Training Loss: 0.0017 ---- Validation Loss: 0.0015    MSELoss  1e-06\n",
            "Epoch [61/250] ---- Training Loss: 0.0016 ---- Validation Loss: 0.0015    MSELoss  1e-06\n",
            "Epoch [62/250] ---- Training Loss: 0.0016 ---- Validation Loss: 0.0014    MSELoss  1e-06\n",
            "Epoch [63/250] ---- Training Loss: 0.0016 ---- Validation Loss: 0.0014    MSELoss  1e-06\n",
            "Epoch [64/250] ---- Training Loss: 0.0016 ---- Validation Loss: 0.0014    MSELoss  1e-06\n",
            "Epoch [65/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0014    MSELoss  1e-06\n",
            "Epoch [66/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0013    MSELoss  1e-06\n",
            "Epoch [67/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0013    MSELoss  1e-06\n",
            "Epoch [68/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0013    MSELoss  1e-06\n",
            "Epoch [69/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0013    MSELoss  1e-06\n",
            "Epoch [70/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0013    MSELoss  1e-06\n",
            "Epoch [71/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0012    MSELoss  1e-06\n",
            "Epoch [72/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0012    MSELoss  1e-06\n",
            "Epoch [73/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0012    MSELoss  1e-06\n",
            "Epoch [74/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0012    MSELoss  1e-06\n",
            "Epoch [75/250] ---- Training Loss: 0.0013 ---- Validation Loss: 0.0012    MSELoss  1e-06\n",
            "Epoch [76/250] ---- Training Loss: 0.0013 ---- Validation Loss: 0.0012    MSELoss  1e-06\n",
            "Epoch [77/250] ---- Training Loss: 0.0013 ---- Validation Loss: 0.0011    MSELoss  1e-06\n",
            "Epoch [78/250] ---- Training Loss: 0.0013 ---- Validation Loss: 0.0011    MSELoss  1e-06\n",
            "Epoch [79/250] ---- Training Loss: 0.0013 ---- Validation Loss: 0.0011    MSELoss  1e-06\n",
            "Epoch [80/250] ---- Training Loss: 0.0013 ---- Validation Loss: 0.0011    MSELoss  1e-06\n",
            "Epoch [81/250] ---- Training Loss: 0.0012 ---- Validation Loss: 0.0011    MSELoss  1e-06\n",
            "Epoch [82/250] ---- Training Loss: 0.0012 ---- Validation Loss: 0.0011    MSELoss  1e-06\n",
            "Epoch [83/250] ---- Training Loss: 0.0012 ---- Validation Loss: 0.0011    MSELoss  1e-06\n",
            "Epoch [84/250] ---- Training Loss: 0.0012 ---- Validation Loss: 0.0010    MSELoss  1e-06\n",
            "Epoch [85/250] ---- Training Loss: 0.0012 ---- Validation Loss: 0.0010    MSELoss  1e-06\n",
            "Epoch [86/250] ---- Training Loss: 0.0012 ---- Validation Loss: 0.0010    MSELoss  1e-06\n",
            "Epoch [87/250] ---- Training Loss: 0.0012 ---- Validation Loss: 0.0010    MSELoss  1e-06\n",
            "Epoch [88/250] ---- Training Loss: 0.0012 ---- Validation Loss: 0.0010    MSELoss  1e-06\n",
            "Epoch [89/250] ---- Training Loss: 0.0011 ---- Validation Loss: 0.0010    MSELoss  1e-06\n",
            "Epoch [90/250] ---- Training Loss: 0.0011 ---- Validation Loss: 0.0010    MSELoss  1e-06\n",
            "Epoch [91/250] ---- Training Loss: 0.0011 ---- Validation Loss: 0.0010    MSELoss  1e-06\n",
            "Epoch [92/250] ---- Training Loss: 0.0011 ---- Validation Loss: 0.0010    MSELoss  1e-06\n",
            "Epoch [93/250] ---- Training Loss: 0.0011 ---- Validation Loss: 0.0010    MSELoss  1e-06\n",
            "Epoch [94/250] ---- Training Loss: 0.0011 ---- Validation Loss: 0.0010    MSELoss  1e-06\n",
            "Epoch [95/250] ---- Training Loss: 0.0011 ---- Validation Loss: 0.0009    MSELoss  1e-06\n",
            "Epoch [96/250] ---- Training Loss: 0.0011 ---- Validation Loss: 0.0009    MSELoss  1e-06\n",
            "Epoch [97/250] ---- Training Loss: 0.0011 ---- Validation Loss: 0.0009    MSELoss  1e-06\n",
            "Epoch [98/250] ---- Training Loss: 0.0011 ---- Validation Loss: 0.0009    MSELoss  1e-06\n",
            "Epoch [99/250] ---- Training Loss: 0.0011 ---- Validation Loss: 0.0009    MSELoss  1e-06\n",
            "Epoch [100/250] ---- Training Loss: 0.0010 ---- Validation Loss: 0.0009    MSELoss  1e-06\n",
            "Epoch [101/250] ---- Training Loss: 0.0010 ---- Validation Loss: 0.0009    MSELoss  1e-06\n",
            "Epoch [102/250] ---- Training Loss: 0.0010 ---- Validation Loss: 0.0009    MSELoss  1e-06\n",
            "Epoch [103/250] ---- Training Loss: 0.0010 ---- Validation Loss: 0.0009    MSELoss  1e-06\n",
            "Epoch [104/250] ---- Training Loss: 0.0010 ---- Validation Loss: 0.0009    MSELoss  1e-06\n",
            "Epoch [105/250] ---- Training Loss: 0.0010 ---- Validation Loss: 0.0009    MSELoss  1e-06\n",
            "Epoch [106/250] ---- Training Loss: 0.0010 ---- Validation Loss: 0.0009    MSELoss  1e-06\n",
            "Epoch [107/250] ---- Training Loss: 0.0010 ---- Validation Loss: 0.0009    MSELoss  1e-06\n",
            "Epoch [108/250] ---- Training Loss: 0.0010 ---- Validation Loss: 0.0009    MSELoss  1e-06\n",
            "Epoch [109/250] ---- Training Loss: 0.0010 ---- Validation Loss: 0.0009    MSELoss  1e-06\n",
            "Epoch [110/250] ---- Training Loss: 0.0010 ---- Validation Loss: 0.0008    MSELoss  1e-06\n",
            "Epoch [111/250] ---- Training Loss: 0.0010 ---- Validation Loss: 0.0008    MSELoss  1e-06\n",
            "Epoch [112/250] ---- Training Loss: 0.0010 ---- Validation Loss: 0.0008    MSELoss  1e-06\n",
            "Epoch [113/250] ---- Training Loss: 0.0010 ---- Validation Loss: 0.0008    MSELoss  1e-06\n",
            "Epoch [114/250] ---- Training Loss: 0.0010 ---- Validation Loss: 0.0008    MSELoss  1e-06\n",
            "Epoch [115/250] ---- Training Loss: 0.0010 ---- Validation Loss: 0.0008    MSELoss  1e-06\n",
            "Epoch [116/250] ---- Training Loss: 0.0009 ---- Validation Loss: 0.0008    MSELoss  1e-06\n",
            "Epoch [117/250] ---- Training Loss: 0.0009 ---- Validation Loss: 0.0008    MSELoss  1e-06\n",
            "Epoch [118/250] ---- Training Loss: 0.0009 ---- Validation Loss: 0.0008    MSELoss  1e-06\n",
            "Epoch [119/250] ---- Training Loss: 0.0009 ---- Validation Loss: 0.0008    MSELoss  1e-06\n",
            "Epoch [120/250] ---- Training Loss: 0.0009 ---- Validation Loss: 0.0008    MSELoss  1e-06\n",
            "Epoch [121/250] ---- Training Loss: 0.0009 ---- Validation Loss: 0.0008    MSELoss  1e-06\n",
            "Epoch [122/250] ---- Training Loss: 0.0009 ---- Validation Loss: 0.0008    MSELoss  1e-06\n",
            "Epoch [123/250] ---- Training Loss: 0.0009 ---- Validation Loss: 0.0008    MSELoss  1e-06\n",
            "Epoch [124/250] ---- Training Loss: 0.0009 ---- Validation Loss: 0.0008    MSELoss  1e-06\n",
            "Epoch [125/250] ---- Training Loss: 0.0009 ---- Validation Loss: 0.0008    MSELoss  1e-06\n",
            "Epoch [126/250] ---- Training Loss: 0.0009 ---- Validation Loss: 0.0008    MSELoss  1e-06\n",
            "Epoch [127/250] ---- Training Loss: 0.0009 ---- Validation Loss: 0.0008    MSELoss  1e-06\n",
            "Epoch [128/250] ---- Training Loss: 0.0009 ---- Validation Loss: 0.0008    MSELoss  1e-06\n",
            "Epoch [129/250] ---- Training Loss: 0.0009 ---- Validation Loss: 0.0008    MSELoss  1e-06\n",
            "Epoch [130/250] ---- Training Loss: 0.0009 ---- Validation Loss: 0.0008    MSELoss  1e-06\n",
            "Epoch [131/250] ---- Training Loss: 0.0009 ---- Validation Loss: 0.0008    MSELoss  1e-06\n",
            "Epoch [132/250] ---- Training Loss: 0.0009 ---- Validation Loss: 0.0008    MSELoss  1e-06\n",
            "Epoch [133/250] ---- Training Loss: 0.0009 ---- Validation Loss: 0.0008    MSELoss  1e-06\n",
            "Epoch [134/250] ---- Training Loss: 0.0009 ---- Validation Loss: 0.0008    MSELoss  1e-06\n",
            "Epoch [135/250] ---- Training Loss: 0.0009 ---- Validation Loss: 0.0008    MSELoss  1e-06\n",
            "Epoch [136/250] ---- Training Loss: 0.0009 ---- Validation Loss: 0.0008    MSELoss  1e-06\n",
            "Epoch [137/250] ---- Training Loss: 0.0009 ---- Validation Loss: 0.0008    MSELoss  1e-06\n",
            "Epoch [138/250] ---- Training Loss: 0.0009 ---- Validation Loss: 0.0008    MSELoss  1e-06\n",
            "Epoch [139/250] ---- Training Loss: 0.0009 ---- Validation Loss: 0.0008    MSELoss  1e-06\n",
            "Epoch [140/250] ---- Training Loss: 0.0009 ---- Validation Loss: 0.0007    MSELoss  1e-06\n",
            "Epoch [141/250] ---- Training Loss: 0.0009 ---- Validation Loss: 0.0007    MSELoss  1e-06\n",
            "Epoch [142/250] ---- Training Loss: 0.0009 ---- Validation Loss: 0.0007    MSELoss  1e-06\n",
            "Epoch [143/250] ---- Training Loss: 0.0009 ---- Validation Loss: 0.0007    MSELoss  1e-06\n",
            "Epoch [144/250] ---- Training Loss: 0.0009 ---- Validation Loss: 0.0007    MSELoss  1e-06\n",
            "Epoch [145/250] ---- Training Loss: 0.0008 ---- Validation Loss: 0.0007    MSELoss  1e-06\n",
            "Epoch [146/250] ---- Training Loss: 0.0008 ---- Validation Loss: 0.0007    MSELoss  1e-06\n",
            "Epoch [147/250] ---- Training Loss: 0.0008 ---- Validation Loss: 0.0007    MSELoss  1e-06\n",
            "Epoch [148/250] ---- Training Loss: 0.0008 ---- Validation Loss: 0.0007    MSELoss  1e-06\n",
            "Epoch [149/250] ---- Training Loss: 0.0008 ---- Validation Loss: 0.0007    MSELoss  1e-06\n",
            "Epoch [150/250] ---- Training Loss: 0.0008 ---- Validation Loss: 0.0007    MSELoss  1e-06\n",
            "Epoch [151/250] ---- Training Loss: 0.0008 ---- Validation Loss: 0.0007    MSELoss  1e-06\n",
            "Epoch [152/250] ---- Training Loss: 0.0008 ---- Validation Loss: 0.0007    MSELoss  1e-06\n",
            "Epoch [153/250] ---- Training Loss: 0.0008 ---- Validation Loss: 0.0007    MSELoss  1e-06\n",
            "Epoch [154/250] ---- Training Loss: 0.0008 ---- Validation Loss: 0.0007    MSELoss  1e-06\n",
            "Epoch [155/250] ---- Training Loss: 0.0008 ---- Validation Loss: 0.0007    MSELoss  1e-06\n",
            "Epoch [156/250] ---- Training Loss: 0.0008 ---- Validation Loss: 0.0007    MSELoss  1e-06\n",
            "Epoch [157/250] ---- Training Loss: 0.0008 ---- Validation Loss: 0.0007    MSELoss  1e-06\n",
            "Epoch [158/250] ---- Training Loss: 0.0008 ---- Validation Loss: 0.0007    MSELoss  1e-06\n",
            "Epoch [159/250] ---- Training Loss: 0.0008 ---- Validation Loss: 0.0007    MSELoss  1e-06\n",
            "Epoch [160/250] ---- Training Loss: 0.0008 ---- Validation Loss: 0.0007    MSELoss  1e-06\n",
            "Epoch [161/250] ---- Training Loss: 0.0008 ---- Validation Loss: 0.0007    MSELoss  1e-06\n",
            "Epoch [162/250] ---- Training Loss: 0.0008 ---- Validation Loss: 0.0007    MSELoss  1e-06\n",
            "Epoch [163/250] ---- Training Loss: 0.0008 ---- Validation Loss: 0.0007    MSELoss  1e-06\n",
            "Epoch [164/250] ---- Training Loss: 0.0008 ---- Validation Loss: 0.0007    MSELoss  1e-06\n",
            "Epoch [165/250] ---- Training Loss: 0.0008 ---- Validation Loss: 0.0007    MSELoss  1e-06\n",
            "Epoch [166/250] ---- Training Loss: 0.0008 ---- Validation Loss: 0.0007    MSELoss  1e-06\n",
            "Epoch [167/250] ---- Training Loss: 0.0008 ---- Validation Loss: 0.0007    MSELoss  1e-06\n",
            "Epoch [168/250] ---- Training Loss: 0.0008 ---- Validation Loss: 0.0007    MSELoss  1e-06\n",
            "Epoch [169/250] ---- Training Loss: 0.0008 ---- Validation Loss: 0.0007    MSELoss  1e-06\n",
            "Epoch [170/250] ---- Training Loss: 0.0008 ---- Validation Loss: 0.0007    MSELoss  1e-06\n",
            "Epoch [171/250] ---- Training Loss: 0.0008 ---- Validation Loss: 0.0007    MSELoss  1e-06\n",
            "Epoch [172/250] ---- Training Loss: 0.0008 ---- Validation Loss: 0.0007    MSELoss  1e-06\n",
            "Epoch [173/250] ---- Training Loss: 0.0008 ---- Validation Loss: 0.0007    MSELoss  1e-06\n",
            "Epoch [174/250] ---- Training Loss: 0.0008 ---- Validation Loss: 0.0007    MSELoss  1e-06\n",
            "Epoch [175/250] ---- Training Loss: 0.0008 ---- Validation Loss: 0.0007    MSELoss  1e-06\n",
            "Epoch [176/250] ---- Training Loss: 0.0008 ---- Validation Loss: 0.0007    MSELoss  1e-06\n",
            "Epoch [177/250] ---- Training Loss: 0.0008 ---- Validation Loss: 0.0007    MSELoss  1e-06\n",
            "Epoch [178/250] ---- Training Loss: 0.0008 ---- Validation Loss: 0.0007    MSELoss  1e-06\n",
            "Epoch [179/250] ---- Training Loss: 0.0008 ---- Validation Loss: 0.0007    MSELoss  1e-06\n",
            "Epoch [180/250] ---- Training Loss: 0.0008 ---- Validation Loss: 0.0007    MSELoss  1e-06\n",
            "Epoch [181/250] ---- Training Loss: 0.0008 ---- Validation Loss: 0.0007    MSELoss  1e-06\n",
            "Epoch [182/250] ---- Training Loss: 0.0008 ---- Validation Loss: 0.0007    MSELoss  1e-06\n",
            "Epoch [183/250] ---- Training Loss: 0.0008 ---- Validation Loss: 0.0007    MSELoss  1e-06\n",
            "Epoch [184/250] ---- Training Loss: 0.0008 ---- Validation Loss: 0.0007    MSELoss  1e-06\n",
            "Epoch [185/250] ---- Training Loss: 0.0008 ---- Validation Loss: 0.0007    MSELoss  1e-06\n",
            "Epoch [186/250] ---- Training Loss: 0.0008 ---- Validation Loss: 0.0007    MSELoss  1e-06\n",
            "Epoch [187/250] ---- Training Loss: 0.0008 ---- Validation Loss: 0.0007    MSELoss  1e-06\n",
            "Epoch [188/250] ---- Training Loss: 0.0008 ---- Validation Loss: 0.0007    MSELoss  1e-06\n",
            "Epoch [189/250] ---- Training Loss: 0.0008 ---- Validation Loss: 0.0007    MSELoss  1e-06\n",
            "Epoch [190/250] ---- Training Loss: 0.0008 ---- Validation Loss: 0.0007    MSELoss  1e-06\n",
            "Epoch [191/250] ---- Training Loss: 0.0008 ---- Validation Loss: 0.0007    MSELoss  1e-06\n",
            "Epoch [192/250] ---- Training Loss: 0.0008 ---- Validation Loss: 0.0007    MSELoss  1e-06\n",
            "Epoch [193/250] ---- Training Loss: 0.0008 ---- Validation Loss: 0.0007    MSELoss  1e-06\n",
            "Epoch [194/250] ---- Training Loss: 0.0008 ---- Validation Loss: 0.0007    MSELoss  1e-06\n",
            "Epoch [195/250] ---- Training Loss: 0.0008 ---- Validation Loss: 0.0007    MSELoss  1e-06\n",
            "Epoch [196/250] ---- Training Loss: 0.0008 ---- Validation Loss: 0.0007    MSELoss  1e-06\n",
            "Epoch [197/250] ---- Training Loss: 0.0008 ---- Validation Loss: 0.0006    MSELoss  1e-06\n",
            "Epoch [198/250] ---- Training Loss: 0.0008 ---- Validation Loss: 0.0006    MSELoss  1e-06\n",
            "Epoch [199/250] ---- Training Loss: 0.0007 ---- Validation Loss: 0.0006    MSELoss  1e-06\n",
            "Epoch [200/250] ---- Training Loss: 0.0007 ---- Validation Loss: 0.0006    MSELoss  1e-06\n",
            "Epoch [201/250] ---- Training Loss: 0.0007 ---- Validation Loss: 0.0006    MSELoss  1e-06\n",
            "Epoch [202/250] ---- Training Loss: 0.0007 ---- Validation Loss: 0.0006    MSELoss  1e-06\n",
            "Epoch [203/250] ---- Training Loss: 0.0007 ---- Validation Loss: 0.0006    MSELoss  1e-06\n",
            "Epoch [204/250] ---- Training Loss: 0.0007 ---- Validation Loss: 0.0006    MSELoss  1e-06\n",
            "Epoch [205/250] ---- Training Loss: 0.0007 ---- Validation Loss: 0.0006    MSELoss  1e-06\n",
            "Epoch [206/250] ---- Training Loss: 0.0007 ---- Validation Loss: 0.0006    MSELoss  1e-06\n",
            "Epoch [207/250] ---- Training Loss: 0.0007 ---- Validation Loss: 0.0006    MSELoss  1e-06\n",
            "Epoch [208/250] ---- Training Loss: 0.0007 ---- Validation Loss: 0.0006    MSELoss  1e-06\n",
            "Epoch [209/250] ---- Training Loss: 0.0007 ---- Validation Loss: 0.0006    MSELoss  1e-06\n",
            "Epoch [210/250] ---- Training Loss: 0.0007 ---- Validation Loss: 0.0006    MSELoss  1e-06\n",
            "Epoch [211/250] ---- Training Loss: 0.0007 ---- Validation Loss: 0.0006    MSELoss  1e-06\n",
            "Epoch [212/250] ---- Training Loss: 0.0007 ---- Validation Loss: 0.0006    MSELoss  1e-06\n",
            "Epoch [213/250] ---- Training Loss: 0.0007 ---- Validation Loss: 0.0006    MSELoss  1e-06\n",
            "Epoch [214/250] ---- Training Loss: 0.0007 ---- Validation Loss: 0.0006    MSELoss  1e-06\n",
            "Epoch [215/250] ---- Training Loss: 0.0007 ---- Validation Loss: 0.0006    MSELoss  1e-06\n",
            "Epoch [216/250] ---- Training Loss: 0.0007 ---- Validation Loss: 0.0006    MSELoss  1e-06\n",
            "Epoch [217/250] ---- Training Loss: 0.0007 ---- Validation Loss: 0.0006    MSELoss  1e-06\n",
            "Epoch [218/250] ---- Training Loss: 0.0007 ---- Validation Loss: 0.0006    MSELoss  1e-06\n",
            "Epoch [219/250] ---- Training Loss: 0.0007 ---- Validation Loss: 0.0006    MSELoss  1e-06\n",
            "Epoch [220/250] ---- Training Loss: 0.0007 ---- Validation Loss: 0.0006    MSELoss  1e-06\n",
            "Epoch [221/250] ---- Training Loss: 0.0007 ---- Validation Loss: 0.0006    MSELoss  1e-06\n",
            "Epoch [222/250] ---- Training Loss: 0.0007 ---- Validation Loss: 0.0006    MSELoss  1e-06\n",
            "Epoch [223/250] ---- Training Loss: 0.0007 ---- Validation Loss: 0.0006    MSELoss  1e-06\n",
            "Epoch [224/250] ---- Training Loss: 0.0007 ---- Validation Loss: 0.0006    MSELoss  1e-06\n",
            "Epoch [225/250] ---- Training Loss: 0.0007 ---- Validation Loss: 0.0006    MSELoss  1e-06\n",
            "Epoch [226/250] ---- Training Loss: 0.0007 ---- Validation Loss: 0.0006    MSELoss  1e-06\n",
            "Epoch [227/250] ---- Training Loss: 0.0007 ---- Validation Loss: 0.0006    MSELoss  1e-06\n",
            "Epoch [228/250] ---- Training Loss: 0.0007 ---- Validation Loss: 0.0006    MSELoss  1e-06\n",
            "Epoch [229/250] ---- Training Loss: 0.0007 ---- Validation Loss: 0.0006    MSELoss  1e-06\n",
            "Epoch [230/250] ---- Training Loss: 0.0007 ---- Validation Loss: 0.0006    MSELoss  1e-06\n",
            "Epoch [231/250] ---- Training Loss: 0.0007 ---- Validation Loss: 0.0006    MSELoss  1e-06\n",
            "Epoch [232/250] ---- Training Loss: 0.0007 ---- Validation Loss: 0.0006    MSELoss  1e-06\n",
            "Epoch [233/250] ---- Training Loss: 0.0007 ---- Validation Loss: 0.0006    MSELoss  1e-06\n",
            "Epoch [234/250] ---- Training Loss: 0.0007 ---- Validation Loss: 0.0006    MSELoss  1e-06\n",
            "Epoch [235/250] ---- Training Loss: 0.0007 ---- Validation Loss: 0.0006    MSELoss  1e-06\n",
            "Epoch [236/250] ---- Training Loss: 0.0007 ---- Validation Loss: 0.0006    MSELoss  1e-06\n",
            "Epoch [237/250] ---- Training Loss: 0.0007 ---- Validation Loss: 0.0006    MSELoss  1e-06\n",
            "Epoch [238/250] ---- Training Loss: 0.0007 ---- Validation Loss: 0.0006    MSELoss  1e-06\n",
            "Epoch [239/250] ---- Training Loss: 0.0007 ---- Validation Loss: 0.0006    MSELoss  1e-06\n",
            "Epoch [240/250] ---- Training Loss: 0.0007 ---- Validation Loss: 0.0006    MSELoss  1e-06\n",
            "Epoch [241/250] ---- Training Loss: 0.0007 ---- Validation Loss: 0.0006    MSELoss  1e-06\n",
            "Epoch [242/250] ---- Training Loss: 0.0007 ---- Validation Loss: 0.0006    MSELoss  1e-06\n",
            "Epoch [243/250] ---- Training Loss: 0.0007 ---- Validation Loss: 0.0006    MSELoss  1e-06\n",
            "Epoch [244/250] ---- Training Loss: 0.0007 ---- Validation Loss: 0.0006    MSELoss  1e-06\n",
            "Epoch [245/250] ---- Training Loss: 0.0007 ---- Validation Loss: 0.0006    MSELoss  1e-06\n",
            "Epoch [246/250] ---- Training Loss: 0.0007 ---- Validation Loss: 0.0006    MSELoss  1e-06\n",
            "Epoch [247/250] ---- Training Loss: 0.0007 ---- Validation Loss: 0.0006    MSELoss  1e-06\n",
            "Epoch [248/250] ---- Training Loss: 0.0007 ---- Validation Loss: 0.0006    MSELoss  1e-06\n",
            "Epoch [249/250] ---- Training Loss: 0.0007 ---- Validation Loss: 0.0006    MSELoss  1e-06\n",
            "Epoch [250/250] ---- Training Loss: 0.0007 ---- Validation Loss: 0.0006    MSELoss  1e-06\n",
            "Saved model: model_loss_MSELoss_wd_1e-06.pth\n",
            "Epoch [1/250] ---- Training Loss: 0.0027 ---- Validation Loss: 0.0023    SmoothL1Loss  1e-04\n",
            "Epoch [2/250] ---- Training Loss: 0.0024 ---- Validation Loss: 0.0022    SmoothL1Loss  1e-04\n",
            "Epoch [3/250] ---- Training Loss: 0.0023 ---- Validation Loss: 0.0021    SmoothL1Loss  1e-04\n",
            "Epoch [4/250] ---- Training Loss: 0.0022 ---- Validation Loss: 0.0021    SmoothL1Loss  1e-04\n",
            "Epoch [5/250] ---- Training Loss: 0.0021 ---- Validation Loss: 0.0020    SmoothL1Loss  1e-04\n",
            "Epoch [6/250] ---- Training Loss: 0.0021 ---- Validation Loss: 0.0020    SmoothL1Loss  1e-04\n",
            "Epoch [7/250] ---- Training Loss: 0.0020 ---- Validation Loss: 0.0019    SmoothL1Loss  1e-04\n",
            "Epoch [8/250] ---- Training Loss: 0.0020 ---- Validation Loss: 0.0019    SmoothL1Loss  1e-04\n",
            "Epoch [9/250] ---- Training Loss: 0.0020 ---- Validation Loss: 0.0019    SmoothL1Loss  1e-04\n",
            "Epoch [10/250] ---- Training Loss: 0.0019 ---- Validation Loss: 0.0019    SmoothL1Loss  1e-04\n",
            "Epoch [11/250] ---- Training Loss: 0.0019 ---- Validation Loss: 0.0018    SmoothL1Loss  1e-04\n",
            "Epoch [12/250] ---- Training Loss: 0.0019 ---- Validation Loss: 0.0018    SmoothL1Loss  1e-04\n",
            "Epoch [13/250] ---- Training Loss: 0.0019 ---- Validation Loss: 0.0018    SmoothL1Loss  1e-04\n",
            "Epoch [14/250] ---- Training Loss: 0.0018 ---- Validation Loss: 0.0018    SmoothL1Loss  1e-04\n",
            "Epoch [15/250] ---- Training Loss: 0.0018 ---- Validation Loss: 0.0018    SmoothL1Loss  1e-04\n",
            "Epoch [16/250] ---- Training Loss: 0.0018 ---- Validation Loss: 0.0018    SmoothL1Loss  1e-04\n",
            "Epoch [17/250] ---- Training Loss: 0.0018 ---- Validation Loss: 0.0017    SmoothL1Loss  1e-04\n",
            "Epoch [18/250] ---- Training Loss: 0.0018 ---- Validation Loss: 0.0017    SmoothL1Loss  1e-04\n",
            "Epoch [19/250] ---- Training Loss: 0.0017 ---- Validation Loss: 0.0017    SmoothL1Loss  1e-04\n",
            "Epoch [20/250] ---- Training Loss: 0.0017 ---- Validation Loss: 0.0017    SmoothL1Loss  1e-04\n",
            "Epoch [21/250] ---- Training Loss: 0.0017 ---- Validation Loss: 0.0017    SmoothL1Loss  1e-04\n",
            "Epoch [22/250] ---- Training Loss: 0.0017 ---- Validation Loss: 0.0017    SmoothL1Loss  1e-04\n",
            "Epoch [23/250] ---- Training Loss: 0.0017 ---- Validation Loss: 0.0016    SmoothL1Loss  1e-04\n",
            "Epoch [24/250] ---- Training Loss: 0.0017 ---- Validation Loss: 0.0016    SmoothL1Loss  1e-04\n",
            "Epoch [25/250] ---- Training Loss: 0.0016 ---- Validation Loss: 0.0016    SmoothL1Loss  1e-04\n",
            "Epoch [26/250] ---- Training Loss: 0.0016 ---- Validation Loss: 0.0016    SmoothL1Loss  1e-04\n",
            "Epoch [27/250] ---- Training Loss: 0.0016 ---- Validation Loss: 0.0016    SmoothL1Loss  1e-04\n",
            "Epoch [28/250] ---- Training Loss: 0.0016 ---- Validation Loss: 0.0016    SmoothL1Loss  1e-04\n",
            "Epoch [29/250] ---- Training Loss: 0.0016 ---- Validation Loss: 0.0016    SmoothL1Loss  1e-04\n",
            "Epoch [30/250] ---- Training Loss: 0.0016 ---- Validation Loss: 0.0016    SmoothL1Loss  1e-04\n",
            "Epoch [31/250] ---- Training Loss: 0.0016 ---- Validation Loss: 0.0016    SmoothL1Loss  1e-04\n",
            "Epoch [32/250] ---- Training Loss: 0.0016 ---- Validation Loss: 0.0016    SmoothL1Loss  1e-04\n",
            "Epoch [33/250] ---- Training Loss: 0.0016 ---- Validation Loss: 0.0016    SmoothL1Loss  1e-04\n",
            "Epoch [34/250] ---- Training Loss: 0.0016 ---- Validation Loss: 0.0016    SmoothL1Loss  1e-04\n",
            "Epoch [35/250] ---- Training Loss: 0.0016 ---- Validation Loss: 0.0015    SmoothL1Loss  1e-04\n",
            "Epoch [36/250] ---- Training Loss: 0.0016 ---- Validation Loss: 0.0015    SmoothL1Loss  1e-04\n",
            "Epoch [37/250] ---- Training Loss: 0.0016 ---- Validation Loss: 0.0015    SmoothL1Loss  1e-04\n",
            "Epoch [38/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  1e-04\n",
            "Epoch [39/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  1e-04\n",
            "Epoch [40/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  1e-04\n",
            "Epoch [41/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  1e-04\n",
            "Epoch [42/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  1e-04\n",
            "Epoch [43/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  1e-04\n",
            "Epoch [44/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  1e-04\n",
            "Epoch [45/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  1e-04\n",
            "Epoch [46/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  1e-04\n",
            "Epoch [47/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  1e-04\n",
            "Epoch [48/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  1e-04\n",
            "Epoch [49/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  1e-04\n",
            "Epoch [50/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  1e-04\n",
            "Epoch [51/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  1e-04\n",
            "Epoch [52/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  1e-04\n",
            "Epoch [53/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  1e-04\n",
            "Epoch [54/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  1e-04\n",
            "Epoch [55/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  1e-04\n",
            "Epoch [56/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  1e-04\n",
            "Epoch [57/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  1e-04\n",
            "Epoch [58/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  1e-04\n",
            "Epoch [59/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  1e-04\n",
            "Epoch [60/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  1e-04\n",
            "Epoch [61/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  1e-04\n",
            "Epoch [62/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  1e-04\n",
            "Epoch [63/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  1e-04\n",
            "Epoch [64/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  1e-04\n",
            "Epoch [65/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  1e-04\n",
            "Epoch [66/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  1e-04\n",
            "Epoch [67/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  1e-04\n",
            "Epoch [68/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  1e-04\n",
            "Epoch [69/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  1e-04\n",
            "Epoch [70/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  1e-04\n",
            "Epoch [71/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  1e-04\n",
            "Epoch [72/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  1e-04\n",
            "Epoch [73/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  1e-04\n",
            "Epoch [74/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  1e-04\n",
            "Epoch [75/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  1e-04\n",
            "Epoch [76/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  1e-04\n",
            "Epoch [77/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  1e-04\n",
            "Epoch [78/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  1e-04\n",
            "Epoch [79/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  1e-04\n",
            "Epoch [80/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  1e-04\n",
            "Epoch [81/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  1e-04\n",
            "Epoch [82/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  1e-04\n",
            "Epoch [83/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  1e-04\n",
            "Epoch [84/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  1e-04\n",
            "Epoch [85/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  1e-04\n",
            "Epoch [86/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  1e-04\n",
            "Epoch [87/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  1e-04\n",
            "Epoch [88/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  1e-04\n",
            "Epoch [89/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  1e-04\n",
            "Epoch [90/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  1e-04\n",
            "Epoch [91/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  1e-04\n",
            "Epoch [92/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  1e-04\n",
            "Epoch [93/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  1e-04\n",
            "Epoch [94/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  1e-04\n",
            "Epoch [95/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  1e-04\n",
            "Epoch [96/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  1e-04\n",
            "Epoch [97/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  1e-04\n",
            "Epoch [98/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  1e-04\n",
            "Epoch [99/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  1e-04\n",
            "Epoch [100/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  1e-04\n",
            "Epoch [101/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  1e-04\n",
            "Epoch [102/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  1e-04\n",
            "Epoch [103/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  1e-04\n",
            "Epoch [104/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  1e-04\n",
            "Epoch [105/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  1e-04\n",
            "Epoch [106/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  1e-04\n",
            "Epoch [107/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  1e-04\n",
            "Epoch [108/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  1e-04\n",
            "Epoch [109/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  1e-04\n",
            "Epoch [110/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  1e-04\n",
            "Epoch [111/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  1e-04\n",
            "Epoch [112/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  1e-04\n",
            "Epoch [113/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  1e-04\n",
            "Epoch [114/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  1e-04\n",
            "Epoch [115/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  1e-04\n",
            "Epoch [116/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  1e-04\n",
            "Epoch [117/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  1e-04\n",
            "Epoch [118/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  1e-04\n",
            "Epoch [119/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  1e-04\n",
            "Epoch [120/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  1e-04\n",
            "Epoch [121/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  1e-04\n",
            "Epoch [122/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  1e-04\n",
            "Epoch [123/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  1e-04\n",
            "Epoch [124/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  1e-04\n",
            "Epoch [125/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  1e-04\n",
            "Epoch [126/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  1e-04\n",
            "Epoch [127/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  1e-04\n",
            "Epoch [128/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  1e-04\n",
            "Epoch [129/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  1e-04\n",
            "Epoch [130/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  1e-04\n",
            "Epoch [131/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  1e-04\n",
            "Epoch [132/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  1e-04\n",
            "Epoch [133/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  1e-04\n",
            "Epoch [134/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  1e-04\n",
            "Epoch [135/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  1e-04\n",
            "Epoch [136/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  1e-04\n",
            "Epoch [137/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  1e-04\n",
            "Epoch [138/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-04\n",
            "Epoch [139/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-04\n",
            "Epoch [140/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-04\n",
            "Epoch [141/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-04\n",
            "Epoch [142/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-04\n",
            "Epoch [143/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-04\n",
            "Epoch [144/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-04\n",
            "Epoch [145/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-04\n",
            "Epoch [146/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-04\n",
            "Epoch [147/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-04\n",
            "Epoch [148/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-04\n",
            "Epoch [149/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-04\n",
            "Epoch [150/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-04\n",
            "Epoch [151/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-04\n",
            "Epoch [152/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-04\n",
            "Epoch [153/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-04\n",
            "Epoch [154/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-04\n",
            "Epoch [155/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-04\n",
            "Epoch [156/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-04\n",
            "Epoch [157/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-04\n",
            "Epoch [158/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-04\n",
            "Epoch [159/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-04\n",
            "Epoch [160/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-04\n",
            "Epoch [161/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-04\n",
            "Epoch [162/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-04\n",
            "Epoch [163/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-04\n",
            "Epoch [164/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-04\n",
            "Epoch [165/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-04\n",
            "Epoch [166/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-04\n",
            "Epoch [167/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-04\n",
            "Epoch [168/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-04\n",
            "Epoch [169/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-04\n",
            "Epoch [170/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-04\n",
            "Epoch [171/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-04\n",
            "Epoch [172/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-04\n",
            "Epoch [173/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-04\n",
            "Epoch [174/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-04\n",
            "Epoch [175/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-04\n",
            "Epoch [176/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-04\n",
            "Epoch [177/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-04\n",
            "Epoch [178/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-04\n",
            "Epoch [179/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-04\n",
            "Epoch [180/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-04\n",
            "Epoch [181/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-04\n",
            "Epoch [182/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-04\n",
            "Epoch [183/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-04\n",
            "Epoch [184/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-04\n",
            "Epoch [185/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-04\n",
            "Epoch [186/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-04\n",
            "Epoch [187/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-04\n",
            "Epoch [188/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-04\n",
            "Epoch [189/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-04\n",
            "Epoch [190/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-04\n",
            "Epoch [191/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-04\n",
            "Epoch [192/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-04\n",
            "Epoch [193/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-04\n",
            "Epoch [194/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-04\n",
            "Epoch [195/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-04\n",
            "Epoch [196/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-04\n",
            "Epoch [197/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-04\n",
            "Epoch [198/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-04\n",
            "Epoch [199/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-04\n",
            "Epoch [200/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-04\n",
            "Epoch [201/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-04\n",
            "Epoch [202/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-04\n",
            "Epoch [203/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-04\n",
            "Epoch [204/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-04\n",
            "Epoch [205/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-04\n",
            "Epoch [206/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-04\n",
            "Epoch [207/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-04\n",
            "Epoch [208/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-04\n",
            "Epoch [209/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-04\n",
            "Epoch [210/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-04\n",
            "Epoch [211/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-04\n",
            "Epoch [212/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-04\n",
            "Epoch [213/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-04\n",
            "Epoch [214/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-04\n",
            "Epoch [215/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-04\n",
            "Epoch [216/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-04\n",
            "Epoch [217/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-04\n",
            "Epoch [218/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-04\n",
            "Epoch [219/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-04\n",
            "Epoch [220/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-04\n",
            "Epoch [221/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-04\n",
            "Epoch [222/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-04\n",
            "Epoch [223/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-04\n",
            "Epoch [224/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-04\n",
            "Epoch [225/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-04\n",
            "Epoch [226/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-04\n",
            "Epoch [227/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-04\n",
            "Epoch [228/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-04\n",
            "Epoch [229/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-04\n",
            "Epoch [230/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-04\n",
            "Epoch [231/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-04\n",
            "Epoch [232/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-04\n",
            "Epoch [233/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-04\n",
            "Epoch [234/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-04\n",
            "Epoch [235/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-04\n",
            "Epoch [236/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-04\n",
            "Epoch [237/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-04\n",
            "Epoch [238/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-04\n",
            "Epoch [239/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-04\n",
            "Epoch [240/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-04\n",
            "Epoch [241/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-04\n",
            "Epoch [242/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-04\n",
            "Epoch [243/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-04\n",
            "Epoch [244/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-04\n",
            "Epoch [245/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-04\n",
            "Epoch [246/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-04\n",
            "Epoch [247/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-04\n",
            "Epoch [248/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-04\n",
            "Epoch [249/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-04\n",
            "Epoch [250/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-04\n",
            "Saved model: model_loss_SmoothL1Loss_wd_1e-04.pth\n",
            "Epoch [1/250] ---- Training Loss: 0.0033 ---- Validation Loss: 0.0022    SmoothL1Loss  1e-05\n",
            "Epoch [2/250] ---- Training Loss: 0.0025 ---- Validation Loss: 0.0021    SmoothL1Loss  1e-05\n",
            "Epoch [3/250] ---- Training Loss: 0.0024 ---- Validation Loss: 0.0021    SmoothL1Loss  1e-05\n",
            "Epoch [4/250] ---- Training Loss: 0.0023 ---- Validation Loss: 0.0020    SmoothL1Loss  1e-05\n",
            "Epoch [5/250] ---- Training Loss: 0.0022 ---- Validation Loss: 0.0019    SmoothL1Loss  1e-05\n",
            "Epoch [6/250] ---- Training Loss: 0.0021 ---- Validation Loss: 0.0019    SmoothL1Loss  1e-05\n",
            "Epoch [7/250] ---- Training Loss: 0.0021 ---- Validation Loss: 0.0019    SmoothL1Loss  1e-05\n",
            "Epoch [8/250] ---- Training Loss: 0.0020 ---- Validation Loss: 0.0018    SmoothL1Loss  1e-05\n",
            "Epoch [9/250] ---- Training Loss: 0.0019 ---- Validation Loss: 0.0018    SmoothL1Loss  1e-05\n",
            "Epoch [10/250] ---- Training Loss: 0.0019 ---- Validation Loss: 0.0018    SmoothL1Loss  1e-05\n",
            "Epoch [11/250] ---- Training Loss: 0.0019 ---- Validation Loss: 0.0017    SmoothL1Loss  1e-05\n",
            "Epoch [12/250] ---- Training Loss: 0.0018 ---- Validation Loss: 0.0017    SmoothL1Loss  1e-05\n",
            "Epoch [13/250] ---- Training Loss: 0.0018 ---- Validation Loss: 0.0017    SmoothL1Loss  1e-05\n",
            "Epoch [14/250] ---- Training Loss: 0.0017 ---- Validation Loss: 0.0016    SmoothL1Loss  1e-05\n",
            "Epoch [15/250] ---- Training Loss: 0.0017 ---- Validation Loss: 0.0016    SmoothL1Loss  1e-05\n",
            "Epoch [16/250] ---- Training Loss: 0.0017 ---- Validation Loss: 0.0016    SmoothL1Loss  1e-05\n",
            "Epoch [17/250] ---- Training Loss: 0.0016 ---- Validation Loss: 0.0016    SmoothL1Loss  1e-05\n",
            "Epoch [18/250] ---- Training Loss: 0.0016 ---- Validation Loss: 0.0016    SmoothL1Loss  1e-05\n",
            "Epoch [19/250] ---- Training Loss: 0.0016 ---- Validation Loss: 0.0015    SmoothL1Loss  1e-05\n",
            "Epoch [20/250] ---- Training Loss: 0.0016 ---- Validation Loss: 0.0015    SmoothL1Loss  1e-05\n",
            "Epoch [21/250] ---- Training Loss: 0.0016 ---- Validation Loss: 0.0015    SmoothL1Loss  1e-05\n",
            "Epoch [22/250] ---- Training Loss: 0.0016 ---- Validation Loss: 0.0015    SmoothL1Loss  1e-05\n",
            "Epoch [23/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  1e-05\n",
            "Epoch [24/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  1e-05\n",
            "Epoch [25/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  1e-05\n",
            "Epoch [26/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  1e-05\n",
            "Epoch [27/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  1e-05\n",
            "Epoch [28/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  1e-05\n",
            "Epoch [29/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-05\n",
            "Epoch [30/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-05\n",
            "Epoch [31/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-05\n",
            "Epoch [32/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-05\n",
            "Epoch [33/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-05\n",
            "Epoch [34/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-05\n",
            "Epoch [35/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-05\n",
            "Epoch [36/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-05\n",
            "Epoch [37/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-05\n",
            "Epoch [38/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-05\n",
            "Epoch [39/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-05\n",
            "Epoch [40/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-05\n",
            "Epoch [41/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-05\n",
            "Epoch [42/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-05\n",
            "Epoch [43/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-05\n",
            "Epoch [44/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-05\n",
            "Epoch [45/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0013    SmoothL1Loss  1e-05\n",
            "Epoch [46/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0013    SmoothL1Loss  1e-05\n",
            "Epoch [47/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0013    SmoothL1Loss  1e-05\n",
            "Epoch [48/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0013    SmoothL1Loss  1e-05\n",
            "Epoch [49/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0013    SmoothL1Loss  1e-05\n",
            "Epoch [50/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0013    SmoothL1Loss  1e-05\n",
            "Epoch [51/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0013    SmoothL1Loss  1e-05\n",
            "Epoch [52/250] ---- Training Loss: 0.0013 ---- Validation Loss: 0.0013    SmoothL1Loss  1e-05\n",
            "Epoch [53/250] ---- Training Loss: 0.0013 ---- Validation Loss: 0.0013    SmoothL1Loss  1e-05\n",
            "Epoch [54/250] ---- Training Loss: 0.0013 ---- Validation Loss: 0.0013    SmoothL1Loss  1e-05\n",
            "Epoch [55/250] ---- Training Loss: 0.0013 ---- Validation Loss: 0.0013    SmoothL1Loss  1e-05\n",
            "Epoch [56/250] ---- Training Loss: 0.0013 ---- Validation Loss: 0.0013    SmoothL1Loss  1e-05\n",
            "Epoch [57/250] ---- Training Loss: 0.0013 ---- Validation Loss: 0.0013    SmoothL1Loss  1e-05\n",
            "Epoch [58/250] ---- Training Loss: 0.0013 ---- Validation Loss: 0.0013    SmoothL1Loss  1e-05\n",
            "Epoch [59/250] ---- Training Loss: 0.0013 ---- Validation Loss: 0.0013    SmoothL1Loss  1e-05\n",
            "Epoch [60/250] ---- Training Loss: 0.0013 ---- Validation Loss: 0.0013    SmoothL1Loss  1e-05\n",
            "Epoch [61/250] ---- Training Loss: 0.0013 ---- Validation Loss: 0.0013    SmoothL1Loss  1e-05\n",
            "Epoch [62/250] ---- Training Loss: 0.0013 ---- Validation Loss: 0.0012    SmoothL1Loss  1e-05\n",
            "Epoch [63/250] ---- Training Loss: 0.0013 ---- Validation Loss: 0.0012    SmoothL1Loss  1e-05\n",
            "Epoch [64/250] ---- Training Loss: 0.0013 ---- Validation Loss: 0.0012    SmoothL1Loss  1e-05\n",
            "Epoch [65/250] ---- Training Loss: 0.0013 ---- Validation Loss: 0.0012    SmoothL1Loss  1e-05\n",
            "Epoch [66/250] ---- Training Loss: 0.0013 ---- Validation Loss: 0.0012    SmoothL1Loss  1e-05\n",
            "Epoch [67/250] ---- Training Loss: 0.0012 ---- Validation Loss: 0.0012    SmoothL1Loss  1e-05\n",
            "Epoch [68/250] ---- Training Loss: 0.0012 ---- Validation Loss: 0.0012    SmoothL1Loss  1e-05\n",
            "Epoch [69/250] ---- Training Loss: 0.0012 ---- Validation Loss: 0.0012    SmoothL1Loss  1e-05\n",
            "Epoch [70/250] ---- Training Loss: 0.0012 ---- Validation Loss: 0.0011    SmoothL1Loss  1e-05\n",
            "Epoch [71/250] ---- Training Loss: 0.0012 ---- Validation Loss: 0.0011    SmoothL1Loss  1e-05\n",
            "Epoch [72/250] ---- Training Loss: 0.0012 ---- Validation Loss: 0.0011    SmoothL1Loss  1e-05\n",
            "Epoch [73/250] ---- Training Loss: 0.0012 ---- Validation Loss: 0.0011    SmoothL1Loss  1e-05\n",
            "Epoch [74/250] ---- Training Loss: 0.0012 ---- Validation Loss: 0.0011    SmoothL1Loss  1e-05\n",
            "Epoch [75/250] ---- Training Loss: 0.0012 ---- Validation Loss: 0.0011    SmoothL1Loss  1e-05\n",
            "Epoch [76/250] ---- Training Loss: 0.0012 ---- Validation Loss: 0.0011    SmoothL1Loss  1e-05\n",
            "Epoch [77/250] ---- Training Loss: 0.0011 ---- Validation Loss: 0.0011    SmoothL1Loss  1e-05\n",
            "Epoch [78/250] ---- Training Loss: 0.0011 ---- Validation Loss: 0.0011    SmoothL1Loss  1e-05\n",
            "Epoch [79/250] ---- Training Loss: 0.0011 ---- Validation Loss: 0.0011    SmoothL1Loss  1e-05\n",
            "Epoch [80/250] ---- Training Loss: 0.0011 ---- Validation Loss: 0.0010    SmoothL1Loss  1e-05\n",
            "Epoch [81/250] ---- Training Loss: 0.0011 ---- Validation Loss: 0.0010    SmoothL1Loss  1e-05\n",
            "Epoch [82/250] ---- Training Loss: 0.0011 ---- Validation Loss: 0.0010    SmoothL1Loss  1e-05\n",
            "Epoch [83/250] ---- Training Loss: 0.0011 ---- Validation Loss: 0.0010    SmoothL1Loss  1e-05\n",
            "Epoch [84/250] ---- Training Loss: 0.0011 ---- Validation Loss: 0.0010    SmoothL1Loss  1e-05\n",
            "Epoch [85/250] ---- Training Loss: 0.0011 ---- Validation Loss: 0.0010    SmoothL1Loss  1e-05\n",
            "Epoch [86/250] ---- Training Loss: 0.0011 ---- Validation Loss: 0.0010    SmoothL1Loss  1e-05\n",
            "Epoch [87/250] ---- Training Loss: 0.0011 ---- Validation Loss: 0.0010    SmoothL1Loss  1e-05\n",
            "Epoch [88/250] ---- Training Loss: 0.0011 ---- Validation Loss: 0.0010    SmoothL1Loss  1e-05\n",
            "Epoch [89/250] ---- Training Loss: 0.0011 ---- Validation Loss: 0.0010    SmoothL1Loss  1e-05\n",
            "Epoch [90/250] ---- Training Loss: 0.0010 ---- Validation Loss: 0.0010    SmoothL1Loss  1e-05\n",
            "Epoch [91/250] ---- Training Loss: 0.0010 ---- Validation Loss: 0.0010    SmoothL1Loss  1e-05\n",
            "Epoch [92/250] ---- Training Loss: 0.0010 ---- Validation Loss: 0.0009    SmoothL1Loss  1e-05\n",
            "Epoch [93/250] ---- Training Loss: 0.0010 ---- Validation Loss: 0.0009    SmoothL1Loss  1e-05\n",
            "Epoch [94/250] ---- Training Loss: 0.0010 ---- Validation Loss: 0.0009    SmoothL1Loss  1e-05\n",
            "Epoch [95/250] ---- Training Loss: 0.0010 ---- Validation Loss: 0.0009    SmoothL1Loss  1e-05\n",
            "Epoch [96/250] ---- Training Loss: 0.0010 ---- Validation Loss: 0.0009    SmoothL1Loss  1e-05\n",
            "Epoch [97/250] ---- Training Loss: 0.0010 ---- Validation Loss: 0.0009    SmoothL1Loss  1e-05\n",
            "Epoch [98/250] ---- Training Loss: 0.0010 ---- Validation Loss: 0.0009    SmoothL1Loss  1e-05\n",
            "Epoch [99/250] ---- Training Loss: 0.0010 ---- Validation Loss: 0.0009    SmoothL1Loss  1e-05\n",
            "Epoch [100/250] ---- Training Loss: 0.0010 ---- Validation Loss: 0.0009    SmoothL1Loss  1e-05\n",
            "Epoch [101/250] ---- Training Loss: 0.0010 ---- Validation Loss: 0.0009    SmoothL1Loss  1e-05\n",
            "Epoch [102/250] ---- Training Loss: 0.0010 ---- Validation Loss: 0.0009    SmoothL1Loss  1e-05\n",
            "Epoch [103/250] ---- Training Loss: 0.0010 ---- Validation Loss: 0.0009    SmoothL1Loss  1e-05\n",
            "Epoch [104/250] ---- Training Loss: 0.0009 ---- Validation Loss: 0.0009    SmoothL1Loss  1e-05\n",
            "Epoch [105/250] ---- Training Loss: 0.0009 ---- Validation Loss: 0.0008    SmoothL1Loss  1e-05\n",
            "Epoch [106/250] ---- Training Loss: 0.0009 ---- Validation Loss: 0.0008    SmoothL1Loss  1e-05\n",
            "Epoch [107/250] ---- Training Loss: 0.0009 ---- Validation Loss: 0.0008    SmoothL1Loss  1e-05\n",
            "Epoch [108/250] ---- Training Loss: 0.0009 ---- Validation Loss: 0.0008    SmoothL1Loss  1e-05\n",
            "Epoch [109/250] ---- Training Loss: 0.0009 ---- Validation Loss: 0.0008    SmoothL1Loss  1e-05\n",
            "Epoch [110/250] ---- Training Loss: 0.0009 ---- Validation Loss: 0.0008    SmoothL1Loss  1e-05\n",
            "Epoch [111/250] ---- Training Loss: 0.0009 ---- Validation Loss: 0.0008    SmoothL1Loss  1e-05\n",
            "Epoch [112/250] ---- Training Loss: 0.0009 ---- Validation Loss: 0.0008    SmoothL1Loss  1e-05\n",
            "Epoch [113/250] ---- Training Loss: 0.0009 ---- Validation Loss: 0.0008    SmoothL1Loss  1e-05\n",
            "Epoch [114/250] ---- Training Loss: 0.0009 ---- Validation Loss: 0.0008    SmoothL1Loss  1e-05\n",
            "Epoch [115/250] ---- Training Loss: 0.0009 ---- Validation Loss: 0.0008    SmoothL1Loss  1e-05\n",
            "Epoch [116/250] ---- Training Loss: 0.0009 ---- Validation Loss: 0.0008    SmoothL1Loss  1e-05\n",
            "Epoch [117/250] ---- Training Loss: 0.0009 ---- Validation Loss: 0.0008    SmoothL1Loss  1e-05\n",
            "Epoch [118/250] ---- Training Loss: 0.0009 ---- Validation Loss: 0.0008    SmoothL1Loss  1e-05\n",
            "Epoch [119/250] ---- Training Loss: 0.0009 ---- Validation Loss: 0.0008    SmoothL1Loss  1e-05\n",
            "Epoch [120/250] ---- Training Loss: 0.0009 ---- Validation Loss: 0.0008    SmoothL1Loss  1e-05\n",
            "Epoch [121/250] ---- Training Loss: 0.0009 ---- Validation Loss: 0.0008    SmoothL1Loss  1e-05\n",
            "Epoch [122/250] ---- Training Loss: 0.0009 ---- Validation Loss: 0.0008    SmoothL1Loss  1e-05\n",
            "Epoch [123/250] ---- Training Loss: 0.0008 ---- Validation Loss: 0.0008    SmoothL1Loss  1e-05\n",
            "Epoch [124/250] ---- Training Loss: 0.0008 ---- Validation Loss: 0.0007    SmoothL1Loss  1e-05\n",
            "Epoch [125/250] ---- Training Loss: 0.0008 ---- Validation Loss: 0.0007    SmoothL1Loss  1e-05\n",
            "Epoch [126/250] ---- Training Loss: 0.0008 ---- Validation Loss: 0.0007    SmoothL1Loss  1e-05\n",
            "Epoch [127/250] ---- Training Loss: 0.0008 ---- Validation Loss: 0.0007    SmoothL1Loss  1e-05\n",
            "Epoch [128/250] ---- Training Loss: 0.0008 ---- Validation Loss: 0.0007    SmoothL1Loss  1e-05\n",
            "Epoch [129/250] ---- Training Loss: 0.0008 ---- Validation Loss: 0.0007    SmoothL1Loss  1e-05\n",
            "Epoch [130/250] ---- Training Loss: 0.0008 ---- Validation Loss: 0.0007    SmoothL1Loss  1e-05\n",
            "Epoch [131/250] ---- Training Loss: 0.0008 ---- Validation Loss: 0.0007    SmoothL1Loss  1e-05\n",
            "Epoch [132/250] ---- Training Loss: 0.0008 ---- Validation Loss: 0.0007    SmoothL1Loss  1e-05\n",
            "Epoch [133/250] ---- Training Loss: 0.0008 ---- Validation Loss: 0.0007    SmoothL1Loss  1e-05\n",
            "Epoch [134/250] ---- Training Loss: 0.0008 ---- Validation Loss: 0.0007    SmoothL1Loss  1e-05\n",
            "Epoch [135/250] ---- Training Loss: 0.0008 ---- Validation Loss: 0.0007    SmoothL1Loss  1e-05\n",
            "Epoch [136/250] ---- Training Loss: 0.0008 ---- Validation Loss: 0.0007    SmoothL1Loss  1e-05\n",
            "Epoch [137/250] ---- Training Loss: 0.0008 ---- Validation Loss: 0.0007    SmoothL1Loss  1e-05\n",
            "Epoch [138/250] ---- Training Loss: 0.0008 ---- Validation Loss: 0.0007    SmoothL1Loss  1e-05\n",
            "Epoch [139/250] ---- Training Loss: 0.0008 ---- Validation Loss: 0.0007    SmoothL1Loss  1e-05\n",
            "Epoch [140/250] ---- Training Loss: 0.0008 ---- Validation Loss: 0.0007    SmoothL1Loss  1e-05\n",
            "Epoch [141/250] ---- Training Loss: 0.0008 ---- Validation Loss: 0.0007    SmoothL1Loss  1e-05\n",
            "Epoch [142/250] ---- Training Loss: 0.0008 ---- Validation Loss: 0.0007    SmoothL1Loss  1e-05\n",
            "Epoch [143/250] ---- Training Loss: 0.0008 ---- Validation Loss: 0.0007    SmoothL1Loss  1e-05\n",
            "Epoch [144/250] ---- Training Loss: 0.0008 ---- Validation Loss: 0.0007    SmoothL1Loss  1e-05\n",
            "Epoch [145/250] ---- Training Loss: 0.0008 ---- Validation Loss: 0.0007    SmoothL1Loss  1e-05\n",
            "Epoch [146/250] ---- Training Loss: 0.0008 ---- Validation Loss: 0.0007    SmoothL1Loss  1e-05\n",
            "Epoch [147/250] ---- Training Loss: 0.0008 ---- Validation Loss: 0.0007    SmoothL1Loss  1e-05\n",
            "Epoch [148/250] ---- Training Loss: 0.0008 ---- Validation Loss: 0.0007    SmoothL1Loss  1e-05\n",
            "Epoch [149/250] ---- Training Loss: 0.0007 ---- Validation Loss: 0.0007    SmoothL1Loss  1e-05\n",
            "Epoch [150/250] ---- Training Loss: 0.0007 ---- Validation Loss: 0.0007    SmoothL1Loss  1e-05\n",
            "Epoch [151/250] ---- Training Loss: 0.0007 ---- Validation Loss: 0.0007    SmoothL1Loss  1e-05\n",
            "Epoch [152/250] ---- Training Loss: 0.0007 ---- Validation Loss: 0.0007    SmoothL1Loss  1e-05\n",
            "Epoch [153/250] ---- Training Loss: 0.0007 ---- Validation Loss: 0.0006    SmoothL1Loss  1e-05\n",
            "Epoch [154/250] ---- Training Loss: 0.0007 ---- Validation Loss: 0.0006    SmoothL1Loss  1e-05\n",
            "Epoch [155/250] ---- Training Loss: 0.0007 ---- Validation Loss: 0.0006    SmoothL1Loss  1e-05\n",
            "Epoch [156/250] ---- Training Loss: 0.0007 ---- Validation Loss: 0.0006    SmoothL1Loss  1e-05\n",
            "Epoch [157/250] ---- Training Loss: 0.0007 ---- Validation Loss: 0.0006    SmoothL1Loss  1e-05\n",
            "Epoch [158/250] ---- Training Loss: 0.0007 ---- Validation Loss: 0.0006    SmoothL1Loss  1e-05\n",
            "Epoch [159/250] ---- Training Loss: 0.0007 ---- Validation Loss: 0.0006    SmoothL1Loss  1e-05\n",
            "Epoch [160/250] ---- Training Loss: 0.0007 ---- Validation Loss: 0.0006    SmoothL1Loss  1e-05\n",
            "Epoch [161/250] ---- Training Loss: 0.0007 ---- Validation Loss: 0.0006    SmoothL1Loss  1e-05\n",
            "Epoch [162/250] ---- Training Loss: 0.0007 ---- Validation Loss: 0.0006    SmoothL1Loss  1e-05\n",
            "Epoch [163/250] ---- Training Loss: 0.0007 ---- Validation Loss: 0.0006    SmoothL1Loss  1e-05\n",
            "Epoch [164/250] ---- Training Loss: 0.0007 ---- Validation Loss: 0.0006    SmoothL1Loss  1e-05\n",
            "Epoch [165/250] ---- Training Loss: 0.0007 ---- Validation Loss: 0.0006    SmoothL1Loss  1e-05\n",
            "Epoch [166/250] ---- Training Loss: 0.0007 ---- Validation Loss: 0.0006    SmoothL1Loss  1e-05\n",
            "Epoch [167/250] ---- Training Loss: 0.0007 ---- Validation Loss: 0.0006    SmoothL1Loss  1e-05\n",
            "Epoch [168/250] ---- Training Loss: 0.0007 ---- Validation Loss: 0.0006    SmoothL1Loss  1e-05\n",
            "Epoch [169/250] ---- Training Loss: 0.0007 ---- Validation Loss: 0.0006    SmoothL1Loss  1e-05\n",
            "Epoch [170/250] ---- Training Loss: 0.0007 ---- Validation Loss: 0.0006    SmoothL1Loss  1e-05\n",
            "Epoch [171/250] ---- Training Loss: 0.0007 ---- Validation Loss: 0.0006    SmoothL1Loss  1e-05\n",
            "Epoch [172/250] ---- Training Loss: 0.0007 ---- Validation Loss: 0.0006    SmoothL1Loss  1e-05\n",
            "Epoch [173/250] ---- Training Loss: 0.0007 ---- Validation Loss: 0.0006    SmoothL1Loss  1e-05\n",
            "Epoch [174/250] ---- Training Loss: 0.0007 ---- Validation Loss: 0.0006    SmoothL1Loss  1e-05\n",
            "Epoch [175/250] ---- Training Loss: 0.0007 ---- Validation Loss: 0.0006    SmoothL1Loss  1e-05\n",
            "Epoch [176/250] ---- Training Loss: 0.0007 ---- Validation Loss: 0.0006    SmoothL1Loss  1e-05\n",
            "Epoch [177/250] ---- Training Loss: 0.0007 ---- Validation Loss: 0.0006    SmoothL1Loss  1e-05\n",
            "Epoch [178/250] ---- Training Loss: 0.0007 ---- Validation Loss: 0.0006    SmoothL1Loss  1e-05\n",
            "Epoch [179/250] ---- Training Loss: 0.0007 ---- Validation Loss: 0.0006    SmoothL1Loss  1e-05\n",
            "Epoch [180/250] ---- Training Loss: 0.0007 ---- Validation Loss: 0.0006    SmoothL1Loss  1e-05\n",
            "Epoch [181/250] ---- Training Loss: 0.0007 ---- Validation Loss: 0.0006    SmoothL1Loss  1e-05\n",
            "Epoch [182/250] ---- Training Loss: 0.0007 ---- Validation Loss: 0.0006    SmoothL1Loss  1e-05\n",
            "Epoch [183/250] ---- Training Loss: 0.0007 ---- Validation Loss: 0.0006    SmoothL1Loss  1e-05\n",
            "Epoch [184/250] ---- Training Loss: 0.0007 ---- Validation Loss: 0.0006    SmoothL1Loss  1e-05\n",
            "Epoch [185/250] ---- Training Loss: 0.0007 ---- Validation Loss: 0.0006    SmoothL1Loss  1e-05\n",
            "Epoch [186/250] ---- Training Loss: 0.0006 ---- Validation Loss: 0.0006    SmoothL1Loss  1e-05\n",
            "Epoch [187/250] ---- Training Loss: 0.0006 ---- Validation Loss: 0.0006    SmoothL1Loss  1e-05\n",
            "Epoch [188/250] ---- Training Loss: 0.0006 ---- Validation Loss: 0.0006    SmoothL1Loss  1e-05\n",
            "Epoch [189/250] ---- Training Loss: 0.0006 ---- Validation Loss: 0.0006    SmoothL1Loss  1e-05\n",
            "Epoch [190/250] ---- Training Loss: 0.0006 ---- Validation Loss: 0.0006    SmoothL1Loss  1e-05\n",
            "Epoch [191/250] ---- Training Loss: 0.0006 ---- Validation Loss: 0.0006    SmoothL1Loss  1e-05\n",
            "Epoch [192/250] ---- Training Loss: 0.0006 ---- Validation Loss: 0.0006    SmoothL1Loss  1e-05\n",
            "Epoch [193/250] ---- Training Loss: 0.0006 ---- Validation Loss: 0.0006    SmoothL1Loss  1e-05\n",
            "Epoch [194/250] ---- Training Loss: 0.0006 ---- Validation Loss: 0.0006    SmoothL1Loss  1e-05\n",
            "Epoch [195/250] ---- Training Loss: 0.0006 ---- Validation Loss: 0.0006    SmoothL1Loss  1e-05\n",
            "Epoch [196/250] ---- Training Loss: 0.0006 ---- Validation Loss: 0.0006    SmoothL1Loss  1e-05\n",
            "Epoch [197/250] ---- Training Loss: 0.0006 ---- Validation Loss: 0.0006    SmoothL1Loss  1e-05\n",
            "Epoch [198/250] ---- Training Loss: 0.0006 ---- Validation Loss: 0.0006    SmoothL1Loss  1e-05\n",
            "Epoch [199/250] ---- Training Loss: 0.0006 ---- Validation Loss: 0.0006    SmoothL1Loss  1e-05\n",
            "Epoch [200/250] ---- Training Loss: 0.0006 ---- Validation Loss: 0.0006    SmoothL1Loss  1e-05\n",
            "Epoch [201/250] ---- Training Loss: 0.0006 ---- Validation Loss: 0.0005    SmoothL1Loss  1e-05\n",
            "Epoch [202/250] ---- Training Loss: 0.0006 ---- Validation Loss: 0.0006    SmoothL1Loss  1e-05\n",
            "Epoch [203/250] ---- Training Loss: 0.0006 ---- Validation Loss: 0.0005    SmoothL1Loss  1e-05\n",
            "Epoch [204/250] ---- Training Loss: 0.0006 ---- Validation Loss: 0.0005    SmoothL1Loss  1e-05\n",
            "Epoch [205/250] ---- Training Loss: 0.0006 ---- Validation Loss: 0.0005    SmoothL1Loss  1e-05\n",
            "Epoch [206/250] ---- Training Loss: 0.0006 ---- Validation Loss: 0.0005    SmoothL1Loss  1e-05\n",
            "Epoch [207/250] ---- Training Loss: 0.0006 ---- Validation Loss: 0.0005    SmoothL1Loss  1e-05\n",
            "Epoch [208/250] ---- Training Loss: 0.0006 ---- Validation Loss: 0.0005    SmoothL1Loss  1e-05\n",
            "Epoch [209/250] ---- Training Loss: 0.0006 ---- Validation Loss: 0.0005    SmoothL1Loss  1e-05\n",
            "Epoch [210/250] ---- Training Loss: 0.0006 ---- Validation Loss: 0.0005    SmoothL1Loss  1e-05\n",
            "Epoch [211/250] ---- Training Loss: 0.0006 ---- Validation Loss: 0.0005    SmoothL1Loss  1e-05\n",
            "Epoch [212/250] ---- Training Loss: 0.0006 ---- Validation Loss: 0.0005    SmoothL1Loss  1e-05\n",
            "Epoch [213/250] ---- Training Loss: 0.0006 ---- Validation Loss: 0.0005    SmoothL1Loss  1e-05\n",
            "Epoch [214/250] ---- Training Loss: 0.0006 ---- Validation Loss: 0.0005    SmoothL1Loss  1e-05\n",
            "Epoch [215/250] ---- Training Loss: 0.0006 ---- Validation Loss: 0.0005    SmoothL1Loss  1e-05\n",
            "Epoch [216/250] ---- Training Loss: 0.0006 ---- Validation Loss: 0.0005    SmoothL1Loss  1e-05\n",
            "Epoch [217/250] ---- Training Loss: 0.0006 ---- Validation Loss: 0.0005    SmoothL1Loss  1e-05\n",
            "Epoch [218/250] ---- Training Loss: 0.0006 ---- Validation Loss: 0.0005    SmoothL1Loss  1e-05\n",
            "Epoch [219/250] ---- Training Loss: 0.0006 ---- Validation Loss: 0.0005    SmoothL1Loss  1e-05\n",
            "Epoch [220/250] ---- Training Loss: 0.0006 ---- Validation Loss: 0.0005    SmoothL1Loss  1e-05\n",
            "Epoch [221/250] ---- Training Loss: 0.0006 ---- Validation Loss: 0.0005    SmoothL1Loss  1e-05\n",
            "Epoch [222/250] ---- Training Loss: 0.0006 ---- Validation Loss: 0.0005    SmoothL1Loss  1e-05\n",
            "Epoch [223/250] ---- Training Loss: 0.0006 ---- Validation Loss: 0.0005    SmoothL1Loss  1e-05\n",
            "Epoch [224/250] ---- Training Loss: 0.0006 ---- Validation Loss: 0.0005    SmoothL1Loss  1e-05\n",
            "Epoch [225/250] ---- Training Loss: 0.0006 ---- Validation Loss: 0.0005    SmoothL1Loss  1e-05\n",
            "Epoch [226/250] ---- Training Loss: 0.0006 ---- Validation Loss: 0.0005    SmoothL1Loss  1e-05\n",
            "Epoch [227/250] ---- Training Loss: 0.0006 ---- Validation Loss: 0.0005    SmoothL1Loss  1e-05\n",
            "Epoch [228/250] ---- Training Loss: 0.0006 ---- Validation Loss: 0.0005    SmoothL1Loss  1e-05\n",
            "Epoch [229/250] ---- Training Loss: 0.0006 ---- Validation Loss: 0.0005    SmoothL1Loss  1e-05\n",
            "Epoch [230/250] ---- Training Loss: 0.0006 ---- Validation Loss: 0.0005    SmoothL1Loss  1e-05\n",
            "Epoch [231/250] ---- Training Loss: 0.0006 ---- Validation Loss: 0.0005    SmoothL1Loss  1e-05\n",
            "Epoch [232/250] ---- Training Loss: 0.0006 ---- Validation Loss: 0.0005    SmoothL1Loss  1e-05\n",
            "Epoch [233/250] ---- Training Loss: 0.0006 ---- Validation Loss: 0.0005    SmoothL1Loss  1e-05\n",
            "Epoch [234/250] ---- Training Loss: 0.0006 ---- Validation Loss: 0.0005    SmoothL1Loss  1e-05\n",
            "Epoch [235/250] ---- Training Loss: 0.0006 ---- Validation Loss: 0.0005    SmoothL1Loss  1e-05\n",
            "Epoch [236/250] ---- Training Loss: 0.0006 ---- Validation Loss: 0.0005    SmoothL1Loss  1e-05\n",
            "Epoch [237/250] ---- Training Loss: 0.0006 ---- Validation Loss: 0.0005    SmoothL1Loss  1e-05\n",
            "Epoch [238/250] ---- Training Loss: 0.0006 ---- Validation Loss: 0.0005    SmoothL1Loss  1e-05\n",
            "Epoch [239/250] ---- Training Loss: 0.0006 ---- Validation Loss: 0.0005    SmoothL1Loss  1e-05\n",
            "Epoch [240/250] ---- Training Loss: 0.0006 ---- Validation Loss: 0.0005    SmoothL1Loss  1e-05\n",
            "Epoch [241/250] ---- Training Loss: 0.0006 ---- Validation Loss: 0.0005    SmoothL1Loss  1e-05\n",
            "Epoch [242/250] ---- Training Loss: 0.0006 ---- Validation Loss: 0.0005    SmoothL1Loss  1e-05\n",
            "Epoch [243/250] ---- Training Loss: 0.0006 ---- Validation Loss: 0.0005    SmoothL1Loss  1e-05\n",
            "Epoch [244/250] ---- Training Loss: 0.0006 ---- Validation Loss: 0.0005    SmoothL1Loss  1e-05\n",
            "Epoch [245/250] ---- Training Loss: 0.0006 ---- Validation Loss: 0.0005    SmoothL1Loss  1e-05\n",
            "Epoch [246/250] ---- Training Loss: 0.0006 ---- Validation Loss: 0.0005    SmoothL1Loss  1e-05\n",
            "Epoch [247/250] ---- Training Loss: 0.0006 ---- Validation Loss: 0.0005    SmoothL1Loss  1e-05\n",
            "Epoch [248/250] ---- Training Loss: 0.0006 ---- Validation Loss: 0.0005    SmoothL1Loss  1e-05\n",
            "Epoch [249/250] ---- Training Loss: 0.0006 ---- Validation Loss: 0.0005    SmoothL1Loss  1e-05\n",
            "Epoch [250/250] ---- Training Loss: 0.0006 ---- Validation Loss: 0.0005    SmoothL1Loss  1e-05\n",
            "Saved model: model_loss_SmoothL1Loss_wd_1e-05.pth\n",
            "Epoch [1/250] ---- Training Loss: 0.0053 ---- Validation Loss: 0.0026    SmoothL1Loss  5e-05\n",
            "Epoch [2/250] ---- Training Loss: 0.0027 ---- Validation Loss: 0.0021    SmoothL1Loss  5e-05\n",
            "Epoch [3/250] ---- Training Loss: 0.0025 ---- Validation Loss: 0.0021    SmoothL1Loss  5e-05\n",
            "Epoch [4/250] ---- Training Loss: 0.0024 ---- Validation Loss: 0.0020    SmoothL1Loss  5e-05\n",
            "Epoch [5/250] ---- Training Loss: 0.0024 ---- Validation Loss: 0.0020    SmoothL1Loss  5e-05\n",
            "Epoch [6/250] ---- Training Loss: 0.0023 ---- Validation Loss: 0.0019    SmoothL1Loss  5e-05\n",
            "Epoch [7/250] ---- Training Loss: 0.0022 ---- Validation Loss: 0.0019    SmoothL1Loss  5e-05\n",
            "Epoch [8/250] ---- Training Loss: 0.0022 ---- Validation Loss: 0.0019    SmoothL1Loss  5e-05\n",
            "Epoch [9/250] ---- Training Loss: 0.0021 ---- Validation Loss: 0.0019    SmoothL1Loss  5e-05\n",
            "Epoch [10/250] ---- Training Loss: 0.0021 ---- Validation Loss: 0.0018    SmoothL1Loss  5e-05\n",
            "Epoch [11/250] ---- Training Loss: 0.0020 ---- Validation Loss: 0.0018    SmoothL1Loss  5e-05\n",
            "Epoch [12/250] ---- Training Loss: 0.0020 ---- Validation Loss: 0.0018    SmoothL1Loss  5e-05\n",
            "Epoch [13/250] ---- Training Loss: 0.0020 ---- Validation Loss: 0.0018    SmoothL1Loss  5e-05\n",
            "Epoch [14/250] ---- Training Loss: 0.0019 ---- Validation Loss: 0.0017    SmoothL1Loss  5e-05\n",
            "Epoch [15/250] ---- Training Loss: 0.0019 ---- Validation Loss: 0.0017    SmoothL1Loss  5e-05\n",
            "Epoch [16/250] ---- Training Loss: 0.0019 ---- Validation Loss: 0.0017    SmoothL1Loss  5e-05\n",
            "Epoch [17/250] ---- Training Loss: 0.0018 ---- Validation Loss: 0.0017    SmoothL1Loss  5e-05\n",
            "Epoch [18/250] ---- Training Loss: 0.0018 ---- Validation Loss: 0.0017    SmoothL1Loss  5e-05\n",
            "Epoch [19/250] ---- Training Loss: 0.0018 ---- Validation Loss: 0.0017    SmoothL1Loss  5e-05\n",
            "Epoch [20/250] ---- Training Loss: 0.0018 ---- Validation Loss: 0.0016    SmoothL1Loss  5e-05\n",
            "Epoch [21/250] ---- Training Loss: 0.0017 ---- Validation Loss: 0.0016    SmoothL1Loss  5e-05\n",
            "Epoch [22/250] ---- Training Loss: 0.0017 ---- Validation Loss: 0.0016    SmoothL1Loss  5e-05\n",
            "Epoch [23/250] ---- Training Loss: 0.0017 ---- Validation Loss: 0.0016    SmoothL1Loss  5e-05\n",
            "Epoch [24/250] ---- Training Loss: 0.0017 ---- Validation Loss: 0.0016    SmoothL1Loss  5e-05\n",
            "Epoch [25/250] ---- Training Loss: 0.0017 ---- Validation Loss: 0.0016    SmoothL1Loss  5e-05\n",
            "Epoch [26/250] ---- Training Loss: 0.0017 ---- Validation Loss: 0.0016    SmoothL1Loss  5e-05\n",
            "Epoch [27/250] ---- Training Loss: 0.0016 ---- Validation Loss: 0.0016    SmoothL1Loss  5e-05\n",
            "Epoch [28/250] ---- Training Loss: 0.0016 ---- Validation Loss: 0.0016    SmoothL1Loss  5e-05\n",
            "Epoch [29/250] ---- Training Loss: 0.0016 ---- Validation Loss: 0.0016    SmoothL1Loss  5e-05\n",
            "Epoch [30/250] ---- Training Loss: 0.0016 ---- Validation Loss: 0.0015    SmoothL1Loss  5e-05\n",
            "Epoch [31/250] ---- Training Loss: 0.0016 ---- Validation Loss: 0.0015    SmoothL1Loss  5e-05\n",
            "Epoch [32/250] ---- Training Loss: 0.0016 ---- Validation Loss: 0.0015    SmoothL1Loss  5e-05\n",
            "Epoch [33/250] ---- Training Loss: 0.0016 ---- Validation Loss: 0.0015    SmoothL1Loss  5e-05\n",
            "Epoch [34/250] ---- Training Loss: 0.0016 ---- Validation Loss: 0.0015    SmoothL1Loss  5e-05\n",
            "Epoch [35/250] ---- Training Loss: 0.0016 ---- Validation Loss: 0.0015    SmoothL1Loss  5e-05\n",
            "Epoch [36/250] ---- Training Loss: 0.0016 ---- Validation Loss: 0.0015    SmoothL1Loss  5e-05\n",
            "Epoch [37/250] ---- Training Loss: 0.0016 ---- Validation Loss: 0.0015    SmoothL1Loss  5e-05\n",
            "Epoch [38/250] ---- Training Loss: 0.0016 ---- Validation Loss: 0.0015    SmoothL1Loss  5e-05\n",
            "Epoch [39/250] ---- Training Loss: 0.0016 ---- Validation Loss: 0.0015    SmoothL1Loss  5e-05\n",
            "Epoch [40/250] ---- Training Loss: 0.0016 ---- Validation Loss: 0.0015    SmoothL1Loss  5e-05\n",
            "Epoch [41/250] ---- Training Loss: 0.0016 ---- Validation Loss: 0.0015    SmoothL1Loss  5e-05\n",
            "Epoch [42/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  5e-05\n",
            "Epoch [43/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  5e-05\n",
            "Epoch [44/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  5e-05\n",
            "Epoch [45/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  5e-05\n",
            "Epoch [46/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  5e-05\n",
            "Epoch [47/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  5e-05\n",
            "Epoch [48/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  5e-05\n",
            "Epoch [49/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  5e-05\n",
            "Epoch [50/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  5e-05\n",
            "Epoch [51/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  5e-05\n",
            "Epoch [52/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  5e-05\n",
            "Epoch [53/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  5e-05\n",
            "Epoch [54/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  5e-05\n",
            "Epoch [55/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  5e-05\n",
            "Epoch [56/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  5e-05\n",
            "Epoch [57/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  5e-05\n",
            "Epoch [58/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  5e-05\n",
            "Epoch [59/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  5e-05\n",
            "Epoch [60/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  5e-05\n",
            "Epoch [61/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  5e-05\n",
            "Epoch [62/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  5e-05\n",
            "Epoch [63/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  5e-05\n",
            "Epoch [64/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  5e-05\n",
            "Epoch [65/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  5e-05\n",
            "Epoch [66/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  5e-05\n",
            "Epoch [67/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  5e-05\n",
            "Epoch [68/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  5e-05\n",
            "Epoch [69/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  5e-05\n",
            "Epoch [70/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  5e-05\n",
            "Epoch [71/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  5e-05\n",
            "Epoch [72/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  5e-05\n",
            "Epoch [73/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  5e-05\n",
            "Epoch [74/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  5e-05\n",
            "Epoch [75/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  5e-05\n",
            "Epoch [76/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0014    SmoothL1Loss  5e-05\n",
            "Epoch [77/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0014    SmoothL1Loss  5e-05\n",
            "Epoch [78/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0014    SmoothL1Loss  5e-05\n",
            "Epoch [79/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0014    SmoothL1Loss  5e-05\n",
            "Epoch [80/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0014    SmoothL1Loss  5e-05\n",
            "Epoch [81/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0014    SmoothL1Loss  5e-05\n",
            "Epoch [82/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0014    SmoothL1Loss  5e-05\n",
            "Epoch [83/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0014    SmoothL1Loss  5e-05\n",
            "Epoch [84/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0014    SmoothL1Loss  5e-05\n",
            "Epoch [85/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0014    SmoothL1Loss  5e-05\n",
            "Epoch [86/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0014    SmoothL1Loss  5e-05\n",
            "Epoch [87/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0014    SmoothL1Loss  5e-05\n",
            "Epoch [88/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0014    SmoothL1Loss  5e-05\n",
            "Epoch [89/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0014    SmoothL1Loss  5e-05\n",
            "Epoch [90/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0014    SmoothL1Loss  5e-05\n",
            "Epoch [91/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0014    SmoothL1Loss  5e-05\n",
            "Epoch [92/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0014    SmoothL1Loss  5e-05\n",
            "Epoch [93/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  5e-05\n",
            "Epoch [94/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  5e-05\n",
            "Epoch [95/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  5e-05\n",
            "Epoch [96/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  5e-05\n",
            "Epoch [97/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  5e-05\n",
            "Epoch [98/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  5e-05\n",
            "Epoch [99/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  5e-05\n",
            "Epoch [100/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  5e-05\n",
            "Epoch [101/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  5e-05\n",
            "Epoch [102/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  5e-05\n",
            "Epoch [103/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  5e-05\n",
            "Epoch [104/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  5e-05\n",
            "Epoch [105/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  5e-05\n",
            "Epoch [106/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  5e-05\n",
            "Epoch [107/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  5e-05\n",
            "Epoch [108/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  5e-05\n",
            "Epoch [109/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  5e-05\n",
            "Epoch [110/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  5e-05\n",
            "Epoch [111/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  5e-05\n",
            "Epoch [112/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  5e-05\n",
            "Epoch [113/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  5e-05\n",
            "Epoch [114/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  5e-05\n",
            "Epoch [115/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  5e-05\n",
            "Epoch [116/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  5e-05\n",
            "Epoch [117/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  5e-05\n",
            "Epoch [118/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  5e-05\n",
            "Epoch [119/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  5e-05\n",
            "Epoch [120/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  5e-05\n",
            "Epoch [121/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  5e-05\n",
            "Epoch [122/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  5e-05\n",
            "Epoch [123/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  5e-05\n",
            "Epoch [124/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  5e-05\n",
            "Epoch [125/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  5e-05\n",
            "Epoch [126/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  5e-05\n",
            "Epoch [127/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  5e-05\n",
            "Epoch [128/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  5e-05\n",
            "Epoch [129/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  5e-05\n",
            "Epoch [130/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  5e-05\n",
            "Epoch [131/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  5e-05\n",
            "Epoch [132/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  5e-05\n",
            "Epoch [133/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  5e-05\n",
            "Epoch [134/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  5e-05\n",
            "Epoch [135/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  5e-05\n",
            "Epoch [136/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  5e-05\n",
            "Epoch [137/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  5e-05\n",
            "Epoch [138/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  5e-05\n",
            "Epoch [139/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  5e-05\n",
            "Epoch [140/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  5e-05\n",
            "Epoch [141/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  5e-05\n",
            "Epoch [142/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0013    SmoothL1Loss  5e-05\n",
            "Epoch [143/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0013    SmoothL1Loss  5e-05\n",
            "Epoch [144/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0013    SmoothL1Loss  5e-05\n",
            "Epoch [145/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0013    SmoothL1Loss  5e-05\n",
            "Epoch [146/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0013    SmoothL1Loss  5e-05\n",
            "Epoch [147/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0013    SmoothL1Loss  5e-05\n",
            "Epoch [148/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0013    SmoothL1Loss  5e-05\n",
            "Epoch [149/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0013    SmoothL1Loss  5e-05\n",
            "Epoch [150/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0013    SmoothL1Loss  5e-05\n",
            "Epoch [151/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0013    SmoothL1Loss  5e-05\n",
            "Epoch [152/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0013    SmoothL1Loss  5e-05\n",
            "Epoch [153/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0013    SmoothL1Loss  5e-05\n",
            "Epoch [154/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0013    SmoothL1Loss  5e-05\n",
            "Epoch [155/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0013    SmoothL1Loss  5e-05\n",
            "Epoch [156/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0013    SmoothL1Loss  5e-05\n",
            "Epoch [157/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0013    SmoothL1Loss  5e-05\n",
            "Epoch [158/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0013    SmoothL1Loss  5e-05\n",
            "Epoch [159/250] ---- Training Loss: 0.0013 ---- Validation Loss: 0.0013    SmoothL1Loss  5e-05\n",
            "Epoch [160/250] ---- Training Loss: 0.0013 ---- Validation Loss: 0.0013    SmoothL1Loss  5e-05\n",
            "Epoch [161/250] ---- Training Loss: 0.0013 ---- Validation Loss: 0.0013    SmoothL1Loss  5e-05\n",
            "Epoch [162/250] ---- Training Loss: 0.0013 ---- Validation Loss: 0.0013    SmoothL1Loss  5e-05\n",
            "Epoch [163/250] ---- Training Loss: 0.0013 ---- Validation Loss: 0.0013    SmoothL1Loss  5e-05\n",
            "Epoch [164/250] ---- Training Loss: 0.0013 ---- Validation Loss: 0.0013    SmoothL1Loss  5e-05\n",
            "Epoch [165/250] ---- Training Loss: 0.0013 ---- Validation Loss: 0.0013    SmoothL1Loss  5e-05\n",
            "Epoch [166/250] ---- Training Loss: 0.0013 ---- Validation Loss: 0.0013    SmoothL1Loss  5e-05\n",
            "Epoch [167/250] ---- Training Loss: 0.0013 ---- Validation Loss: 0.0013    SmoothL1Loss  5e-05\n",
            "Epoch [168/250] ---- Training Loss: 0.0013 ---- Validation Loss: 0.0013    SmoothL1Loss  5e-05\n",
            "Epoch [169/250] ---- Training Loss: 0.0013 ---- Validation Loss: 0.0013    SmoothL1Loss  5e-05\n",
            "Epoch [170/250] ---- Training Loss: 0.0013 ---- Validation Loss: 0.0013    SmoothL1Loss  5e-05\n",
            "Epoch [171/250] ---- Training Loss: 0.0013 ---- Validation Loss: 0.0013    SmoothL1Loss  5e-05\n",
            "Epoch [172/250] ---- Training Loss: 0.0013 ---- Validation Loss: 0.0013    SmoothL1Loss  5e-05\n",
            "Epoch [173/250] ---- Training Loss: 0.0013 ---- Validation Loss: 0.0013    SmoothL1Loss  5e-05\n",
            "Epoch [174/250] ---- Training Loss: 0.0013 ---- Validation Loss: 0.0013    SmoothL1Loss  5e-05\n",
            "Epoch [175/250] ---- Training Loss: 0.0013 ---- Validation Loss: 0.0013    SmoothL1Loss  5e-05\n",
            "Epoch [176/250] ---- Training Loss: 0.0013 ---- Validation Loss: 0.0013    SmoothL1Loss  5e-05\n",
            "Epoch [177/250] ---- Training Loss: 0.0013 ---- Validation Loss: 0.0013    SmoothL1Loss  5e-05\n",
            "Epoch [178/250] ---- Training Loss: 0.0013 ---- Validation Loss: 0.0013    SmoothL1Loss  5e-05\n",
            "Epoch [179/250] ---- Training Loss: 0.0013 ---- Validation Loss: 0.0013    SmoothL1Loss  5e-05\n",
            "Epoch [180/250] ---- Training Loss: 0.0013 ---- Validation Loss: 0.0013    SmoothL1Loss  5e-05\n",
            "Epoch [181/250] ---- Training Loss: 0.0013 ---- Validation Loss: 0.0013    SmoothL1Loss  5e-05\n",
            "Epoch [182/250] ---- Training Loss: 0.0013 ---- Validation Loss: 0.0013    SmoothL1Loss  5e-05\n",
            "Epoch [183/250] ---- Training Loss: 0.0013 ---- Validation Loss: 0.0013    SmoothL1Loss  5e-05\n",
            "Epoch [184/250] ---- Training Loss: 0.0013 ---- Validation Loss: 0.0013    SmoothL1Loss  5e-05\n",
            "Epoch [185/250] ---- Training Loss: 0.0013 ---- Validation Loss: 0.0013    SmoothL1Loss  5e-05\n",
            "Epoch [186/250] ---- Training Loss: 0.0013 ---- Validation Loss: 0.0013    SmoothL1Loss  5e-05\n",
            "Epoch [187/250] ---- Training Loss: 0.0013 ---- Validation Loss: 0.0013    SmoothL1Loss  5e-05\n",
            "Epoch [188/250] ---- Training Loss: 0.0013 ---- Validation Loss: 0.0013    SmoothL1Loss  5e-05\n",
            "Epoch [189/250] ---- Training Loss: 0.0013 ---- Validation Loss: 0.0013    SmoothL1Loss  5e-05\n",
            "Epoch [190/250] ---- Training Loss: 0.0013 ---- Validation Loss: 0.0013    SmoothL1Loss  5e-05\n",
            "Epoch [191/250] ---- Training Loss: 0.0013 ---- Validation Loss: 0.0013    SmoothL1Loss  5e-05\n",
            "Epoch [192/250] ---- Training Loss: 0.0013 ---- Validation Loss: 0.0013    SmoothL1Loss  5e-05\n",
            "Epoch [193/250] ---- Training Loss: 0.0013 ---- Validation Loss: 0.0013    SmoothL1Loss  5e-05\n",
            "Epoch [194/250] ---- Training Loss: 0.0013 ---- Validation Loss: 0.0013    SmoothL1Loss  5e-05\n",
            "Epoch [195/250] ---- Training Loss: 0.0013 ---- Validation Loss: 0.0013    SmoothL1Loss  5e-05\n",
            "Epoch [196/250] ---- Training Loss: 0.0013 ---- Validation Loss: 0.0013    SmoothL1Loss  5e-05\n",
            "Epoch [197/250] ---- Training Loss: 0.0013 ---- Validation Loss: 0.0013    SmoothL1Loss  5e-05\n",
            "Epoch [198/250] ---- Training Loss: 0.0013 ---- Validation Loss: 0.0013    SmoothL1Loss  5e-05\n",
            "Epoch [199/250] ---- Training Loss: 0.0013 ---- Validation Loss: 0.0013    SmoothL1Loss  5e-05\n",
            "Epoch [200/250] ---- Training Loss: 0.0013 ---- Validation Loss: 0.0013    SmoothL1Loss  5e-05\n",
            "Epoch [201/250] ---- Training Loss: 0.0013 ---- Validation Loss: 0.0013    SmoothL1Loss  5e-05\n",
            "Epoch [202/250] ---- Training Loss: 0.0013 ---- Validation Loss: 0.0012    SmoothL1Loss  5e-05\n",
            "Epoch [203/250] ---- Training Loss: 0.0013 ---- Validation Loss: 0.0012    SmoothL1Loss  5e-05\n",
            "Epoch [204/250] ---- Training Loss: 0.0013 ---- Validation Loss: 0.0012    SmoothL1Loss  5e-05\n",
            "Epoch [205/250] ---- Training Loss: 0.0013 ---- Validation Loss: 0.0012    SmoothL1Loss  5e-05\n",
            "Epoch [206/250] ---- Training Loss: 0.0013 ---- Validation Loss: 0.0012    SmoothL1Loss  5e-05\n",
            "Epoch [207/250] ---- Training Loss: 0.0013 ---- Validation Loss: 0.0012    SmoothL1Loss  5e-05\n",
            "Epoch [208/250] ---- Training Loss: 0.0013 ---- Validation Loss: 0.0012    SmoothL1Loss  5e-05\n",
            "Epoch [209/250] ---- Training Loss: 0.0013 ---- Validation Loss: 0.0012    SmoothL1Loss  5e-05\n",
            "Epoch [210/250] ---- Training Loss: 0.0013 ---- Validation Loss: 0.0012    SmoothL1Loss  5e-05\n",
            "Epoch [211/250] ---- Training Loss: 0.0013 ---- Validation Loss: 0.0012    SmoothL1Loss  5e-05\n",
            "Epoch [212/250] ---- Training Loss: 0.0013 ---- Validation Loss: 0.0012    SmoothL1Loss  5e-05\n",
            "Epoch [213/250] ---- Training Loss: 0.0013 ---- Validation Loss: 0.0012    SmoothL1Loss  5e-05\n",
            "Epoch [214/250] ---- Training Loss: 0.0013 ---- Validation Loss: 0.0012    SmoothL1Loss  5e-05\n",
            "Epoch [215/250] ---- Training Loss: 0.0013 ---- Validation Loss: 0.0012    SmoothL1Loss  5e-05\n",
            "Epoch [216/250] ---- Training Loss: 0.0013 ---- Validation Loss: 0.0012    SmoothL1Loss  5e-05\n",
            "Epoch [217/250] ---- Training Loss: 0.0013 ---- Validation Loss: 0.0012    SmoothL1Loss  5e-05\n",
            "Epoch [218/250] ---- Training Loss: 0.0013 ---- Validation Loss: 0.0012    SmoothL1Loss  5e-05\n",
            "Epoch [219/250] ---- Training Loss: 0.0013 ---- Validation Loss: 0.0012    SmoothL1Loss  5e-05\n",
            "Epoch [220/250] ---- Training Loss: 0.0013 ---- Validation Loss: 0.0012    SmoothL1Loss  5e-05\n",
            "Epoch [221/250] ---- Training Loss: 0.0013 ---- Validation Loss: 0.0012    SmoothL1Loss  5e-05\n",
            "Epoch [222/250] ---- Training Loss: 0.0013 ---- Validation Loss: 0.0012    SmoothL1Loss  5e-05\n",
            "Epoch [223/250] ---- Training Loss: 0.0013 ---- Validation Loss: 0.0012    SmoothL1Loss  5e-05\n",
            "Epoch [224/250] ---- Training Loss: 0.0013 ---- Validation Loss: 0.0012    SmoothL1Loss  5e-05\n",
            "Epoch [225/250] ---- Training Loss: 0.0013 ---- Validation Loss: 0.0012    SmoothL1Loss  5e-05\n",
            "Epoch [226/250] ---- Training Loss: 0.0013 ---- Validation Loss: 0.0012    SmoothL1Loss  5e-05\n",
            "Epoch [227/250] ---- Training Loss: 0.0013 ---- Validation Loss: 0.0012    SmoothL1Loss  5e-05\n",
            "Epoch [228/250] ---- Training Loss: 0.0012 ---- Validation Loss: 0.0012    SmoothL1Loss  5e-05\n",
            "Epoch [229/250] ---- Training Loss: 0.0012 ---- Validation Loss: 0.0012    SmoothL1Loss  5e-05\n",
            "Epoch [230/250] ---- Training Loss: 0.0012 ---- Validation Loss: 0.0012    SmoothL1Loss  5e-05\n",
            "Epoch [231/250] ---- Training Loss: 0.0012 ---- Validation Loss: 0.0012    SmoothL1Loss  5e-05\n",
            "Epoch [232/250] ---- Training Loss: 0.0012 ---- Validation Loss: 0.0012    SmoothL1Loss  5e-05\n",
            "Epoch [233/250] ---- Training Loss: 0.0012 ---- Validation Loss: 0.0012    SmoothL1Loss  5e-05\n",
            "Epoch [234/250] ---- Training Loss: 0.0012 ---- Validation Loss: 0.0012    SmoothL1Loss  5e-05\n",
            "Epoch [235/250] ---- Training Loss: 0.0012 ---- Validation Loss: 0.0012    SmoothL1Loss  5e-05\n",
            "Epoch [236/250] ---- Training Loss: 0.0012 ---- Validation Loss: 0.0012    SmoothL1Loss  5e-05\n",
            "Epoch [237/250] ---- Training Loss: 0.0012 ---- Validation Loss: 0.0012    SmoothL1Loss  5e-05\n",
            "Epoch [238/250] ---- Training Loss: 0.0012 ---- Validation Loss: 0.0012    SmoothL1Loss  5e-05\n",
            "Epoch [239/250] ---- Training Loss: 0.0012 ---- Validation Loss: 0.0012    SmoothL1Loss  5e-05\n",
            "Epoch [240/250] ---- Training Loss: 0.0012 ---- Validation Loss: 0.0012    SmoothL1Loss  5e-05\n",
            "Epoch [241/250] ---- Training Loss: 0.0012 ---- Validation Loss: 0.0012    SmoothL1Loss  5e-05\n",
            "Epoch [242/250] ---- Training Loss: 0.0012 ---- Validation Loss: 0.0012    SmoothL1Loss  5e-05\n",
            "Epoch [243/250] ---- Training Loss: 0.0012 ---- Validation Loss: 0.0012    SmoothL1Loss  5e-05\n",
            "Epoch [244/250] ---- Training Loss: 0.0012 ---- Validation Loss: 0.0012    SmoothL1Loss  5e-05\n",
            "Epoch [245/250] ---- Training Loss: 0.0012 ---- Validation Loss: 0.0012    SmoothL1Loss  5e-05\n",
            "Epoch [246/250] ---- Training Loss: 0.0012 ---- Validation Loss: 0.0012    SmoothL1Loss  5e-05\n",
            "Epoch [247/250] ---- Training Loss: 0.0012 ---- Validation Loss: 0.0012    SmoothL1Loss  5e-05\n",
            "Epoch [248/250] ---- Training Loss: 0.0012 ---- Validation Loss: 0.0012    SmoothL1Loss  5e-05\n",
            "Epoch [249/250] ---- Training Loss: 0.0012 ---- Validation Loss: 0.0012    SmoothL1Loss  5e-05\n",
            "Epoch [250/250] ---- Training Loss: 0.0012 ---- Validation Loss: 0.0012    SmoothL1Loss  5e-05\n",
            "Saved model: model_loss_SmoothL1Loss_wd_5e-05.pth\n",
            "Epoch [1/250] ---- Training Loss: 0.0046 ---- Validation Loss: 0.0025    SmoothL1Loss  1e-06\n",
            "Epoch [2/250] ---- Training Loss: 0.0027 ---- Validation Loss: 0.0022    SmoothL1Loss  1e-06\n",
            "Epoch [3/250] ---- Training Loss: 0.0026 ---- Validation Loss: 0.0021    SmoothL1Loss  1e-06\n",
            "Epoch [4/250] ---- Training Loss: 0.0025 ---- Validation Loss: 0.0021    SmoothL1Loss  1e-06\n",
            "Epoch [5/250] ---- Training Loss: 0.0024 ---- Validation Loss: 0.0020    SmoothL1Loss  1e-06\n",
            "Epoch [6/250] ---- Training Loss: 0.0023 ---- Validation Loss: 0.0019    SmoothL1Loss  1e-06\n",
            "Epoch [7/250] ---- Training Loss: 0.0022 ---- Validation Loss: 0.0019    SmoothL1Loss  1e-06\n",
            "Epoch [8/250] ---- Training Loss: 0.0021 ---- Validation Loss: 0.0018    SmoothL1Loss  1e-06\n",
            "Epoch [9/250] ---- Training Loss: 0.0021 ---- Validation Loss: 0.0018    SmoothL1Loss  1e-06\n",
            "Epoch [10/250] ---- Training Loss: 0.0020 ---- Validation Loss: 0.0018    SmoothL1Loss  1e-06\n",
            "Epoch [11/250] ---- Training Loss: 0.0020 ---- Validation Loss: 0.0017    SmoothL1Loss  1e-06\n",
            "Epoch [12/250] ---- Training Loss: 0.0019 ---- Validation Loss: 0.0017    SmoothL1Loss  1e-06\n",
            "Epoch [13/250] ---- Training Loss: 0.0018 ---- Validation Loss: 0.0017    SmoothL1Loss  1e-06\n",
            "Epoch [14/250] ---- Training Loss: 0.0018 ---- Validation Loss: 0.0016    SmoothL1Loss  1e-06\n",
            "Epoch [15/250] ---- Training Loss: 0.0018 ---- Validation Loss: 0.0016    SmoothL1Loss  1e-06\n",
            "Epoch [16/250] ---- Training Loss: 0.0017 ---- Validation Loss: 0.0016    SmoothL1Loss  1e-06\n",
            "Epoch [17/250] ---- Training Loss: 0.0017 ---- Validation Loss: 0.0016    SmoothL1Loss  1e-06\n",
            "Epoch [18/250] ---- Training Loss: 0.0017 ---- Validation Loss: 0.0015    SmoothL1Loss  1e-06\n",
            "Epoch [19/250] ---- Training Loss: 0.0016 ---- Validation Loss: 0.0015    SmoothL1Loss  1e-06\n",
            "Epoch [20/250] ---- Training Loss: 0.0016 ---- Validation Loss: 0.0015    SmoothL1Loss  1e-06\n",
            "Epoch [21/250] ---- Training Loss: 0.0016 ---- Validation Loss: 0.0015    SmoothL1Loss  1e-06\n",
            "Epoch [22/250] ---- Training Loss: 0.0016 ---- Validation Loss: 0.0015    SmoothL1Loss  1e-06\n",
            "Epoch [23/250] ---- Training Loss: 0.0016 ---- Validation Loss: 0.0015    SmoothL1Loss  1e-06\n",
            "Epoch [24/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0015    SmoothL1Loss  1e-06\n",
            "Epoch [25/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-06\n",
            "Epoch [26/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-06\n",
            "Epoch [27/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-06\n",
            "Epoch [28/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-06\n",
            "Epoch [29/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-06\n",
            "Epoch [30/250] ---- Training Loss: 0.0015 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-06\n",
            "Epoch [31/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-06\n",
            "Epoch [32/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-06\n",
            "Epoch [33/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0014    SmoothL1Loss  1e-06\n",
            "Epoch [34/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0013    SmoothL1Loss  1e-06\n",
            "Epoch [35/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0013    SmoothL1Loss  1e-06\n",
            "Epoch [36/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0013    SmoothL1Loss  1e-06\n",
            "Epoch [37/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0013    SmoothL1Loss  1e-06\n",
            "Epoch [38/250] ---- Training Loss: 0.0014 ---- Validation Loss: 0.0013    SmoothL1Loss  1e-06\n",
            "Epoch [39/250] ---- Training Loss: 0.0013 ---- Validation Loss: 0.0013    SmoothL1Loss  1e-06\n",
            "Epoch [40/250] ---- Training Loss: 0.0013 ---- Validation Loss: 0.0012    SmoothL1Loss  1e-06\n",
            "Epoch [41/250] ---- Training Loss: 0.0013 ---- Validation Loss: 0.0012    SmoothL1Loss  1e-06\n",
            "Epoch [42/250] ---- Training Loss: 0.0013 ---- Validation Loss: 0.0012    SmoothL1Loss  1e-06\n",
            "Epoch [43/250] ---- Training Loss: 0.0013 ---- Validation Loss: 0.0012    SmoothL1Loss  1e-06\n",
            "Epoch [44/250] ---- Training Loss: 0.0013 ---- Validation Loss: 0.0012    SmoothL1Loss  1e-06\n",
            "Epoch [45/250] ---- Training Loss: 0.0012 ---- Validation Loss: 0.0012    SmoothL1Loss  1e-06\n",
            "Epoch [46/250] ---- Training Loss: 0.0012 ---- Validation Loss: 0.0011    SmoothL1Loss  1e-06\n",
            "Epoch [47/250] ---- Training Loss: 0.0012 ---- Validation Loss: 0.0011    SmoothL1Loss  1e-06\n",
            "Epoch [48/250] ---- Training Loss: 0.0012 ---- Validation Loss: 0.0011    SmoothL1Loss  1e-06\n",
            "Epoch [49/250] ---- Training Loss: 0.0012 ---- Validation Loss: 0.0011    SmoothL1Loss  1e-06\n",
            "Epoch [50/250] ---- Training Loss: 0.0012 ---- Validation Loss: 0.0011    SmoothL1Loss  1e-06\n",
            "Epoch [51/250] ---- Training Loss: 0.0011 ---- Validation Loss: 0.0011    SmoothL1Loss  1e-06\n",
            "Epoch [52/250] ---- Training Loss: 0.0011 ---- Validation Loss: 0.0010    SmoothL1Loss  1e-06\n",
            "Epoch [53/250] ---- Training Loss: 0.0011 ---- Validation Loss: 0.0010    SmoothL1Loss  1e-06\n",
            "Epoch [54/250] ---- Training Loss: 0.0011 ---- Validation Loss: 0.0010    SmoothL1Loss  1e-06\n",
            "Epoch [55/250] ---- Training Loss: 0.0011 ---- Validation Loss: 0.0010    SmoothL1Loss  1e-06\n",
            "Epoch [56/250] ---- Training Loss: 0.0011 ---- Validation Loss: 0.0010    SmoothL1Loss  1e-06\n",
            "Epoch [57/250] ---- Training Loss: 0.0011 ---- Validation Loss: 0.0010    SmoothL1Loss  1e-06\n",
            "Epoch [58/250] ---- Training Loss: 0.0011 ---- Validation Loss: 0.0010    SmoothL1Loss  1e-06\n",
            "Epoch [59/250] ---- Training Loss: 0.0010 ---- Validation Loss: 0.0010    SmoothL1Loss  1e-06\n",
            "Epoch [60/250] ---- Training Loss: 0.0010 ---- Validation Loss: 0.0009    SmoothL1Loss  1e-06\n",
            "Epoch [61/250] ---- Training Loss: 0.0010 ---- Validation Loss: 0.0009    SmoothL1Loss  1e-06\n",
            "Epoch [62/250] ---- Training Loss: 0.0010 ---- Validation Loss: 0.0009    SmoothL1Loss  1e-06\n",
            "Epoch [63/250] ---- Training Loss: 0.0010 ---- Validation Loss: 0.0009    SmoothL1Loss  1e-06\n",
            "Epoch [64/250] ---- Training Loss: 0.0010 ---- Validation Loss: 0.0009    SmoothL1Loss  1e-06\n",
            "Epoch [65/250] ---- Training Loss: 0.0010 ---- Validation Loss: 0.0009    SmoothL1Loss  1e-06\n",
            "Epoch [66/250] ---- Training Loss: 0.0010 ---- Validation Loss: 0.0009    SmoothL1Loss  1e-06\n",
            "Epoch [67/250] ---- Training Loss: 0.0009 ---- Validation Loss: 0.0009    SmoothL1Loss  1e-06\n",
            "Epoch [68/250] ---- Training Loss: 0.0009 ---- Validation Loss: 0.0008    SmoothL1Loss  1e-06\n",
            "Epoch [69/250] ---- Training Loss: 0.0009 ---- Validation Loss: 0.0008    SmoothL1Loss  1e-06\n",
            "Epoch [70/250] ---- Training Loss: 0.0009 ---- Validation Loss: 0.0008    SmoothL1Loss  1e-06\n",
            "Epoch [71/250] ---- Training Loss: 0.0009 ---- Validation Loss: 0.0008    SmoothL1Loss  1e-06\n",
            "Epoch [72/250] ---- Training Loss: 0.0009 ---- Validation Loss: 0.0008    SmoothL1Loss  1e-06\n",
            "Epoch [73/250] ---- Training Loss: 0.0009 ---- Validation Loss: 0.0008    SmoothL1Loss  1e-06\n",
            "Epoch [74/250] ---- Training Loss: 0.0009 ---- Validation Loss: 0.0008    SmoothL1Loss  1e-06\n",
            "Epoch [75/250] ---- Training Loss: 0.0009 ---- Validation Loss: 0.0008    SmoothL1Loss  1e-06\n",
            "Epoch [76/250] ---- Training Loss: 0.0008 ---- Validation Loss: 0.0008    SmoothL1Loss  1e-06\n",
            "Epoch [77/250] ---- Training Loss: 0.0008 ---- Validation Loss: 0.0007    SmoothL1Loss  1e-06\n",
            "Epoch [78/250] ---- Training Loss: 0.0008 ---- Validation Loss: 0.0007    SmoothL1Loss  1e-06\n",
            "Epoch [79/250] ---- Training Loss: 0.0008 ---- Validation Loss: 0.0007    SmoothL1Loss  1e-06\n",
            "Epoch [80/250] ---- Training Loss: 0.0008 ---- Validation Loss: 0.0007    SmoothL1Loss  1e-06\n",
            "Epoch [81/250] ---- Training Loss: 0.0008 ---- Validation Loss: 0.0007    SmoothL1Loss  1e-06\n",
            "Epoch [82/250] ---- Training Loss: 0.0008 ---- Validation Loss: 0.0007    SmoothL1Loss  1e-06\n",
            "Epoch [83/250] ---- Training Loss: 0.0008 ---- Validation Loss: 0.0007    SmoothL1Loss  1e-06\n",
            "Epoch [84/250] ---- Training Loss: 0.0008 ---- Validation Loss: 0.0007    SmoothL1Loss  1e-06\n",
            "Epoch [85/250] ---- Training Loss: 0.0008 ---- Validation Loss: 0.0007    SmoothL1Loss  1e-06\n",
            "Epoch [86/250] ---- Training Loss: 0.0008 ---- Validation Loss: 0.0007    SmoothL1Loss  1e-06\n",
            "Epoch [87/250] ---- Training Loss: 0.0007 ---- Validation Loss: 0.0007    SmoothL1Loss  1e-06\n",
            "Epoch [88/250] ---- Training Loss: 0.0007 ---- Validation Loss: 0.0007    SmoothL1Loss  1e-06\n",
            "Epoch [89/250] ---- Training Loss: 0.0007 ---- Validation Loss: 0.0007    SmoothL1Loss  1e-06\n",
            "Epoch [90/250] ---- Training Loss: 0.0007 ---- Validation Loss: 0.0006    SmoothL1Loss  1e-06\n",
            "Epoch [91/250] ---- Training Loss: 0.0007 ---- Validation Loss: 0.0006    SmoothL1Loss  1e-06\n",
            "Epoch [92/250] ---- Training Loss: 0.0007 ---- Validation Loss: 0.0006    SmoothL1Loss  1e-06\n",
            "Epoch [93/250] ---- Training Loss: 0.0007 ---- Validation Loss: 0.0006    SmoothL1Loss  1e-06\n",
            "Epoch [94/250] ---- Training Loss: 0.0007 ---- Validation Loss: 0.0006    SmoothL1Loss  1e-06\n",
            "Epoch [95/250] ---- Training Loss: 0.0007 ---- Validation Loss: 0.0006    SmoothL1Loss  1e-06\n",
            "Epoch [96/250] ---- Training Loss: 0.0007 ---- Validation Loss: 0.0006    SmoothL1Loss  1e-06\n",
            "Epoch [97/250] ---- Training Loss: 0.0007 ---- Validation Loss: 0.0006    SmoothL1Loss  1e-06\n",
            "Epoch [98/250] ---- Training Loss: 0.0007 ---- Validation Loss: 0.0006    SmoothL1Loss  1e-06\n",
            "Epoch [99/250] ---- Training Loss: 0.0007 ---- Validation Loss: 0.0006    SmoothL1Loss  1e-06\n",
            "Epoch [100/250] ---- Training Loss: 0.0007 ---- Validation Loss: 0.0006    SmoothL1Loss  1e-06\n",
            "Epoch [101/250] ---- Training Loss: 0.0007 ---- Validation Loss: 0.0006    SmoothL1Loss  1e-06\n",
            "Epoch [102/250] ---- Training Loss: 0.0006 ---- Validation Loss: 0.0006    SmoothL1Loss  1e-06\n",
            "Epoch [103/250] ---- Training Loss: 0.0006 ---- Validation Loss: 0.0006    SmoothL1Loss  1e-06\n",
            "Epoch [104/250] ---- Training Loss: 0.0006 ---- Validation Loss: 0.0006    SmoothL1Loss  1e-06\n",
            "Epoch [105/250] ---- Training Loss: 0.0006 ---- Validation Loss: 0.0006    SmoothL1Loss  1e-06\n",
            "Epoch [106/250] ---- Training Loss: 0.0006 ---- Validation Loss: 0.0006    SmoothL1Loss  1e-06\n",
            "Epoch [107/250] ---- Training Loss: 0.0006 ---- Validation Loss: 0.0006    SmoothL1Loss  1e-06\n",
            "Epoch [108/250] ---- Training Loss: 0.0006 ---- Validation Loss: 0.0005    SmoothL1Loss  1e-06\n",
            "Epoch [109/250] ---- Training Loss: 0.0006 ---- Validation Loss: 0.0005    SmoothL1Loss  1e-06\n",
            "Epoch [110/250] ---- Training Loss: 0.0006 ---- Validation Loss: 0.0005    SmoothL1Loss  1e-06\n",
            "Epoch [111/250] ---- Training Loss: 0.0006 ---- Validation Loss: 0.0005    SmoothL1Loss  1e-06\n",
            "Epoch [112/250] ---- Training Loss: 0.0006 ---- Validation Loss: 0.0005    SmoothL1Loss  1e-06\n",
            "Epoch [113/250] ---- Training Loss: 0.0006 ---- Validation Loss: 0.0005    SmoothL1Loss  1e-06\n",
            "Epoch [114/250] ---- Training Loss: 0.0006 ---- Validation Loss: 0.0005    SmoothL1Loss  1e-06\n",
            "Epoch [115/250] ---- Training Loss: 0.0006 ---- Validation Loss: 0.0005    SmoothL1Loss  1e-06\n",
            "Epoch [116/250] ---- Training Loss: 0.0006 ---- Validation Loss: 0.0005    SmoothL1Loss  1e-06\n",
            "Epoch [117/250] ---- Training Loss: 0.0006 ---- Validation Loss: 0.0005    SmoothL1Loss  1e-06\n",
            "Epoch [118/250] ---- Training Loss: 0.0006 ---- Validation Loss: 0.0005    SmoothL1Loss  1e-06\n",
            "Epoch [119/250] ---- Training Loss: 0.0006 ---- Validation Loss: 0.0005    SmoothL1Loss  1e-06\n",
            "Epoch [120/250] ---- Training Loss: 0.0006 ---- Validation Loss: 0.0005    SmoothL1Loss  1e-06\n",
            "Epoch [121/250] ---- Training Loss: 0.0006 ---- Validation Loss: 0.0005    SmoothL1Loss  1e-06\n",
            "Epoch [122/250] ---- Training Loss: 0.0006 ---- Validation Loss: 0.0005    SmoothL1Loss  1e-06\n",
            "Epoch [123/250] ---- Training Loss: 0.0006 ---- Validation Loss: 0.0005    SmoothL1Loss  1e-06\n",
            "Epoch [124/250] ---- Training Loss: 0.0006 ---- Validation Loss: 0.0005    SmoothL1Loss  1e-06\n",
            "Epoch [125/250] ---- Training Loss: 0.0006 ---- Validation Loss: 0.0005    SmoothL1Loss  1e-06\n",
            "Epoch [126/250] ---- Training Loss: 0.0006 ---- Validation Loss: 0.0005    SmoothL1Loss  1e-06\n",
            "Epoch [127/250] ---- Training Loss: 0.0005 ---- Validation Loss: 0.0005    SmoothL1Loss  1e-06\n",
            "Epoch [128/250] ---- Training Loss: 0.0005 ---- Validation Loss: 0.0005    SmoothL1Loss  1e-06\n",
            "Epoch [129/250] ---- Training Loss: 0.0005 ---- Validation Loss: 0.0005    SmoothL1Loss  1e-06\n",
            "Epoch [130/250] ---- Training Loss: 0.0005 ---- Validation Loss: 0.0005    SmoothL1Loss  1e-06\n",
            "Epoch [131/250] ---- Training Loss: 0.0005 ---- Validation Loss: 0.0005    SmoothL1Loss  1e-06\n",
            "Epoch [132/250] ---- Training Loss: 0.0005 ---- Validation Loss: 0.0005    SmoothL1Loss  1e-06\n",
            "Epoch [133/250] ---- Training Loss: 0.0005 ---- Validation Loss: 0.0005    SmoothL1Loss  1e-06\n",
            "Epoch [134/250] ---- Training Loss: 0.0005 ---- Validation Loss: 0.0005    SmoothL1Loss  1e-06\n",
            "Epoch [135/250] ---- Training Loss: 0.0005 ---- Validation Loss: 0.0005    SmoothL1Loss  1e-06\n",
            "Epoch [136/250] ---- Training Loss: 0.0005 ---- Validation Loss: 0.0005    SmoothL1Loss  1e-06\n",
            "Epoch [137/250] ---- Training Loss: 0.0005 ---- Validation Loss: 0.0005    SmoothL1Loss  1e-06\n",
            "Epoch [138/250] ---- Training Loss: 0.0005 ---- Validation Loss: 0.0005    SmoothL1Loss  1e-06\n",
            "Epoch [139/250] ---- Training Loss: 0.0005 ---- Validation Loss: 0.0005    SmoothL1Loss  1e-06\n",
            "Epoch [140/250] ---- Training Loss: 0.0005 ---- Validation Loss: 0.0005    SmoothL1Loss  1e-06\n",
            "Epoch [141/250] ---- Training Loss: 0.0005 ---- Validation Loss: 0.0005    SmoothL1Loss  1e-06\n",
            "Epoch [142/250] ---- Training Loss: 0.0005 ---- Validation Loss: 0.0004    SmoothL1Loss  1e-06\n",
            "Epoch [143/250] ---- Training Loss: 0.0005 ---- Validation Loss: 0.0004    SmoothL1Loss  1e-06\n",
            "Epoch [144/250] ---- Training Loss: 0.0005 ---- Validation Loss: 0.0004    SmoothL1Loss  1e-06\n",
            "Epoch [145/250] ---- Training Loss: 0.0005 ---- Validation Loss: 0.0004    SmoothL1Loss  1e-06\n",
            "Epoch [146/250] ---- Training Loss: 0.0005 ---- Validation Loss: 0.0004    SmoothL1Loss  1e-06\n",
            "Epoch [147/250] ---- Training Loss: 0.0005 ---- Validation Loss: 0.0004    SmoothL1Loss  1e-06\n",
            "Epoch [148/250] ---- Training Loss: 0.0005 ---- Validation Loss: 0.0004    SmoothL1Loss  1e-06\n",
            "Epoch [149/250] ---- Training Loss: 0.0005 ---- Validation Loss: 0.0004    SmoothL1Loss  1e-06\n",
            "Epoch [150/250] ---- Training Loss: 0.0005 ---- Validation Loss: 0.0004    SmoothL1Loss  1e-06\n",
            "Epoch [151/250] ---- Training Loss: 0.0005 ---- Validation Loss: 0.0004    SmoothL1Loss  1e-06\n",
            "Epoch [152/250] ---- Training Loss: 0.0005 ---- Validation Loss: 0.0004    SmoothL1Loss  1e-06\n",
            "Epoch [153/250] ---- Training Loss: 0.0005 ---- Validation Loss: 0.0004    SmoothL1Loss  1e-06\n",
            "Epoch [154/250] ---- Training Loss: 0.0005 ---- Validation Loss: 0.0004    SmoothL1Loss  1e-06\n",
            "Epoch [155/250] ---- Training Loss: 0.0005 ---- Validation Loss: 0.0004    SmoothL1Loss  1e-06\n",
            "Epoch [156/250] ---- Training Loss: 0.0005 ---- Validation Loss: 0.0004    SmoothL1Loss  1e-06\n",
            "Epoch [157/250] ---- Training Loss: 0.0005 ---- Validation Loss: 0.0004    SmoothL1Loss  1e-06\n",
            "Epoch [158/250] ---- Training Loss: 0.0005 ---- Validation Loss: 0.0004    SmoothL1Loss  1e-06\n",
            "Epoch [159/250] ---- Training Loss: 0.0005 ---- Validation Loss: 0.0004    SmoothL1Loss  1e-06\n",
            "Epoch [160/250] ---- Training Loss: 0.0005 ---- Validation Loss: 0.0004    SmoothL1Loss  1e-06\n",
            "Epoch [161/250] ---- Training Loss: 0.0005 ---- Validation Loss: 0.0004    SmoothL1Loss  1e-06\n",
            "Epoch [162/250] ---- Training Loss: 0.0005 ---- Validation Loss: 0.0004    SmoothL1Loss  1e-06\n",
            "Epoch [163/250] ---- Training Loss: 0.0005 ---- Validation Loss: 0.0004    SmoothL1Loss  1e-06\n",
            "Epoch [164/250] ---- Training Loss: 0.0005 ---- Validation Loss: 0.0004    SmoothL1Loss  1e-06\n",
            "Epoch [165/250] ---- Training Loss: 0.0005 ---- Validation Loss: 0.0004    SmoothL1Loss  1e-06\n",
            "Epoch [166/250] ---- Training Loss: 0.0005 ---- Validation Loss: 0.0004    SmoothL1Loss  1e-06\n",
            "Epoch [167/250] ---- Training Loss: 0.0005 ---- Validation Loss: 0.0004    SmoothL1Loss  1e-06\n",
            "Epoch [168/250] ---- Training Loss: 0.0005 ---- Validation Loss: 0.0004    SmoothL1Loss  1e-06\n",
            "Epoch [169/250] ---- Training Loss: 0.0005 ---- Validation Loss: 0.0004    SmoothL1Loss  1e-06\n",
            "Epoch [170/250] ---- Training Loss: 0.0005 ---- Validation Loss: 0.0004    SmoothL1Loss  1e-06\n",
            "Epoch [171/250] ---- Training Loss: 0.0005 ---- Validation Loss: 0.0004    SmoothL1Loss  1e-06\n",
            "Epoch [172/250] ---- Training Loss: 0.0005 ---- Validation Loss: 0.0004    SmoothL1Loss  1e-06\n",
            "Epoch [173/250] ---- Training Loss: 0.0005 ---- Validation Loss: 0.0004    SmoothL1Loss  1e-06\n",
            "Epoch [174/250] ---- Training Loss: 0.0005 ---- Validation Loss: 0.0004    SmoothL1Loss  1e-06\n",
            "Epoch [175/250] ---- Training Loss: 0.0005 ---- Validation Loss: 0.0004    SmoothL1Loss  1e-06\n",
            "Epoch [176/250] ---- Training Loss: 0.0005 ---- Validation Loss: 0.0004    SmoothL1Loss  1e-06\n",
            "Epoch [177/250] ---- Training Loss: 0.0005 ---- Validation Loss: 0.0004    SmoothL1Loss  1e-06\n",
            "Epoch [178/250] ---- Training Loss: 0.0004 ---- Validation Loss: 0.0004    SmoothL1Loss  1e-06\n",
            "Epoch [179/250] ---- Training Loss: 0.0004 ---- Validation Loss: 0.0004    SmoothL1Loss  1e-06\n",
            "Epoch [180/250] ---- Training Loss: 0.0004 ---- Validation Loss: 0.0004    SmoothL1Loss  1e-06\n",
            "Epoch [181/250] ---- Training Loss: 0.0004 ---- Validation Loss: 0.0004    SmoothL1Loss  1e-06\n",
            "Epoch [182/250] ---- Training Loss: 0.0004 ---- Validation Loss: 0.0004    SmoothL1Loss  1e-06\n",
            "Epoch [183/250] ---- Training Loss: 0.0004 ---- Validation Loss: 0.0004    SmoothL1Loss  1e-06\n",
            "Epoch [184/250] ---- Training Loss: 0.0004 ---- Validation Loss: 0.0004    SmoothL1Loss  1e-06\n",
            "Epoch [185/250] ---- Training Loss: 0.0004 ---- Validation Loss: 0.0004    SmoothL1Loss  1e-06\n",
            "Epoch [186/250] ---- Training Loss: 0.0004 ---- Validation Loss: 0.0004    SmoothL1Loss  1e-06\n",
            "Epoch [187/250] ---- Training Loss: 0.0004 ---- Validation Loss: 0.0004    SmoothL1Loss  1e-06\n",
            "Epoch [188/250] ---- Training Loss: 0.0004 ---- Validation Loss: 0.0004    SmoothL1Loss  1e-06\n",
            "Epoch [189/250] ---- Training Loss: 0.0004 ---- Validation Loss: 0.0004    SmoothL1Loss  1e-06\n",
            "Epoch [190/250] ---- Training Loss: 0.0004 ---- Validation Loss: 0.0004    SmoothL1Loss  1e-06\n",
            "Epoch [191/250] ---- Training Loss: 0.0004 ---- Validation Loss: 0.0004    SmoothL1Loss  1e-06\n",
            "Epoch [192/250] ---- Training Loss: 0.0004 ---- Validation Loss: 0.0004    SmoothL1Loss  1e-06\n",
            "Epoch [193/250] ---- Training Loss: 0.0004 ---- Validation Loss: 0.0004    SmoothL1Loss  1e-06\n",
            "Epoch [194/250] ---- Training Loss: 0.0004 ---- Validation Loss: 0.0004    SmoothL1Loss  1e-06\n",
            "Epoch [195/250] ---- Training Loss: 0.0004 ---- Validation Loss: 0.0004    SmoothL1Loss  1e-06\n",
            "Epoch [196/250] ---- Training Loss: 0.0004 ---- Validation Loss: 0.0004    SmoothL1Loss  1e-06\n",
            "Epoch [197/250] ---- Training Loss: 0.0004 ---- Validation Loss: 0.0004    SmoothL1Loss  1e-06\n",
            "Epoch [198/250] ---- Training Loss: 0.0004 ---- Validation Loss: 0.0004    SmoothL1Loss  1e-06\n",
            "Epoch [199/250] ---- Training Loss: 0.0004 ---- Validation Loss: 0.0004    SmoothL1Loss  1e-06\n",
            "Epoch [200/250] ---- Training Loss: 0.0004 ---- Validation Loss: 0.0004    SmoothL1Loss  1e-06\n",
            "Epoch [201/250] ---- Training Loss: 0.0004 ---- Validation Loss: 0.0004    SmoothL1Loss  1e-06\n",
            "Epoch [202/250] ---- Training Loss: 0.0004 ---- Validation Loss: 0.0004    SmoothL1Loss  1e-06\n",
            "Epoch [203/250] ---- Training Loss: 0.0004 ---- Validation Loss: 0.0004    SmoothL1Loss  1e-06\n",
            "Epoch [204/250] ---- Training Loss: 0.0004 ---- Validation Loss: 0.0004    SmoothL1Loss  1e-06\n",
            "Epoch [205/250] ---- Training Loss: 0.0004 ---- Validation Loss: 0.0004    SmoothL1Loss  1e-06\n",
            "Epoch [206/250] ---- Training Loss: 0.0004 ---- Validation Loss: 0.0004    SmoothL1Loss  1e-06\n",
            "Epoch [207/250] ---- Training Loss: 0.0004 ---- Validation Loss: 0.0004    SmoothL1Loss  1e-06\n",
            "Epoch [208/250] ---- Training Loss: 0.0004 ---- Validation Loss: 0.0004    SmoothL1Loss  1e-06\n",
            "Epoch [209/250] ---- Training Loss: 0.0004 ---- Validation Loss: 0.0004    SmoothL1Loss  1e-06\n",
            "Epoch [210/250] ---- Training Loss: 0.0004 ---- Validation Loss: 0.0004    SmoothL1Loss  1e-06\n",
            "Epoch [211/250] ---- Training Loss: 0.0004 ---- Validation Loss: 0.0004    SmoothL1Loss  1e-06\n",
            "Epoch [212/250] ---- Training Loss: 0.0004 ---- Validation Loss: 0.0004    SmoothL1Loss  1e-06\n",
            "Epoch [213/250] ---- Training Loss: 0.0004 ---- Validation Loss: 0.0004    SmoothL1Loss  1e-06\n",
            "Epoch [214/250] ---- Training Loss: 0.0004 ---- Validation Loss: 0.0004    SmoothL1Loss  1e-06\n",
            "Epoch [215/250] ---- Training Loss: 0.0004 ---- Validation Loss: 0.0004    SmoothL1Loss  1e-06\n",
            "Epoch [216/250] ---- Training Loss: 0.0004 ---- Validation Loss: 0.0004    SmoothL1Loss  1e-06\n",
            "Epoch [217/250] ---- Training Loss: 0.0004 ---- Validation Loss: 0.0004    SmoothL1Loss  1e-06\n",
            "Epoch [218/250] ---- Training Loss: 0.0004 ---- Validation Loss: 0.0004    SmoothL1Loss  1e-06\n",
            "Epoch [219/250] ---- Training Loss: 0.0004 ---- Validation Loss: 0.0004    SmoothL1Loss  1e-06\n",
            "Epoch [220/250] ---- Training Loss: 0.0004 ---- Validation Loss: 0.0004    SmoothL1Loss  1e-06\n",
            "Epoch [221/250] ---- Training Loss: 0.0004 ---- Validation Loss: 0.0004    SmoothL1Loss  1e-06\n",
            "Epoch [222/250] ---- Training Loss: 0.0004 ---- Validation Loss: 0.0004    SmoothL1Loss  1e-06\n",
            "Epoch [223/250] ---- Training Loss: 0.0004 ---- Validation Loss: 0.0004    SmoothL1Loss  1e-06\n",
            "Epoch [224/250] ---- Training Loss: 0.0004 ---- Validation Loss: 0.0004    SmoothL1Loss  1e-06\n",
            "Epoch [225/250] ---- Training Loss: 0.0004 ---- Validation Loss: 0.0004    SmoothL1Loss  1e-06\n",
            "Epoch [226/250] ---- Training Loss: 0.0004 ---- Validation Loss: 0.0004    SmoothL1Loss  1e-06\n",
            "Epoch [227/250] ---- Training Loss: 0.0004 ---- Validation Loss: 0.0004    SmoothL1Loss  1e-06\n",
            "Epoch [228/250] ---- Training Loss: 0.0004 ---- Validation Loss: 0.0004    SmoothL1Loss  1e-06\n",
            "Epoch [229/250] ---- Training Loss: 0.0004 ---- Validation Loss: 0.0004    SmoothL1Loss  1e-06\n",
            "Epoch [230/250] ---- Training Loss: 0.0004 ---- Validation Loss: 0.0004    SmoothL1Loss  1e-06\n",
            "Epoch [231/250] ---- Training Loss: 0.0004 ---- Validation Loss: 0.0004    SmoothL1Loss  1e-06\n",
            "Epoch [232/250] ---- Training Loss: 0.0004 ---- Validation Loss: 0.0004    SmoothL1Loss  1e-06\n",
            "Epoch [233/250] ---- Training Loss: 0.0004 ---- Validation Loss: 0.0004    SmoothL1Loss  1e-06\n",
            "Epoch [234/250] ---- Training Loss: 0.0004 ---- Validation Loss: 0.0004    SmoothL1Loss  1e-06\n",
            "Epoch [235/250] ---- Training Loss: 0.0004 ---- Validation Loss: 0.0004    SmoothL1Loss  1e-06\n",
            "Epoch [236/250] ---- Training Loss: 0.0004 ---- Validation Loss: 0.0004    SmoothL1Loss  1e-06\n",
            "Epoch [237/250] ---- Training Loss: 0.0004 ---- Validation Loss: 0.0004    SmoothL1Loss  1e-06\n",
            "Epoch [238/250] ---- Training Loss: 0.0004 ---- Validation Loss: 0.0004    SmoothL1Loss  1e-06\n",
            "Epoch [239/250] ---- Training Loss: 0.0004 ---- Validation Loss: 0.0003    SmoothL1Loss  1e-06\n",
            "Epoch [240/250] ---- Training Loss: 0.0004 ---- Validation Loss: 0.0003    SmoothL1Loss  1e-06\n",
            "Epoch [241/250] ---- Training Loss: 0.0004 ---- Validation Loss: 0.0003    SmoothL1Loss  1e-06\n",
            "Epoch [242/250] ---- Training Loss: 0.0004 ---- Validation Loss: 0.0003    SmoothL1Loss  1e-06\n",
            "Epoch [243/250] ---- Training Loss: 0.0004 ---- Validation Loss: 0.0003    SmoothL1Loss  1e-06\n",
            "Epoch [244/250] ---- Training Loss: 0.0004 ---- Validation Loss: 0.0003    SmoothL1Loss  1e-06\n",
            "Epoch [245/250] ---- Training Loss: 0.0004 ---- Validation Loss: 0.0003    SmoothL1Loss  1e-06\n",
            "Epoch [246/250] ---- Training Loss: 0.0004 ---- Validation Loss: 0.0003    SmoothL1Loss  1e-06\n",
            "Epoch [247/250] ---- Training Loss: 0.0004 ---- Validation Loss: 0.0003    SmoothL1Loss  1e-06\n",
            "Epoch [248/250] ---- Training Loss: 0.0004 ---- Validation Loss: 0.0003    SmoothL1Loss  1e-06\n",
            "Epoch [249/250] ---- Training Loss: 0.0004 ---- Validation Loss: 0.0003    SmoothL1Loss  1e-06\n",
            "Epoch [250/250] ---- Training Loss: 0.0004 ---- Validation Loss: 0.0003    SmoothL1Loss  1e-06\n",
            "Saved model: model_loss_SmoothL1Loss_wd_1e-06.pth\n",
            "Best Validation Loss: 0.0003\n",
            "Best Loss Function: SmoothL1Loss\n",
            "Best Weight Decay: 1e-06\n",
            "Best model saved as 'best_fuel_consumption_model_overall.pth'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "QgiHLL31mUEE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "yg9vSugumUBg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "7o2xWWKvmT67"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#test"
      ],
      "metadata": {
        "id": "X7RXfsU_mNR8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "# Constants\n",
        "SEQUENCE_LENGTH = 600\n",
        "PLOT_SAVE_DIR = 'predicted_vs_actual_plots'\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Define the PyTorch model structure (same as the one used for training)\n",
        "class FuelConsumptionModel(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(FuelConsumptionModel, self).__init__()\n",
        "        self.lstm1 = nn.LSTM(input_size, 32, batch_first=True, bidirectional=True)\n",
        "        self.dropout1 = nn.Dropout(0.2)\n",
        "        self.lstm2 = nn.LSTM(64, 32, batch_first=True, bidirectional=True)\n",
        "        self.dropout2 = nn.Dropout(0.2)\n",
        "        self.dense = nn.Linear(64, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x, _ = self.lstm1(x)\n",
        "        x = self.dropout1(x)\n",
        "        x, _ = self.lstm2(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.dense(x)\n",
        "        return x\n",
        "\n",
        "# Load the trained model\n",
        "def load_trained_model(model_path, input_size):\n",
        "    model = FuelConsumptionModel(input_size=input_size)\n",
        "    model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n",
        "    model.to(DEVICE)\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "# Process the file and prepare segments\n",
        "def process_file(file_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "    df['Time'] = df['Time'] - df['Time'].iloc[0]\n",
        "    df['Trip fuel consumption'] = df['Trip fuel consumption'] - df['Trip fuel consumption'].iloc[0]\n",
        "    df['Acceleration'] = df['Speed'].diff().fillna(0)\n",
        "    features = df[['Engine speed', 'Speed', 'slope', 'Acceleration']]\n",
        "    df['Momentary fuel consumption'] = df['Trip fuel consumption'].diff().fillna(0)\n",
        "    target = df['Momentary fuel consumption']\n",
        "    return features, target\n",
        "\n",
        "# Pad and normalize the data\n",
        "def pad_and_normalize(data, sequence_length=SEQUENCE_LENGTH):\n",
        "    padded_data = np.zeros((len(data), sequence_length, data[0].shape[1]))\n",
        "    for i, seq in enumerate(data):\n",
        "        length = min(len(seq), sequence_length)\n",
        "        padded_data[i, :length] = seq[:length]\n",
        "\n",
        "    # Normalization (same as in your script)\n",
        "    min_val_x = [0, 0, -10, -10]\n",
        "    max_val_x = [8000, 150, 10, 10]\n",
        "    for i in range(padded_data.shape[-1]):\n",
        "        padded_data[:, :, i] = (padded_data[:, :, i] - min_val_x[i]) / (max_val_x[i] - min_val_x[i])\n",
        "\n",
        "    return torch.tensor(padded_data, dtype=torch.float32).to(DEVICE)\n",
        "\n",
        "# Predict and plot the results\n",
        "def plot_predicted_vs_real(input_file, model):\n",
        "    features, actual_values = process_file(input_file)\n",
        "    num_segments = len(features) // SEQUENCE_LENGTH\n",
        "    predictions = []\n",
        "\n",
        "    for i in range(num_segments):\n",
        "        segment = features.iloc[i * SEQUENCE_LENGTH:(i + 1) * SEQUENCE_LENGTH]\n",
        "        segment_normalized = pad_and_normalize([segment.values])\n",
        "        with torch.no_grad():\n",
        "            segment_predictions = model(segment_normalized).cpu().numpy()\n",
        "        predictions.extend(segment_predictions.flatten() * 20000)\n",
        "\n",
        "    # Handle any remaining data\n",
        "    remainder = len(features) % SEQUENCE_LENGTH\n",
        "    if remainder != 0:\n",
        "        last_segment = features.iloc[-remainder:]\n",
        "        last_segment_normalized = pad_and_normalize([last_segment.values])\n",
        "        with torch.no_grad():\n",
        "            last_segment_predictions = model(last_segment_normalized).cpu().numpy()\n",
        "        predictions.extend(last_segment_predictions.flatten() * 20000)\n",
        "\n",
        "    predictions = np.array(predictions)\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(np.cumsum(actual_values.values[:len(predictions)], axis=0), label='Real', color='blue')\n",
        "    plt.plot(np.cumsum(predictions[:len(actual_values)], axis=0), label='Predicted', color='red')\n",
        "    plt.xlabel('Index')\n",
        "    plt.ylabel('Fuel Consumption')\n",
        "    plt.title('Predicted vs Real Fuel Consumption')\n",
        "    plt.legend()\n",
        "\n",
        "    directory, filename = os.path.split(input_file)\n",
        "    plot_filename = os.path.join(directory, f'{os.path.splitext(filename)[0]}_predicted_vs_real.png')\n",
        "    plt.savefig(plot_filename)\n",
        "    plt.close()\n",
        "\n",
        "    print(f\"Plot saved as: {plot_filename}\")\n",
        "\n",
        "    # Save predictions and actual values to CSV\n",
        "    results_df = pd.DataFrame({\n",
        "        'Speed': features[\"Speed\"].iloc[:len(predictions)],\n",
        "        'Actual': np.cumsum(actual_values.values[:len(predictions)], axis=0),\n",
        "        'Predicted': np.cumsum(predictions[:len(actual_values)], axis=0)\n",
        "    })\n",
        "\n",
        "    csv_filename = os.path.join(directory, f'{os.path.splitext(filename)[0]}_predicted_vs_real.csv')\n",
        "    results_df.to_csv(csv_filename, index=False)\n",
        "\n",
        "# Paths to model and input file\n",
        "model_path = '/content/best_fuel_consumption_model.pth'  # Adjust this to your PyTorch model path\n",
        "input_file_path = '/content/NEDC_1000_slope_added.csv'\n",
        "\n",
        "# Load model and predict\n",
        "input_size = 4  # Number of features in the input data\n",
        "model = load_trained_model(model_path, input_size)\n",
        "plot_predicted_vs_real(input_file_path, model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZreM025mMne",
        "outputId": "cb729058-5314-49c8-bbe8-2a827d4513ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Plot saved as: /content/NEDC_1000_slope_added_predicted_vs_real.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bL109hS0blFn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KWKVbu6QblCQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Test with all saved model\n"
      ],
      "metadata": {
        "id": "PT91Xk2ibldP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "# Constants\n",
        "SEQUENCE_LENGTH = 600\n",
        "PLOT_SAVE_DIR = 'predicted_vs_actual_plots'\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Define the PyTorch model structure (same as the one used for training)\n",
        "class FuelConsumptionModel(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(FuelConsumptionModel, self).__init__()\n",
        "        self.lstm1 = nn.LSTM(input_size, 32, batch_first=True, bidirectional=True)\n",
        "        self.dropout1 = nn.Dropout(0.2)\n",
        "        self.lstm2 = nn.LSTM(64, 32, batch_first=True, bidirectional=True)\n",
        "        self.dropout2 = nn.Dropout(0.2)\n",
        "        self.dense = nn.Linear(64, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x, _ = self.lstm1(x)\n",
        "        x = self.dropout1(x)\n",
        "        x, _ = self.lstm2(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.dense(x)\n",
        "        return x\n",
        "\n",
        "# Load the trained model\n",
        "def load_trained_model(model_path, input_size):\n",
        "    model = FuelConsumptionModel(input_size=input_size)\n",
        "    model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n",
        "    model.to(DEVICE)\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "# Process the file and prepare segments\n",
        "def process_file(file_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "    df['Time'] = df['Time'] - df['Time'].iloc[0]\n",
        "    df['Trip fuel consumption'] = df['Trip fuel consumption'] - df['Trip fuel consumption'].iloc[0]\n",
        "    df['Acceleration'] = df['Speed'].diff().fillna(0)\n",
        "    features = df[['Engine speed', 'Speed', 'slope', 'Acceleration']]\n",
        "    df['Momentary fuel consumption'] = df['Trip fuel consumption'].diff().fillna(0)\n",
        "    target = df['Momentary fuel consumption']\n",
        "    return features, target\n",
        "\n",
        "# Pad and normalize the data\n",
        "def pad_and_normalize(data, sequence_length=SEQUENCE_LENGTH):\n",
        "    padded_data = np.zeros((len(data), sequence_length, data[0].shape[1]))\n",
        "    for i, seq in enumerate(data):\n",
        "        length = min(len(seq), sequence_length)\n",
        "        padded_data[i, :length] = seq[:length]\n",
        "\n",
        "    # Normalization (same as in your script)\n",
        "    min_val_x = [0, 0, -10, -10]\n",
        "    max_val_x = [8000, 150, 10, 10]\n",
        "    for i in range(padded_data.shape[-1]):\n",
        "        padded_data[:, :, i] = (padded_data[:, :, i] - min_val_x[i]) / (max_val_x[i] - min_val_x[i])\n",
        "\n",
        "    return torch.tensor(padded_data, dtype=torch.float32).to(DEVICE)\n",
        "\n",
        "# Predict and plot the results\n",
        "def plot_predicted_vs_real(input_file, model, model_name):\n",
        "    features, actual_values = process_file(input_file)\n",
        "    num_segments = len(features) // SEQUENCE_LENGTH\n",
        "    predictions = []\n",
        "\n",
        "    for i in range(num_segments):\n",
        "        segment = features.iloc[i * SEQUENCE_LENGTH:(i + 1) * SEQUENCE_LENGTH]\n",
        "        segment_normalized = pad_and_normalize([segment.values])\n",
        "        with torch.no_grad():\n",
        "            segment_predictions = model(segment_normalized).cpu().numpy()\n",
        "        predictions.extend(segment_predictions.flatten() * 20000)\n",
        "\n",
        "    # Handle any remaining data\n",
        "    remainder = len(features) % SEQUENCE_LENGTH\n",
        "    if remainder != 0:\n",
        "        last_segment = features.iloc[-remainder:]\n",
        "        last_segment_normalized = pad_and_normalize([last_segment.values])\n",
        "        with torch.no_grad():\n",
        "            last_segment_predictions = model(last_segment_normalized).cpu().numpy()\n",
        "        predictions.extend(last_segment_predictions.flatten() * 20000)\n",
        "\n",
        "    predictions = np.array(predictions)\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(np.cumsum(actual_values.values[:len(predictions)], axis=0), label='Real', color='blue')\n",
        "    plt.plot(np.cumsum(predictions[:len(actual_values)], axis=0), label='Predicted', color='red')\n",
        "    plt.xlabel('Index')\n",
        "    plt.ylabel('Fuel Consumption')\n",
        "    plt.title('Predicted vs Real Fuel Consumption')\n",
        "    plt.legend()\n",
        "\n",
        "    # Save plot using model name\n",
        "    directory, filename = os.path.split(input_file)\n",
        "    plot_filename = os.path.join(directory, f'{os.path.splitext(filename)[0]}_predicted_vs_real_{model_name}.png')\n",
        "    plt.savefig(plot_filename)\n",
        "    plt.close()\n",
        "\n",
        "    print(f\"Plot saved as: {plot_filename}\")\n",
        "\n",
        "    # Save predictions and actual values to CSV using model name\n",
        "    results_df = pd.DataFrame({\n",
        "        'Speed': features[\"Speed\"].iloc[:len(predictions)],\n",
        "        'Actual': np.cumsum(actual_values.values[:len(predictions)], axis=0),\n",
        "        'Predicted': np.cumsum(predictions[:len(actual_values)], axis=0)\n",
        "    })\n",
        "\n",
        "    csv_filename = os.path.join(directory, f'{os.path.splitext(filename)[0]}_predicted_vs_real_{model_name}.csv')\n",
        "    results_df.to_csv(csv_filename, index=False)\n",
        "    print(f\"CSV saved as: {csv_filename}\")\n",
        "\n",
        "# Paths to input file and directory with models\n",
        "input_file_path = '/content/NEDC_1000_slope_added.csv'\n",
        "model_dir = '/content/'  # Directory containing all saved models\n",
        "\n",
        "# List all model files in the directory\n",
        "model_files = [f for f in os.listdir(model_dir) if f.endswith('.pth')]\n",
        "\n",
        "# Load and test each model\n",
        "input_size = 4  # Number of features in the input data\n",
        "\n",
        "for model_file in model_files:\n",
        "    model_path = os.path.join(model_dir, model_file)\n",
        "    model_name = os.path.splitext(model_file)[0]  # Get the base name of the model file\n",
        "    print(f\"Testing model: {model_name}\")\n",
        "\n",
        "    model = load_trained_model(model_path, input_size)\n",
        "    plot_predicted_vs_real(input_file_path, model, model_name)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N8OBvq4hbk8Y",
        "outputId": "6d0b5194-80e1-4420-d92d-4c6a24c735d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing model: model_loss_MSELoss_wd_1e-04\n",
            "Plot saved as: /content/NEDC_1000_slope_added_predicted_vs_real_model_loss_MSELoss_wd_1e-04.png\n",
            "CSV saved as: /content/NEDC_1000_slope_added_predicted_vs_real_model_loss_MSELoss_wd_1e-04.csv\n",
            "Testing model: model_loss_SmoothL1Loss_wd_1e-05\n",
            "Plot saved as: /content/NEDC_1000_slope_added_predicted_vs_real_model_loss_SmoothL1Loss_wd_1e-05.png\n",
            "CSV saved as: /content/NEDC_1000_slope_added_predicted_vs_real_model_loss_SmoothL1Loss_wd_1e-05.csv\n",
            "Testing model: model_loss_L1Loss_wd_5e-05\n",
            "Plot saved as: /content/NEDC_1000_slope_added_predicted_vs_real_model_loss_L1Loss_wd_5e-05.png\n",
            "CSV saved as: /content/NEDC_1000_slope_added_predicted_vs_real_model_loss_L1Loss_wd_5e-05.csv\n",
            "Testing model: model_loss_MSELoss_wd_5e-05\n",
            "Plot saved as: /content/NEDC_1000_slope_added_predicted_vs_real_model_loss_MSELoss_wd_5e-05.png\n",
            "CSV saved as: /content/NEDC_1000_slope_added_predicted_vs_real_model_loss_MSELoss_wd_5e-05.csv\n",
            "Testing model: model_loss_MSELoss_wd_1e-05\n",
            "Plot saved as: /content/NEDC_1000_slope_added_predicted_vs_real_model_loss_MSELoss_wd_1e-05.png\n",
            "CSV saved as: /content/NEDC_1000_slope_added_predicted_vs_real_model_loss_MSELoss_wd_1e-05.csv\n",
            "Testing model: model_loss_L1Loss_wd_1e-04\n",
            "Plot saved as: /content/NEDC_1000_slope_added_predicted_vs_real_model_loss_L1Loss_wd_1e-04.png\n",
            "CSV saved as: /content/NEDC_1000_slope_added_predicted_vs_real_model_loss_L1Loss_wd_1e-04.csv\n",
            "Testing model: model_loss_L1Loss_wd_1e-06\n",
            "Plot saved as: /content/NEDC_1000_slope_added_predicted_vs_real_model_loss_L1Loss_wd_1e-06.png\n",
            "CSV saved as: /content/NEDC_1000_slope_added_predicted_vs_real_model_loss_L1Loss_wd_1e-06.csv\n",
            "Testing model: model_loss_L1Loss_wd_1e-05\n",
            "Plot saved as: /content/NEDC_1000_slope_added_predicted_vs_real_model_loss_L1Loss_wd_1e-05.png\n",
            "CSV saved as: /content/NEDC_1000_slope_added_predicted_vs_real_model_loss_L1Loss_wd_1e-05.csv\n",
            "Testing model: model_loss_SmoothL1Loss_wd_1e-04\n",
            "Plot saved as: /content/NEDC_1000_slope_added_predicted_vs_real_model_loss_SmoothL1Loss_wd_1e-04.png\n",
            "CSV saved as: /content/NEDC_1000_slope_added_predicted_vs_real_model_loss_SmoothL1Loss_wd_1e-04.csv\n",
            "Testing model: model_loss_SmoothL1Loss_wd_1e-06\n",
            "Plot saved as: /content/NEDC_1000_slope_added_predicted_vs_real_model_loss_SmoothL1Loss_wd_1e-06.png\n",
            "CSV saved as: /content/NEDC_1000_slope_added_predicted_vs_real_model_loss_SmoothL1Loss_wd_1e-06.csv\n",
            "Testing model: model_loss_SmoothL1Loss_wd_5e-05\n",
            "Plot saved as: /content/NEDC_1000_slope_added_predicted_vs_real_model_loss_SmoothL1Loss_wd_5e-05.png\n",
            "CSV saved as: /content/NEDC_1000_slope_added_predicted_vs_real_model_loss_SmoothL1Loss_wd_5e-05.csv\n",
            "Testing model: model_loss_MSELoss_wd_1e-06\n",
            "Plot saved as: /content/NEDC_1000_slope_added_predicted_vs_real_model_loss_MSELoss_wd_1e-06.png\n",
            "CSV saved as: /content/NEDC_1000_slope_added_predicted_vs_real_model_loss_MSELoss_wd_1e-06.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "# Constants\n",
        "SEQUENCE_LENGTH = 600\n",
        "PLOT_SAVE_DIR = 'predicted_vs_actual_plots'  # Base directory to save plots and CSVs\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Ensure the base save directory exists\n",
        "os.makedirs(PLOT_SAVE_DIR, exist_ok=True)\n",
        "\n",
        "# Define the PyTorch model structure (same as the one used for training)\n",
        "class FuelConsumptionModel(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(FuelConsumptionModel, self).__init__()\n",
        "        self.lstm1 = nn.LSTM(input_size, 32, batch_first=True, bidirectional=True)\n",
        "        self.dropout1 = nn.Dropout(0.2)\n",
        "        self.lstm2 = nn.LSTM(64, 32, batch_first=True, bidirectional=True)\n",
        "        self.dropout2 = nn.Dropout(0.2)\n",
        "        self.dense = nn.Linear(64, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x, _ = self.lstm1(x)\n",
        "        x = self.dropout1(x)\n",
        "        x, _ = self.lstm2(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.dense(x)\n",
        "        return x\n",
        "\n",
        "# Load the trained model\n",
        "def load_trained_model(model_path, input_size):\n",
        "    model = FuelConsumptionModel(input_size=input_size)\n",
        "    model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n",
        "    model.to(DEVICE)\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "# Process the file and prepare segments\n",
        "def process_file(file_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "    df['Time'] = df['Time'] - df['Time'].iloc[0]\n",
        "    df['Trip fuel consumption'] = df['Trip fuel consumption'] - df['Trip fuel consumption'].iloc[0]\n",
        "    df['Acceleration'] = df['Speed'].diff().fillna(0)\n",
        "    features = df[['Engine speed', 'Speed', 'slope', 'Acceleration']]\n",
        "    df['Momentary fuel consumption'] = df['Trip fuel consumption'].diff().fillna(0)\n",
        "    target = df['Momentary fuel consumption']\n",
        "    return features, target\n",
        "\n",
        "# Pad and normalize the data\n",
        "def pad_and_normalize(data, sequence_length=SEQUENCE_LENGTH):\n",
        "    padded_data = np.zeros((len(data), sequence_length, data[0].shape[1]))\n",
        "    for i, seq in enumerate(data):\n",
        "        length = min(len(seq), sequence_length)\n",
        "        padded_data[i, :length] = seq[:length]\n",
        "\n",
        "    # Normalization (same as in your script)\n",
        "    min_val_x = [0, 0, -10, -10]\n",
        "    max_val_x = [8000, 150, 10, 10]\n",
        "    for i in range(padded_data.shape[-1]):\n",
        "        padded_data[:, :, i] = (padded_data[:, :, i] - min_val_x[i]) / (max_val_x[i] - min_val_x[i])\n",
        "\n",
        "    return torch.tensor(padded_data, dtype=torch.float32).to(DEVICE)\n",
        "\n",
        "# Predict and plot the results\n",
        "def plot_predicted_vs_real(input_file, model, model_name):\n",
        "    features, actual_values = process_file(input_file)\n",
        "    num_segments = len(features) // SEQUENCE_LENGTH\n",
        "    predictions = []\n",
        "\n",
        "    for i in range(num_segments):\n",
        "        segment = features.iloc[i * SEQUENCE_LENGTH:(i + 1) * SEQUENCE_LENGTH]\n",
        "        segment_normalized = pad_and_normalize([segment.values])\n",
        "        with torch.no_grad():\n",
        "            segment_predictions = model(segment_normalized).cpu().numpy()\n",
        "        predictions.extend(segment_predictions.flatten() * 20000)\n",
        "\n",
        "    # Handle any remaining data\n",
        "    remainder = len(features) % SEQUENCE_LENGTH\n",
        "    if remainder != 0:\n",
        "        last_segment = features.iloc[-remainder:]\n",
        "        last_segment_normalized = pad_and_normalize([last_segment.values])\n",
        "        with torch.no_grad():\n",
        "            last_segment_predictions = model(last_segment_normalized).cpu().numpy()\n",
        "        predictions.extend(last_segment_predictions.flatten() * 20000)\n",
        "\n",
        "    predictions = np.array(predictions)\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(np.cumsum(actual_values.values[:len(predictions)], axis=0), label='Real', color='blue')\n",
        "    plt.plot(np.cumsum(predictions[:len(actual_values)], axis=0), label='Predicted', color='red')\n",
        "    plt.xlabel('Index')\n",
        "    plt.ylabel('Fuel Consumption')\n",
        "    plt.title(f'Predicted vs Real Fuel Consumption ({model_name})')\n",
        "    plt.legend()\n",
        "\n",
        "    # Create model-specific directory\n",
        "    model_save_dir = os.path.join(PLOT_SAVE_DIR, model_name)\n",
        "    os.makedirs(model_save_dir, exist_ok=True)\n",
        "\n",
        "    # Save plot using model name in the designated directory\n",
        "    plot_filename = os.path.join(model_save_dir, f'{os.path.splitext(os.path.basename(input_file))[0]}_predicted_vs_real_{model_name}.png')\n",
        "    plt.savefig(plot_filename)\n",
        "    plt.close()\n",
        "\n",
        "    print(f\"Plot saved as: {plot_filename}\")\n",
        "\n",
        "    # Save predictions and actual values to CSV using model name in the designated directory\n",
        "    results_df = pd.DataFrame({\n",
        "        'Speed': features[\"Speed\"].iloc[:len(predictions)],\n",
        "        'Actual': np.cumsum(actual_values.values[:len(predictions)], axis=0),\n",
        "        'Predicted': np.cumsum(predictions[:len(actual_values)], axis=0)\n",
        "    })\n",
        "\n",
        "    csv_filename = os.path.join(model_save_dir, f'{os.path.splitext(os.path.basename(input_file))[0]}_predicted_vs_real_{model_name}.csv')\n",
        "    results_df.to_csv(csv_filename, index=False)\n",
        "    print(f\"CSV saved as: {csv_filename}\")\n",
        "\n",
        "# Paths to input file and directory with models\n",
        "input_file_path = '/content/NEDC_1000_slope_added.csv'\n",
        "model_dir = '/content/'  # Directory containing all saved models\n",
        "\n",
        "# List all model files in the directory\n",
        "model_files = [f for f in os.listdir(model_dir) if f.endswith('.pth')]\n",
        "\n",
        "# Load and test each model\n",
        "input_size = 4  # Number of features in the input data\n",
        "\n",
        "for model_file in model_files:\n",
        "    model_path = os.path.join(model_dir, model_file)\n",
        "    model_name = os.path.splitext(model_file)[0]  # Get the base name of the model file\n",
        "    print(f\"Testing model: {model_name}\")\n",
        "\n",
        "    model = load_trained_model(model_path, input_size)\n",
        "    plot_predicted_vs_real(input_file_path, model, model_name)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89nsl4H08I7T",
        "outputId": "0bd0c5c6-7a28-481d-e6b3-d31b76c6137c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing model: model_loss_MSELoss_wd_1e-04\n",
            "Plot saved as: predicted_vs_actual_plots/model_loss_MSELoss_wd_1e-04/NEDC_1000_slope_added_predicted_vs_real_model_loss_MSELoss_wd_1e-04.png\n",
            "CSV saved as: predicted_vs_actual_plots/model_loss_MSELoss_wd_1e-04/NEDC_1000_slope_added_predicted_vs_real_model_loss_MSELoss_wd_1e-04.csv\n",
            "Testing model: model_loss_SmoothL1Loss_wd_1e-05\n",
            "Plot saved as: predicted_vs_actual_plots/model_loss_SmoothL1Loss_wd_1e-05/NEDC_1000_slope_added_predicted_vs_real_model_loss_SmoothL1Loss_wd_1e-05.png\n",
            "CSV saved as: predicted_vs_actual_plots/model_loss_SmoothL1Loss_wd_1e-05/NEDC_1000_slope_added_predicted_vs_real_model_loss_SmoothL1Loss_wd_1e-05.csv\n",
            "Testing model: model_loss_L1Loss_wd_5e-05\n",
            "Plot saved as: predicted_vs_actual_plots/model_loss_L1Loss_wd_5e-05/NEDC_1000_slope_added_predicted_vs_real_model_loss_L1Loss_wd_5e-05.png\n",
            "CSV saved as: predicted_vs_actual_plots/model_loss_L1Loss_wd_5e-05/NEDC_1000_slope_added_predicted_vs_real_model_loss_L1Loss_wd_5e-05.csv\n",
            "Testing model: model_loss_MSELoss_wd_5e-05\n",
            "Plot saved as: predicted_vs_actual_plots/model_loss_MSELoss_wd_5e-05/NEDC_1000_slope_added_predicted_vs_real_model_loss_MSELoss_wd_5e-05.png\n",
            "CSV saved as: predicted_vs_actual_plots/model_loss_MSELoss_wd_5e-05/NEDC_1000_slope_added_predicted_vs_real_model_loss_MSELoss_wd_5e-05.csv\n",
            "Testing model: model_loss_MSELoss_wd_1e-05\n",
            "Plot saved as: predicted_vs_actual_plots/model_loss_MSELoss_wd_1e-05/NEDC_1000_slope_added_predicted_vs_real_model_loss_MSELoss_wd_1e-05.png\n",
            "CSV saved as: predicted_vs_actual_plots/model_loss_MSELoss_wd_1e-05/NEDC_1000_slope_added_predicted_vs_real_model_loss_MSELoss_wd_1e-05.csv\n",
            "Testing model: model_loss_L1Loss_wd_1e-04\n",
            "Plot saved as: predicted_vs_actual_plots/model_loss_L1Loss_wd_1e-04/NEDC_1000_slope_added_predicted_vs_real_model_loss_L1Loss_wd_1e-04.png\n",
            "CSV saved as: predicted_vs_actual_plots/model_loss_L1Loss_wd_1e-04/NEDC_1000_slope_added_predicted_vs_real_model_loss_L1Loss_wd_1e-04.csv\n",
            "Testing model: model_loss_L1Loss_wd_1e-06\n",
            "Plot saved as: predicted_vs_actual_plots/model_loss_L1Loss_wd_1e-06/NEDC_1000_slope_added_predicted_vs_real_model_loss_L1Loss_wd_1e-06.png\n",
            "CSV saved as: predicted_vs_actual_plots/model_loss_L1Loss_wd_1e-06/NEDC_1000_slope_added_predicted_vs_real_model_loss_L1Loss_wd_1e-06.csv\n",
            "Testing model: model_loss_L1Loss_wd_1e-05\n",
            "Plot saved as: predicted_vs_actual_plots/model_loss_L1Loss_wd_1e-05/NEDC_1000_slope_added_predicted_vs_real_model_loss_L1Loss_wd_1e-05.png\n",
            "CSV saved as: predicted_vs_actual_plots/model_loss_L1Loss_wd_1e-05/NEDC_1000_slope_added_predicted_vs_real_model_loss_L1Loss_wd_1e-05.csv\n",
            "Testing model: model_loss_SmoothL1Loss_wd_1e-04\n",
            "Plot saved as: predicted_vs_actual_plots/model_loss_SmoothL1Loss_wd_1e-04/NEDC_1000_slope_added_predicted_vs_real_model_loss_SmoothL1Loss_wd_1e-04.png\n",
            "CSV saved as: predicted_vs_actual_plots/model_loss_SmoothL1Loss_wd_1e-04/NEDC_1000_slope_added_predicted_vs_real_model_loss_SmoothL1Loss_wd_1e-04.csv\n",
            "Testing model: model_loss_SmoothL1Loss_wd_1e-06\n",
            "Plot saved as: predicted_vs_actual_plots/model_loss_SmoothL1Loss_wd_1e-06/NEDC_1000_slope_added_predicted_vs_real_model_loss_SmoothL1Loss_wd_1e-06.png\n",
            "CSV saved as: predicted_vs_actual_plots/model_loss_SmoothL1Loss_wd_1e-06/NEDC_1000_slope_added_predicted_vs_real_model_loss_SmoothL1Loss_wd_1e-06.csv\n",
            "Testing model: model_loss_SmoothL1Loss_wd_5e-05\n",
            "Plot saved as: predicted_vs_actual_plots/model_loss_SmoothL1Loss_wd_5e-05/NEDC_1000_slope_added_predicted_vs_real_model_loss_SmoothL1Loss_wd_5e-05.png\n",
            "CSV saved as: predicted_vs_actual_plots/model_loss_SmoothL1Loss_wd_5e-05/NEDC_1000_slope_added_predicted_vs_real_model_loss_SmoothL1Loss_wd_5e-05.csv\n",
            "Testing model: model_loss_MSELoss_wd_1e-06\n",
            "Plot saved as: predicted_vs_actual_plots/model_loss_MSELoss_wd_1e-06/NEDC_1000_slope_added_predicted_vs_real_model_loss_MSELoss_wd_1e-06.png\n",
            "CSV saved as: predicted_vs_actual_plots/model_loss_MSELoss_wd_1e-06/NEDC_1000_slope_added_predicted_vs_real_model_loss_MSELoss_wd_1e-06.csv\n"
          ]
        }
      ]
    }
  ]
}
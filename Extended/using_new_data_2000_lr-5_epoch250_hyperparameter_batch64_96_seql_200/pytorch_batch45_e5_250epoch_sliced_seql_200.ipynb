{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "\n",
        "# !cp \"/content/gdrive/My Drive/DataAugumentation.zip\" .\n",
        "#ADDED NEW SOURCE\n",
        "!cp \"/content/gdrive/My Drive/data_aug(3_slices_with_repeated)_acceleration_full_data_20000.zip\" .\n",
        "!unzip -qq DataAugumentation.zip\n",
        "!unzip -qq data_aug_3_slices_with_repeated_cluster_5.zip\n",
        "!rm DataAugumentation.zip\n",
        "!rm data_aug_3_slices_with_repeated_cluster_5.zip\n",
        "data_path = 'DataAugumentation'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X_RQgrvEbkfv",
        "outputId": "f42afdf7-8c6d-43fb-cc45-1f38a8b8bd44"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "^C\n",
            "unzip:  cannot find or open DataAugumentation.zip, DataAugumentation.zip.zip or DataAugumentation.zip.ZIP.\n",
            "unzip:  cannot find or open data_aug_3_slices_with_repeated_cluster_5.zip, data_aug_3_slices_with_repeated_cluster_5.zip.zip or data_aug_3_slices_with_repeated_cluster_5.zip.ZIP.\n",
            "rm: cannot remove 'DataAugumentation.zip': No such file or directory\n",
            "rm: cannot remove 'data_aug_3_slices_with_repeated_cluster_5.zip': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -qq '/content/data_aug(3_slices_with_repeated)_acceleration_full_data_20000.zip'"
      ],
      "metadata": {
        "id": "5xBVXOonbnUe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55a5f531-a967-4a3f-cd74-64834849c58b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[/content/data_aug(3_slices_with_repeated)_acceleration_full_data_20000.zip]\n",
            "  End-of-central-directory signature not found.  Either this file is not\n",
            "  a zipfile, or it constitutes one disk of a multi-part archive.  In the\n",
            "  latter case the central directory and zipfile comment will be found on\n",
            "  the last disk(s) of this archive.\n",
            "unzip:  cannot find zipfile directory in one of /content/data_aug(3_slices_with_repeated)_acceleration_full_data_20000.zip or\n",
            "        /content/data_aug(3_slices_with_repeated)_acceleration_full_data_20000.zip.zip, and cannot find /content/data_aug(3_slices_with_repeated)_acceleration_full_data_20000.zip.ZIP, period.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import joblib\n",
        "\n",
        "SEQUENCE_LENGTH = 200  # Updated sequence length\n",
        "BATCH_SIZE = 45\n",
        "EPOCHS = 250\n",
        "LEARNING_RATE = 1e-5\n",
        "PLOT_SAVE_DIR = 'predicted_vs_actual_plots'\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(DEVICE)\n",
        "\n",
        "# Data processing function\n",
        "def process_file(file_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    df['Time'] = df['Time'] - df['Time'].iloc[0]\n",
        "\n",
        "    df['Momentary fuel consumption'] = df['Trip fuel consumption'].diff().fillna(0)\n",
        "    df['Acceleration'] = df['Speed'].diff().fillna(0)\n",
        "\n",
        "    features = df[['Engine speed', 'Speed', 'slope', 'Acceleration']]\n",
        "    target = df['Momentary fuel consumption']\n",
        "\n",
        "    return features.values, target.values\n",
        "\n",
        "# Function to slice data into chunks and append to lists\n",
        "def slice_and_append(features, target, sequence_length, X_list, y_list):\n",
        "    num_chunks = len(features) // sequence_length  # Should be 12 for your case\n",
        "\n",
        "    for i in range(num_chunks):\n",
        "        start_idx = i * sequence_length\n",
        "        end_idx = start_idx + sequence_length\n",
        "\n",
        "        # Slicing the features and target arrays\n",
        "        sliced_features = features[start_idx:end_idx]\n",
        "        sliced_target = target[start_idx:end_idx]\n",
        "\n",
        "        # Append the sliced features and target to the respective lists\n",
        "        X_list.append(sliced_features)\n",
        "        y_list.append(sliced_target)\n",
        "\n",
        "scaler_X = MinMaxScaler()\n",
        "scaler_y = MinMaxScaler()\n",
        "\n",
        "X_original = []\n",
        "y_original = []\n",
        "X_augmented = []\n",
        "y_augmented = []\n",
        "\n",
        "base_folder_path = '/content/'\n",
        "\n",
        "# Process the data and slice into smaller DataFrames\n",
        "for i in range(6):\n",
        "    if i == 5:\n",
        "        folder_path = os.path.join(base_folder_path, f'data_aug(3_slices_with_repeated)_cluster_{i}')\n",
        "        for filename in os.listdir(folder_path):\n",
        "            if filename.endswith('.csv'):\n",
        "                file_path = os.path.join(folder_path, filename)\n",
        "                features, target = process_file(file_path)\n",
        "\n",
        "                slices = filename.split('_')\n",
        "                is_original_trip = slices[2] == slices[6] and slices[6] == slices[10]\n",
        "\n",
        "                if is_original_trip:\n",
        "                    slice_and_append(features, target, SEQUENCE_LENGTH, X_original, y_original)\n",
        "                else:\n",
        "                    slice_and_append(features, target, SEQUENCE_LENGTH, X_augmented, y_augmented)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dIRpL-EGbjhP",
        "outputId": "6e0c04db-308d-40c6-a88c-fe0e04d7e271"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Function to pad or truncate sequences\n",
        "def pad_or_truncate(sequence, length):\n",
        "    if len(sequence) > length:\n",
        "        return sequence[:length]\n",
        "    elif len(sequence) < length:\n",
        "        return np.pad(sequence, ((0, length - len(sequence)), (0, 0)), mode='constant')\n",
        "    else:\n",
        "        return sequence\n",
        "\n"
      ],
      "metadata": {
        "id": "btJOSO6edHGt"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Apply padding/truncating to ensure consistent sequence length\n",
        "X_original = [pad_or_truncate(x, SEQUENCE_LENGTH) for x in X_original]\n",
        "y_original = [pad_or_truncate(y.reshape(-1, 1), SEQUENCE_LENGTH) for y in y_original]\n",
        "X_augmented = [pad_or_truncate(x, SEQUENCE_LENGTH) for x in X_augmented]\n",
        "y_augmented = [pad_or_truncate(y.reshape(-1, 1), SEQUENCE_LENGTH) for y in y_augmented]\n"
      ],
      "metadata": {
        "id": "nOGD-6Ibekzm"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Convert data to PyTorch tensors\n",
        "try:\n",
        "    X_original = torch.tensor(np.array(X_original), dtype=torch.float32).to(DEVICE)\n",
        "    y_original = torch.tensor(np.array(y_original), dtype=torch.float32).to(DEVICE)\n",
        "    X_augmented = torch.tensor(np.array(X_augmented), dtype=torch.float32).to(DEVICE)\n",
        "    y_augmented = torch.tensor(np.array(y_augmented), dtype=torch.float32).to(DEVICE)\n",
        "except Exception as e:\n",
        "    print(f\"Error during tensor conversion: {e}\")\n",
        "    print(f\"Shapes: X_original - {np.array(X_original).shape}, y_original - {np.array(y_original).shape}\")\n",
        "    print(f\"Shapes: X_augmented - {np.array(X_augmented).shape}, y_augmented - {np.array(y_augmented).shape}\")\n",
        "    raise\n"
      ],
      "metadata": {
        "id": "eTGExYs0enuV"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Split into training and test sets\n",
        "# num_test = int(0.2 * len(X_original))\n",
        "# X_test = X_original[:num_test]\n",
        "# y_test = y_original[:num_test]\n",
        "# X_train = torch.cat([X_original[num_test:], X_augmented])\n",
        "# y_train = torch.cat([y_original[num_test:], y_augmented])\n",
        "\n",
        "\n",
        "X_train = torch.cat([X_original, X_augmented])\n",
        "y_train = torch.cat([y_original, y_augmented])\n",
        "# X_train = X_original[num_test:]\n",
        "# y_train = y_original[num_test:]\n"
      ],
      "metadata": {
        "id": "DN9TrizAeqPk"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Convert the min and max values to tensors\n",
        "min_val_x = torch.tensor([0, 0, -10, -10], dtype=torch.float32).to(DEVICE)\n",
        "max_val_x = torch.tensor([8000, 150, 10, 10], dtype=torch.float32).to(DEVICE)\n",
        "\n",
        "min_val_y = torch.tensor([0], dtype=torch.float32).to(DEVICE)\n",
        "max_val_y = torch.tensor([20000], dtype=torch.float32).to(DEVICE)\n",
        "\n",
        "# Custom normalization function for X\n",
        "def custom_normalize_X(data, min_vals, max_vals):\n",
        "    for i in range(data.shape[-1]):\n",
        "        data[:, :, i] = (data[:, :, i] - min_vals[i]) / (max_vals[i] - min_vals[i])\n",
        "    return data\n",
        "\n",
        "# Custom normalization function for y\n",
        "def custom_normalize_y(data, min_val, max_val):\n",
        "    return (data - min_val) / (max_val - min_val)\n",
        "\n",
        "# Normalize X_train and X_test\n",
        "X_train_normalized = custom_normalize_X(X_train, min_val_x, max_val_x)\n",
        "# X_test_normalized = custom_normalize_X(X_test, min_val_x, max_val_x)\n",
        "\n",
        "# Normalize y_train and y_test\n",
        "y_train_normalized = custom_normalize_y(y_train, min_val_y, max_val_y)\n",
        "# y_test_normalized = custom_normalize_y(y_test, min_val_y, max_val_y)\n"
      ],
      "metadata": {
        "id": "YSaA3VdOes7k"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define the PyTorch model\n",
        "class FuelConsumptionModel(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(FuelConsumptionModel, self).__init__()\n",
        "        self.lstm1 = nn.LSTM(input_size,32, batch_first=True, bidirectional=True)\n",
        "        self.dropout1 = nn.Dropout(0.2)\n",
        "        self.lstm2 = nn.LSTM(64, 32, batch_first=True, bidirectional=True)\n",
        "        self.dropout2 = nn.Dropout(0.2)\n",
        "        self.dense = nn.Linear(64, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x, _ = self.lstm1(x)\n",
        "        x = self.dropout1(x)\n",
        "        x, _ = self.lstm2(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.dense(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "doWF6sefexia"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Instantiate the model, define the loss function and the optimizer\n",
        "model = FuelConsumptionModel(input_size=X_train_normalized.shape[-1]).to(DEVICE)\n",
        "criterion = nn.L1Loss()\n",
        "weight_decay = 1e-4  # L2 regularization factor\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=weight_decay)"
      ],
      "metadata": {
        "id": "mRtldQG5e4-a"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assume 80% of the data is used for training and 20% for validation\n",
        "train_size = int(0.8 * len(X_train_normalized))\n",
        "val_size = len(X_train_normalized) - train_size\n",
        "\n",
        "# Split the data while preserving the order\n",
        "X_train_split = X_train_normalized[:train_size]\n",
        "y_train_split = y_train_normalized[:train_size]\n",
        "\n",
        "X_val_split = X_train_normalized[train_size:]\n",
        "y_val_split = y_train_normalized[train_size:]\n",
        "\n",
        "\n",
        "\n",
        "# BATCH_SIZE = 96\n",
        "# # Create DataLoader for training and validation sets\n",
        "# train_loader = DataLoader(TensorDataset(X_train_split, y_train_split), batch_size=BATCH_SIZE, shuffle=True)\n",
        "# val_loader = DataLoader(TensorDataset(X_val_split, y_val_split), batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NrURTgcFgyYL"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import StepLR  # Importing StepLR for learning rate decay\n",
        "\n",
        "# Assuming `FuelConsumptionModel`, `X_train_normalized`, `train_loader`, `val_loader`, `DEVICE`, and `EPOCHS` are defined\n",
        "\n",
        "# Choose a single loss function to use for all experiments\n",
        "criterion = nn.L1Loss()  # You can change this to your preferred loss function\n",
        "\n",
        "# Define weight decay values and initial learning rates for decay schedules\n",
        "weight_decay_values = 1e-5\n",
        "initial_learning_rates = [1e-4]  # Two starting learning rates\n",
        "\n",
        "# Initialize variables to track the best parameters\n",
        "best_weight_decay = None\n",
        "best_initial_lr = None\n",
        "best_model_weights = None\n",
        "best_val_loss = float('inf')\n",
        "\n",
        "ini_batches = [45,64,96]\n",
        "# Loop over all combinations of initial learning rates and weight decay values\n",
        "for ini_batch in ini_batches:\n",
        "    BATCH_SIZE = ini_batch\n",
        "    # Create DataLoader for training and validation sets\n",
        "    train_loader = DataLoader(TensorDataset(X_train_split, y_train_split), batch_size=BATCH_SIZE, shuffle=True)\n",
        "    val_loader = DataLoader(TensorDataset(X_val_split, y_val_split), batch_size=BATCH_SIZE, shuffle=False)\n",
        "    for init_lr in initial_learning_rates:\n",
        "        # Initialize model, criterion, and optimizer for each combination\n",
        "        model = FuelConsumptionModel(input_size=X_train_normalized.shape[-1]).to(DEVICE)\n",
        "        optimizer = optim.Adam(model.parameters(), lr=init_lr, weight_decay=weight_decay_values)\n",
        "        scheduler = StepLR(optimizer, step_size=50, gamma=0.5)  # Decay LR by 0.5 every 50 epochs\n",
        "\n",
        "        # Early stopping parameters\n",
        "        patience = 10  # Number of epochs to wait before stopping if no improvement\n",
        "        best_loss = float('inf')  # Initialize best loss to infinity\n",
        "        epochs_without_improvement = 0  # Counter for epochs without improvement\n",
        "\n",
        "        # Training loop with early stopping\n",
        "        model.train()\n",
        "        for epoch in range(EPOCHS):\n",
        "            running_loss = 0.0\n",
        "            model.train()  # Ensure model is in training mode\n",
        "\n",
        "            # Training phase\n",
        "            for inputs, targets in train_loader:\n",
        "                inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)  # Ensure data is on the correct device\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, targets)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                running_loss += loss.item()\n",
        "\n",
        "            # Calculate average loss for the epoch\n",
        "            avg_training_loss = running_loss / len(train_loader)\n",
        "\n",
        "            # Validation phase\n",
        "            model.eval()  # Switch to evaluation mode\n",
        "            val_running_loss = 0.0\n",
        "            with torch.no_grad():\n",
        "                for inputs, targets in val_loader:  # Assume you have a validation DataLoader `val_loader`\n",
        "                    inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)  # Ensure data is on the correct device\n",
        "                    outputs = model(inputs)\n",
        "                    val_loss = criterion(outputs, targets)\n",
        "                    val_running_loss += val_loss.item()\n",
        "\n",
        "            avg_val_loss = val_running_loss / len(val_loader)\n",
        "\n",
        "            current_lr = optimizer.param_groups[0]['lr']\n",
        "            print(f\"Epoch [{epoch+1}/{EPOCHS}] ---- Training Loss: {avg_training_loss:.4f} ---- Validation Loss: {avg_val_loss:.4f} ---- Learning Rate: {current_lr:.2e}    Init LR: {init_lr:.0e}  batch: {BATCH_SIZE}\")\n",
        "\n",
        "            # print(f\"Epoch [{epoch+1}/{EPOCHS}] ---- Training Loss: {avg_training_loss:.4f} ---- Validation Loss: {avg_val_loss:.4f}    Init LR: {init_lr:.0e}  WD: {wd:.0e}\")\n",
        "\n",
        "            # Adjust the learning rate\n",
        "            scheduler.step()\n",
        "\n",
        "            # Early stopping check\n",
        "            if avg_val_loss < best_loss:\n",
        "                best_loss = avg_val_loss\n",
        "                epochs_without_improvement = 0\n",
        "            else:\n",
        "                epochs_without_improvement += 1\n",
        "                if epochs_without_improvement >= patience:\n",
        "                    print(f\"Early stopping at epoch {epoch+1}\")\n",
        "                    break\n",
        "\n",
        "        # Save the model for the current combination\n",
        "        model_filename = f'best_fuel_consumption_model_batch{BATCH_SIZE}_l50.pth'\n",
        "        torch.save(model.state_dict(), model_filename)\n",
        "        print(f\"Saved model: {model_filename}\")\n",
        "\n",
        "        # Update best parameters based on validation loss\n",
        "        if best_loss < best_val_loss:\n",
        "            best_val_loss = best_loss\n",
        "            best_weight_decay = wd\n",
        "            best_initial_lr = init_lr\n",
        "            best_model_weights = model.state_dict()  # Store the best model weights\n",
        "\n",
        "# After all combinations, print the best initial learning rate and weight decay\n",
        "print(f\"Best Validation Loss: {best_val_loss:.4f}\")\n",
        "print(f\"Best Initial Learning Rate: {best_initial_lr}\")\n",
        "print(f\"Best Weight Decay: {best_weight_decay}\")\n",
        "\n",
        "# Save the best model weights\n",
        "torch.save(best_model_weights, 'best_fuel_consumption_model_batch96.pth')\n",
        "print(\"Best model saved as 'best_fuel_consumption_model_overall.pth'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_z0wpMf9gK-h",
        "outputId": "1fec6d3e-11ba-46a3-cc91-2c2b166abfaf"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/250] ---- Training Loss: 0.0377 ---- Validation Loss: 0.0318 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 45\n",
            "Epoch [2/250] ---- Training Loss: 0.0305 ---- Validation Loss: 0.0272 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 45\n",
            "Epoch [3/250] ---- Training Loss: 0.0262 ---- Validation Loss: 0.0225 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 45\n",
            "Epoch [4/250] ---- Training Loss: 0.0226 ---- Validation Loss: 0.0200 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 45\n",
            "Epoch [5/250] ---- Training Loss: 0.0208 ---- Validation Loss: 0.0190 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 45\n",
            "Epoch [6/250] ---- Training Loss: 0.0199 ---- Validation Loss: 0.0182 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 45\n",
            "Epoch [7/250] ---- Training Loss: 0.0192 ---- Validation Loss: 0.0178 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 45\n",
            "Epoch [8/250] ---- Training Loss: 0.0188 ---- Validation Loss: 0.0174 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 45\n",
            "Epoch [9/250] ---- Training Loss: 0.0185 ---- Validation Loss: 0.0172 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 45\n",
            "Epoch [10/250] ---- Training Loss: 0.0183 ---- Validation Loss: 0.0170 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 45\n",
            "Epoch [11/250] ---- Training Loss: 0.0181 ---- Validation Loss: 0.0169 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 45\n",
            "Epoch [12/250] ---- Training Loss: 0.0179 ---- Validation Loss: 0.0167 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 45\n",
            "Epoch [13/250] ---- Training Loss: 0.0178 ---- Validation Loss: 0.0167 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 45\n",
            "Epoch [14/250] ---- Training Loss: 0.0177 ---- Validation Loss: 0.0166 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 45\n",
            "Epoch [15/250] ---- Training Loss: 0.0176 ---- Validation Loss: 0.0167 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 45\n",
            "Epoch [16/250] ---- Training Loss: 0.0175 ---- Validation Loss: 0.0164 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 45\n",
            "Epoch [17/250] ---- Training Loss: 0.0174 ---- Validation Loss: 0.0163 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 45\n",
            "Epoch [18/250] ---- Training Loss: 0.0173 ---- Validation Loss: 0.0162 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 45\n",
            "Epoch [19/250] ---- Training Loss: 0.0172 ---- Validation Loss: 0.0161 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 45\n",
            "Epoch [20/250] ---- Training Loss: 0.0172 ---- Validation Loss: 0.0160 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 45\n",
            "Epoch [21/250] ---- Training Loss: 0.0171 ---- Validation Loss: 0.0159 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 45\n",
            "Epoch [22/250] ---- Training Loss: 0.0170 ---- Validation Loss: 0.0159 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 45\n",
            "Epoch [23/250] ---- Training Loss: 0.0169 ---- Validation Loss: 0.0157 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 45\n",
            "Epoch [24/250] ---- Training Loss: 0.0170 ---- Validation Loss: 0.0158 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 45\n",
            "Epoch [25/250] ---- Training Loss: 0.0168 ---- Validation Loss: 0.0157 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 45\n",
            "Epoch [26/250] ---- Training Loss: 0.0167 ---- Validation Loss: 0.0156 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 45\n",
            "Epoch [27/250] ---- Training Loss: 0.0167 ---- Validation Loss: 0.0157 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 45\n",
            "Epoch [28/250] ---- Training Loss: 0.0166 ---- Validation Loss: 0.0155 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 45\n",
            "Epoch [29/250] ---- Training Loss: 0.0165 ---- Validation Loss: 0.0153 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 45\n",
            "Epoch [30/250] ---- Training Loss: 0.0165 ---- Validation Loss: 0.0152 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 45\n",
            "Epoch [31/250] ---- Training Loss: 0.0164 ---- Validation Loss: 0.0152 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 45\n",
            "Epoch [32/250] ---- Training Loss: 0.0163 ---- Validation Loss: 0.0151 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 45\n",
            "Epoch [33/250] ---- Training Loss: 0.0163 ---- Validation Loss: 0.0150 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 45\n",
            "Epoch [34/250] ---- Training Loss: 0.0162 ---- Validation Loss: 0.0149 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 45\n",
            "Epoch [35/250] ---- Training Loss: 0.0161 ---- Validation Loss: 0.0148 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 45\n",
            "Epoch [36/250] ---- Training Loss: 0.0160 ---- Validation Loss: 0.0149 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 45\n",
            "Epoch [37/250] ---- Training Loss: 0.0160 ---- Validation Loss: 0.0147 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 45\n",
            "Epoch [38/250] ---- Training Loss: 0.0159 ---- Validation Loss: 0.0146 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 45\n",
            "Epoch [39/250] ---- Training Loss: 0.0158 ---- Validation Loss: 0.0145 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 45\n",
            "Epoch [40/250] ---- Training Loss: 0.0157 ---- Validation Loss: 0.0146 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 45\n",
            "Epoch [41/250] ---- Training Loss: 0.0157 ---- Validation Loss: 0.0144 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 45\n",
            "Epoch [42/250] ---- Training Loss: 0.0156 ---- Validation Loss: 0.0144 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 45\n",
            "Epoch [43/250] ---- Training Loss: 0.0156 ---- Validation Loss: 0.0143 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 45\n",
            "Epoch [44/250] ---- Training Loss: 0.0155 ---- Validation Loss: 0.0143 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 45\n",
            "Epoch [45/250] ---- Training Loss: 0.0155 ---- Validation Loss: 0.0144 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 45\n",
            "Epoch [46/250] ---- Training Loss: 0.0154 ---- Validation Loss: 0.0141 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 45\n",
            "Epoch [47/250] ---- Training Loss: 0.0154 ---- Validation Loss: 0.0140 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 45\n",
            "Epoch [48/250] ---- Training Loss: 0.0153 ---- Validation Loss: 0.0141 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 45\n",
            "Epoch [49/250] ---- Training Loss: 0.0153 ---- Validation Loss: 0.0139 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 45\n",
            "Epoch [50/250] ---- Training Loss: 0.0152 ---- Validation Loss: 0.0138 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 45\n",
            "Epoch [51/250] ---- Training Loss: 0.0152 ---- Validation Loss: 0.0138 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [52/250] ---- Training Loss: 0.0152 ---- Validation Loss: 0.0138 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [53/250] ---- Training Loss: 0.0151 ---- Validation Loss: 0.0138 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [54/250] ---- Training Loss: 0.0151 ---- Validation Loss: 0.0138 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [55/250] ---- Training Loss: 0.0151 ---- Validation Loss: 0.0138 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [56/250] ---- Training Loss: 0.0151 ---- Validation Loss: 0.0138 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [57/250] ---- Training Loss: 0.0150 ---- Validation Loss: 0.0137 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [58/250] ---- Training Loss: 0.0150 ---- Validation Loss: 0.0137 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [59/250] ---- Training Loss: 0.0150 ---- Validation Loss: 0.0137 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [60/250] ---- Training Loss: 0.0150 ---- Validation Loss: 0.0136 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [61/250] ---- Training Loss: 0.0150 ---- Validation Loss: 0.0136 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [62/250] ---- Training Loss: 0.0149 ---- Validation Loss: 0.0135 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [63/250] ---- Training Loss: 0.0149 ---- Validation Loss: 0.0136 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [64/250] ---- Training Loss: 0.0149 ---- Validation Loss: 0.0135 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [65/250] ---- Training Loss: 0.0149 ---- Validation Loss: 0.0135 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [66/250] ---- Training Loss: 0.0148 ---- Validation Loss: 0.0134 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [67/250] ---- Training Loss: 0.0148 ---- Validation Loss: 0.0134 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [68/250] ---- Training Loss: 0.0148 ---- Validation Loss: 0.0134 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [69/250] ---- Training Loss: 0.0148 ---- Validation Loss: 0.0134 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [70/250] ---- Training Loss: 0.0148 ---- Validation Loss: 0.0134 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [71/250] ---- Training Loss: 0.0148 ---- Validation Loss: 0.0133 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [72/250] ---- Training Loss: 0.0147 ---- Validation Loss: 0.0133 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [73/250] ---- Training Loss: 0.0147 ---- Validation Loss: 0.0135 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [74/250] ---- Training Loss: 0.0147 ---- Validation Loss: 0.0134 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [75/250] ---- Training Loss: 0.0147 ---- Validation Loss: 0.0133 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [76/250] ---- Training Loss: 0.0146 ---- Validation Loss: 0.0133 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [77/250] ---- Training Loss: 0.0146 ---- Validation Loss: 0.0133 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [78/250] ---- Training Loss: 0.0146 ---- Validation Loss: 0.0132 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [79/250] ---- Training Loss: 0.0146 ---- Validation Loss: 0.0132 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [80/250] ---- Training Loss: 0.0146 ---- Validation Loss: 0.0132 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [81/250] ---- Training Loss: 0.0146 ---- Validation Loss: 0.0131 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [82/250] ---- Training Loss: 0.0146 ---- Validation Loss: 0.0131 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [83/250] ---- Training Loss: 0.0145 ---- Validation Loss: 0.0131 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [84/250] ---- Training Loss: 0.0145 ---- Validation Loss: 0.0131 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [85/250] ---- Training Loss: 0.0145 ---- Validation Loss: 0.0131 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [86/250] ---- Training Loss: 0.0145 ---- Validation Loss: 0.0131 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [87/250] ---- Training Loss: 0.0145 ---- Validation Loss: 0.0132 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [88/250] ---- Training Loss: 0.0145 ---- Validation Loss: 0.0130 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [89/250] ---- Training Loss: 0.0144 ---- Validation Loss: 0.0131 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [90/250] ---- Training Loss: 0.0144 ---- Validation Loss: 0.0130 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [91/250] ---- Training Loss: 0.0144 ---- Validation Loss: 0.0131 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [92/250] ---- Training Loss: 0.0144 ---- Validation Loss: 0.0130 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [93/250] ---- Training Loss: 0.0144 ---- Validation Loss: 0.0130 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [94/250] ---- Training Loss: 0.0144 ---- Validation Loss: 0.0130 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [95/250] ---- Training Loss: 0.0144 ---- Validation Loss: 0.0129 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [96/250] ---- Training Loss: 0.0144 ---- Validation Loss: 0.0130 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [97/250] ---- Training Loss: 0.0143 ---- Validation Loss: 0.0129 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [98/250] ---- Training Loss: 0.0143 ---- Validation Loss: 0.0129 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [99/250] ---- Training Loss: 0.0143 ---- Validation Loss: 0.0130 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [100/250] ---- Training Loss: 0.0143 ---- Validation Loss: 0.0129 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [101/250] ---- Training Loss: 0.0143 ---- Validation Loss: 0.0129 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [102/250] ---- Training Loss: 0.0143 ---- Validation Loss: 0.0128 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [103/250] ---- Training Loss: 0.0143 ---- Validation Loss: 0.0128 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [104/250] ---- Training Loss: 0.0142 ---- Validation Loss: 0.0128 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [105/250] ---- Training Loss: 0.0142 ---- Validation Loss: 0.0128 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [106/250] ---- Training Loss: 0.0142 ---- Validation Loss: 0.0128 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [107/250] ---- Training Loss: 0.0142 ---- Validation Loss: 0.0128 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [108/250] ---- Training Loss: 0.0142 ---- Validation Loss: 0.0128 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [109/250] ---- Training Loss: 0.0142 ---- Validation Loss: 0.0128 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [110/250] ---- Training Loss: 0.0142 ---- Validation Loss: 0.0128 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [111/250] ---- Training Loss: 0.0142 ---- Validation Loss: 0.0128 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [112/250] ---- Training Loss: 0.0142 ---- Validation Loss: 0.0128 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [113/250] ---- Training Loss: 0.0142 ---- Validation Loss: 0.0127 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [114/250] ---- Training Loss: 0.0142 ---- Validation Loss: 0.0128 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [115/250] ---- Training Loss: 0.0142 ---- Validation Loss: 0.0127 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [116/250] ---- Training Loss: 0.0142 ---- Validation Loss: 0.0127 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [117/250] ---- Training Loss: 0.0142 ---- Validation Loss: 0.0127 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [118/250] ---- Training Loss: 0.0142 ---- Validation Loss: 0.0127 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [119/250] ---- Training Loss: 0.0141 ---- Validation Loss: 0.0127 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [120/250] ---- Training Loss: 0.0141 ---- Validation Loss: 0.0127 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [121/250] ---- Training Loss: 0.0141 ---- Validation Loss: 0.0127 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [122/250] ---- Training Loss: 0.0141 ---- Validation Loss: 0.0127 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [123/250] ---- Training Loss: 0.0141 ---- Validation Loss: 0.0127 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [124/250] ---- Training Loss: 0.0141 ---- Validation Loss: 0.0127 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [125/250] ---- Training Loss: 0.0141 ---- Validation Loss: 0.0127 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [126/250] ---- Training Loss: 0.0141 ---- Validation Loss: 0.0127 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [127/250] ---- Training Loss: 0.0141 ---- Validation Loss: 0.0127 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [128/250] ---- Training Loss: 0.0141 ---- Validation Loss: 0.0127 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [129/250] ---- Training Loss: 0.0141 ---- Validation Loss: 0.0127 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [130/250] ---- Training Loss: 0.0141 ---- Validation Loss: 0.0126 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [131/250] ---- Training Loss: 0.0141 ---- Validation Loss: 0.0126 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [132/250] ---- Training Loss: 0.0141 ---- Validation Loss: 0.0126 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [133/250] ---- Training Loss: 0.0140 ---- Validation Loss: 0.0126 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [134/250] ---- Training Loss: 0.0140 ---- Validation Loss: 0.0126 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [135/250] ---- Training Loss: 0.0140 ---- Validation Loss: 0.0126 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [136/250] ---- Training Loss: 0.0140 ---- Validation Loss: 0.0126 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [137/250] ---- Training Loss: 0.0140 ---- Validation Loss: 0.0126 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [138/250] ---- Training Loss: 0.0140 ---- Validation Loss: 0.0126 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [139/250] ---- Training Loss: 0.0140 ---- Validation Loss: 0.0126 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [140/250] ---- Training Loss: 0.0140 ---- Validation Loss: 0.0126 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [141/250] ---- Training Loss: 0.0140 ---- Validation Loss: 0.0126 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [142/250] ---- Training Loss: 0.0140 ---- Validation Loss: 0.0126 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [143/250] ---- Training Loss: 0.0140 ---- Validation Loss: 0.0126 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [144/250] ---- Training Loss: 0.0140 ---- Validation Loss: 0.0125 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [145/250] ---- Training Loss: 0.0140 ---- Validation Loss: 0.0126 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [146/250] ---- Training Loss: 0.0140 ---- Validation Loss: 0.0126 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [147/250] ---- Training Loss: 0.0140 ---- Validation Loss: 0.0125 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [148/250] ---- Training Loss: 0.0140 ---- Validation Loss: 0.0125 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [149/250] ---- Training Loss: 0.0139 ---- Validation Loss: 0.0125 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [150/250] ---- Training Loss: 0.0139 ---- Validation Loss: 0.0125 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [151/250] ---- Training Loss: 0.0139 ---- Validation Loss: 0.0125 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [152/250] ---- Training Loss: 0.0139 ---- Validation Loss: 0.0125 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [153/250] ---- Training Loss: 0.0139 ---- Validation Loss: 0.0125 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [154/250] ---- Training Loss: 0.0139 ---- Validation Loss: 0.0125 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [155/250] ---- Training Loss: 0.0139 ---- Validation Loss: 0.0125 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [156/250] ---- Training Loss: 0.0139 ---- Validation Loss: 0.0125 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [157/250] ---- Training Loss: 0.0139 ---- Validation Loss: 0.0125 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [158/250] ---- Training Loss: 0.0139 ---- Validation Loss: 0.0125 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [159/250] ---- Training Loss: 0.0139 ---- Validation Loss: 0.0124 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [160/250] ---- Training Loss: 0.0139 ---- Validation Loss: 0.0124 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [161/250] ---- Training Loss: 0.0139 ---- Validation Loss: 0.0124 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [162/250] ---- Training Loss: 0.0139 ---- Validation Loss: 0.0124 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [163/250] ---- Training Loss: 0.0139 ---- Validation Loss: 0.0124 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [164/250] ---- Training Loss: 0.0139 ---- Validation Loss: 0.0124 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [165/250] ---- Training Loss: 0.0139 ---- Validation Loss: 0.0124 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [166/250] ---- Training Loss: 0.0139 ---- Validation Loss: 0.0124 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [167/250] ---- Training Loss: 0.0139 ---- Validation Loss: 0.0124 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [168/250] ---- Training Loss: 0.0139 ---- Validation Loss: 0.0124 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [169/250] ---- Training Loss: 0.0139 ---- Validation Loss: 0.0124 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [170/250] ---- Training Loss: 0.0139 ---- Validation Loss: 0.0124 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [171/250] ---- Training Loss: 0.0139 ---- Validation Loss: 0.0124 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [172/250] ---- Training Loss: 0.0139 ---- Validation Loss: 0.0124 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [173/250] ---- Training Loss: 0.0139 ---- Validation Loss: 0.0124 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [174/250] ---- Training Loss: 0.0138 ---- Validation Loss: 0.0124 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [175/250] ---- Training Loss: 0.0138 ---- Validation Loss: 0.0124 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [176/250] ---- Training Loss: 0.0138 ---- Validation Loss: 0.0124 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [177/250] ---- Training Loss: 0.0138 ---- Validation Loss: 0.0124 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [178/250] ---- Training Loss: 0.0138 ---- Validation Loss: 0.0124 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [179/250] ---- Training Loss: 0.0138 ---- Validation Loss: 0.0124 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [180/250] ---- Training Loss: 0.0138 ---- Validation Loss: 0.0124 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [181/250] ---- Training Loss: 0.0138 ---- Validation Loss: 0.0124 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [182/250] ---- Training Loss: 0.0138 ---- Validation Loss: 0.0124 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [183/250] ---- Training Loss: 0.0138 ---- Validation Loss: 0.0124 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [184/250] ---- Training Loss: 0.0138 ---- Validation Loss: 0.0124 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [185/250] ---- Training Loss: 0.0138 ---- Validation Loss: 0.0124 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [186/250] ---- Training Loss: 0.0138 ---- Validation Loss: 0.0124 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [187/250] ---- Training Loss: 0.0138 ---- Validation Loss: 0.0123 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [188/250] ---- Training Loss: 0.0138 ---- Validation Loss: 0.0123 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [189/250] ---- Training Loss: 0.0138 ---- Validation Loss: 0.0123 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [190/250] ---- Training Loss: 0.0138 ---- Validation Loss: 0.0124 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [191/250] ---- Training Loss: 0.0138 ---- Validation Loss: 0.0124 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [192/250] ---- Training Loss: 0.0138 ---- Validation Loss: 0.0123 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [193/250] ---- Training Loss: 0.0138 ---- Validation Loss: 0.0123 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [194/250] ---- Training Loss: 0.0138 ---- Validation Loss: 0.0124 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [195/250] ---- Training Loss: 0.0138 ---- Validation Loss: 0.0123 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [196/250] ---- Training Loss: 0.0138 ---- Validation Loss: 0.0123 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [197/250] ---- Training Loss: 0.0138 ---- Validation Loss: 0.0123 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [198/250] ---- Training Loss: 0.0138 ---- Validation Loss: 0.0123 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [199/250] ---- Training Loss: 0.0138 ---- Validation Loss: 0.0123 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [200/250] ---- Training Loss: 0.0138 ---- Validation Loss: 0.0123 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 45\n",
            "Epoch [201/250] ---- Training Loss: 0.0138 ---- Validation Loss: 0.0123 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 45\n",
            "Epoch [202/250] ---- Training Loss: 0.0138 ---- Validation Loss: 0.0123 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 45\n",
            "Epoch [203/250] ---- Training Loss: 0.0137 ---- Validation Loss: 0.0123 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 45\n",
            "Epoch [204/250] ---- Training Loss: 0.0137 ---- Validation Loss: 0.0123 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 45\n",
            "Epoch [205/250] ---- Training Loss: 0.0138 ---- Validation Loss: 0.0123 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 45\n",
            "Epoch [206/250] ---- Training Loss: 0.0137 ---- Validation Loss: 0.0123 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 45\n",
            "Epoch [207/250] ---- Training Loss: 0.0137 ---- Validation Loss: 0.0123 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 45\n",
            "Epoch [208/250] ---- Training Loss: 0.0137 ---- Validation Loss: 0.0123 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 45\n",
            "Epoch [209/250] ---- Training Loss: 0.0137 ---- Validation Loss: 0.0123 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 45\n",
            "Epoch [210/250] ---- Training Loss: 0.0137 ---- Validation Loss: 0.0123 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 45\n",
            "Epoch [211/250] ---- Training Loss: 0.0137 ---- Validation Loss: 0.0123 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 45\n",
            "Epoch [212/250] ---- Training Loss: 0.0137 ---- Validation Loss: 0.0123 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 45\n",
            "Epoch [213/250] ---- Training Loss: 0.0137 ---- Validation Loss: 0.0123 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 45\n",
            "Epoch [214/250] ---- Training Loss: 0.0137 ---- Validation Loss: 0.0123 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 45\n",
            "Epoch [215/250] ---- Training Loss: 0.0137 ---- Validation Loss: 0.0123 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 45\n",
            "Epoch [216/250] ---- Training Loss: 0.0137 ---- Validation Loss: 0.0123 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 45\n",
            "Epoch [217/250] ---- Training Loss: 0.0137 ---- Validation Loss: 0.0123 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 45\n",
            "Epoch [218/250] ---- Training Loss: 0.0137 ---- Validation Loss: 0.0123 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 45\n",
            "Epoch [219/250] ---- Training Loss: 0.0137 ---- Validation Loss: 0.0123 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 45\n",
            "Epoch [220/250] ---- Training Loss: 0.0137 ---- Validation Loss: 0.0123 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 45\n",
            "Epoch [221/250] ---- Training Loss: 0.0137 ---- Validation Loss: 0.0123 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 45\n",
            "Epoch [222/250] ---- Training Loss: 0.0137 ---- Validation Loss: 0.0123 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 45\n",
            "Epoch [223/250] ---- Training Loss: 0.0137 ---- Validation Loss: 0.0123 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 45\n",
            "Epoch [224/250] ---- Training Loss: 0.0137 ---- Validation Loss: 0.0123 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 45\n",
            "Epoch [225/250] ---- Training Loss: 0.0137 ---- Validation Loss: 0.0123 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 45\n",
            "Epoch [226/250] ---- Training Loss: 0.0137 ---- Validation Loss: 0.0123 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 45\n",
            "Epoch [227/250] ---- Training Loss: 0.0137 ---- Validation Loss: 0.0123 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 45\n",
            "Epoch [228/250] ---- Training Loss: 0.0137 ---- Validation Loss: 0.0122 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 45\n",
            "Epoch [229/250] ---- Training Loss: 0.0137 ---- Validation Loss: 0.0123 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 45\n",
            "Epoch [230/250] ---- Training Loss: 0.0137 ---- Validation Loss: 0.0123 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 45\n",
            "Epoch [231/250] ---- Training Loss: 0.0137 ---- Validation Loss: 0.0123 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 45\n",
            "Epoch [232/250] ---- Training Loss: 0.0137 ---- Validation Loss: 0.0122 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 45\n",
            "Epoch [233/250] ---- Training Loss: 0.0137 ---- Validation Loss: 0.0122 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 45\n",
            "Epoch [234/250] ---- Training Loss: 0.0137 ---- Validation Loss: 0.0122 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 45\n",
            "Epoch [235/250] ---- Training Loss: 0.0137 ---- Validation Loss: 0.0122 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 45\n",
            "Epoch [236/250] ---- Training Loss: 0.0137 ---- Validation Loss: 0.0122 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 45\n",
            "Epoch [237/250] ---- Training Loss: 0.0137 ---- Validation Loss: 0.0122 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 45\n",
            "Epoch [238/250] ---- Training Loss: 0.0137 ---- Validation Loss: 0.0123 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 45\n",
            "Epoch [239/250] ---- Training Loss: 0.0137 ---- Validation Loss: 0.0122 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 45\n",
            "Epoch [240/250] ---- Training Loss: 0.0137 ---- Validation Loss: 0.0122 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 45\n",
            "Epoch [241/250] ---- Training Loss: 0.0137 ---- Validation Loss: 0.0122 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 45\n",
            "Epoch [242/250] ---- Training Loss: 0.0137 ---- Validation Loss: 0.0122 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 45\n",
            "Epoch [243/250] ---- Training Loss: 0.0137 ---- Validation Loss: 0.0122 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 45\n",
            "Epoch [244/250] ---- Training Loss: 0.0137 ---- Validation Loss: 0.0122 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 45\n",
            "Epoch [245/250] ---- Training Loss: 0.0137 ---- Validation Loss: 0.0123 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 45\n",
            "Epoch [246/250] ---- Training Loss: 0.0137 ---- Validation Loss: 0.0122 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 45\n",
            "Epoch [247/250] ---- Training Loss: 0.0137 ---- Validation Loss: 0.0122 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 45\n",
            "Epoch [248/250] ---- Training Loss: 0.0137 ---- Validation Loss: 0.0122 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 45\n",
            "Epoch [249/250] ---- Training Loss: 0.0137 ---- Validation Loss: 0.0122 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 45\n",
            "Epoch [250/250] ---- Training Loss: 0.0137 ---- Validation Loss: 0.0122 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 45\n",
            "Saved model: best_fuel_consumption_model_batch45_l50.pth\n",
            "Epoch [1/250] ---- Training Loss: 0.0425 ---- Validation Loss: 0.0335 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 64\n",
            "Epoch [2/250] ---- Training Loss: 0.0339 ---- Validation Loss: 0.0304 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 64\n",
            "Epoch [3/250] ---- Training Loss: 0.0310 ---- Validation Loss: 0.0277 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 64\n",
            "Epoch [4/250] ---- Training Loss: 0.0281 ---- Validation Loss: 0.0248 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 64\n",
            "Epoch [5/250] ---- Training Loss: 0.0256 ---- Validation Loss: 0.0227 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 64\n",
            "Epoch [6/250] ---- Training Loss: 0.0237 ---- Validation Loss: 0.0212 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 64\n",
            "Epoch [7/250] ---- Training Loss: 0.0223 ---- Validation Loss: 0.0201 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 64\n",
            "Epoch [8/250] ---- Training Loss: 0.0213 ---- Validation Loss: 0.0194 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 64\n",
            "Epoch [9/250] ---- Training Loss: 0.0205 ---- Validation Loss: 0.0189 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 64\n",
            "Epoch [10/250] ---- Training Loss: 0.0199 ---- Validation Loss: 0.0184 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 64\n",
            "Epoch [11/250] ---- Training Loss: 0.0195 ---- Validation Loss: 0.0180 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 64\n",
            "Epoch [12/250] ---- Training Loss: 0.0192 ---- Validation Loss: 0.0179 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 64\n",
            "Epoch [13/250] ---- Training Loss: 0.0189 ---- Validation Loss: 0.0176 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 64\n",
            "Epoch [14/250] ---- Training Loss: 0.0187 ---- Validation Loss: 0.0173 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 64\n",
            "Epoch [15/250] ---- Training Loss: 0.0184 ---- Validation Loss: 0.0172 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 64\n",
            "Epoch [16/250] ---- Training Loss: 0.0183 ---- Validation Loss: 0.0170 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 64\n",
            "Epoch [17/250] ---- Training Loss: 0.0181 ---- Validation Loss: 0.0168 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 64\n",
            "Epoch [18/250] ---- Training Loss: 0.0180 ---- Validation Loss: 0.0168 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 64\n",
            "Epoch [19/250] ---- Training Loss: 0.0178 ---- Validation Loss: 0.0168 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 64\n",
            "Epoch [20/250] ---- Training Loss: 0.0177 ---- Validation Loss: 0.0166 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 64\n",
            "Epoch [21/250] ---- Training Loss: 0.0176 ---- Validation Loss: 0.0164 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 64\n",
            "Epoch [22/250] ---- Training Loss: 0.0175 ---- Validation Loss: 0.0164 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 64\n",
            "Epoch [23/250] ---- Training Loss: 0.0174 ---- Validation Loss: 0.0163 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 64\n",
            "Epoch [24/250] ---- Training Loss: 0.0173 ---- Validation Loss: 0.0162 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 64\n",
            "Epoch [25/250] ---- Training Loss: 0.0173 ---- Validation Loss: 0.0162 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 64\n",
            "Epoch [26/250] ---- Training Loss: 0.0172 ---- Validation Loss: 0.0160 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 64\n",
            "Epoch [27/250] ---- Training Loss: 0.0171 ---- Validation Loss: 0.0159 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 64\n",
            "Epoch [28/250] ---- Training Loss: 0.0170 ---- Validation Loss: 0.0159 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 64\n",
            "Epoch [29/250] ---- Training Loss: 0.0169 ---- Validation Loss: 0.0158 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 64\n",
            "Epoch [30/250] ---- Training Loss: 0.0169 ---- Validation Loss: 0.0157 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 64\n",
            "Epoch [31/250] ---- Training Loss: 0.0168 ---- Validation Loss: 0.0156 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 64\n",
            "Epoch [32/250] ---- Training Loss: 0.0167 ---- Validation Loss: 0.0155 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 64\n",
            "Epoch [33/250] ---- Training Loss: 0.0167 ---- Validation Loss: 0.0155 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 64\n",
            "Epoch [34/250] ---- Training Loss: 0.0166 ---- Validation Loss: 0.0154 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 64\n",
            "Epoch [35/250] ---- Training Loss: 0.0165 ---- Validation Loss: 0.0154 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 64\n",
            "Epoch [36/250] ---- Training Loss: 0.0165 ---- Validation Loss: 0.0154 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 64\n",
            "Epoch [37/250] ---- Training Loss: 0.0164 ---- Validation Loss: 0.0154 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 64\n",
            "Epoch [38/250] ---- Training Loss: 0.0164 ---- Validation Loss: 0.0152 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 64\n",
            "Epoch [39/250] ---- Training Loss: 0.0163 ---- Validation Loss: 0.0152 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 64\n",
            "Epoch [40/250] ---- Training Loss: 0.0163 ---- Validation Loss: 0.0151 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 64\n",
            "Epoch [41/250] ---- Training Loss: 0.0163 ---- Validation Loss: 0.0151 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 64\n",
            "Epoch [42/250] ---- Training Loss: 0.0162 ---- Validation Loss: 0.0150 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 64\n",
            "Epoch [43/250] ---- Training Loss: 0.0162 ---- Validation Loss: 0.0149 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 64\n",
            "Epoch [44/250] ---- Training Loss: 0.0161 ---- Validation Loss: 0.0149 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 64\n",
            "Epoch [45/250] ---- Training Loss: 0.0161 ---- Validation Loss: 0.0150 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 64\n",
            "Epoch [46/250] ---- Training Loss: 0.0161 ---- Validation Loss: 0.0148 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 64\n",
            "Epoch [47/250] ---- Training Loss: 0.0160 ---- Validation Loss: 0.0148 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 64\n",
            "Epoch [48/250] ---- Training Loss: 0.0160 ---- Validation Loss: 0.0148 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 64\n",
            "Epoch [49/250] ---- Training Loss: 0.0160 ---- Validation Loss: 0.0149 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 64\n",
            "Epoch [50/250] ---- Training Loss: 0.0159 ---- Validation Loss: 0.0148 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 64\n",
            "Epoch [51/250] ---- Training Loss: 0.0159 ---- Validation Loss: 0.0148 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [52/250] ---- Training Loss: 0.0158 ---- Validation Loss: 0.0147 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [53/250] ---- Training Loss: 0.0158 ---- Validation Loss: 0.0146 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [54/250] ---- Training Loss: 0.0158 ---- Validation Loss: 0.0146 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [55/250] ---- Training Loss: 0.0158 ---- Validation Loss: 0.0146 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [56/250] ---- Training Loss: 0.0158 ---- Validation Loss: 0.0146 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [57/250] ---- Training Loss: 0.0157 ---- Validation Loss: 0.0146 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [58/250] ---- Training Loss: 0.0157 ---- Validation Loss: 0.0145 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [59/250] ---- Training Loss: 0.0157 ---- Validation Loss: 0.0145 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [60/250] ---- Training Loss: 0.0157 ---- Validation Loss: 0.0146 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [61/250] ---- Training Loss: 0.0157 ---- Validation Loss: 0.0145 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [62/250] ---- Training Loss: 0.0157 ---- Validation Loss: 0.0144 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [63/250] ---- Training Loss: 0.0156 ---- Validation Loss: 0.0144 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [64/250] ---- Training Loss: 0.0156 ---- Validation Loss: 0.0144 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [65/250] ---- Training Loss: 0.0156 ---- Validation Loss: 0.0144 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [66/250] ---- Training Loss: 0.0156 ---- Validation Loss: 0.0144 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [67/250] ---- Training Loss: 0.0156 ---- Validation Loss: 0.0144 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [68/250] ---- Training Loss: 0.0155 ---- Validation Loss: 0.0143 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [69/250] ---- Training Loss: 0.0155 ---- Validation Loss: 0.0143 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [70/250] ---- Training Loss: 0.0155 ---- Validation Loss: 0.0143 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [71/250] ---- Training Loss: 0.0155 ---- Validation Loss: 0.0143 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [72/250] ---- Training Loss: 0.0155 ---- Validation Loss: 0.0142 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [73/250] ---- Training Loss: 0.0154 ---- Validation Loss: 0.0142 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [74/250] ---- Training Loss: 0.0154 ---- Validation Loss: 0.0142 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [75/250] ---- Training Loss: 0.0154 ---- Validation Loss: 0.0141 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [76/250] ---- Training Loss: 0.0154 ---- Validation Loss: 0.0141 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [77/250] ---- Training Loss: 0.0154 ---- Validation Loss: 0.0141 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [78/250] ---- Training Loss: 0.0153 ---- Validation Loss: 0.0142 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [79/250] ---- Training Loss: 0.0153 ---- Validation Loss: 0.0141 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [80/250] ---- Training Loss: 0.0153 ---- Validation Loss: 0.0141 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [81/250] ---- Training Loss: 0.0153 ---- Validation Loss: 0.0140 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [82/250] ---- Training Loss: 0.0153 ---- Validation Loss: 0.0140 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [83/250] ---- Training Loss: 0.0152 ---- Validation Loss: 0.0140 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [84/250] ---- Training Loss: 0.0152 ---- Validation Loss: 0.0140 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [85/250] ---- Training Loss: 0.0152 ---- Validation Loss: 0.0139 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [86/250] ---- Training Loss: 0.0152 ---- Validation Loss: 0.0140 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [87/250] ---- Training Loss: 0.0152 ---- Validation Loss: 0.0140 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [88/250] ---- Training Loss: 0.0151 ---- Validation Loss: 0.0139 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [89/250] ---- Training Loss: 0.0151 ---- Validation Loss: 0.0139 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [90/250] ---- Training Loss: 0.0151 ---- Validation Loss: 0.0139 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [91/250] ---- Training Loss: 0.0151 ---- Validation Loss: 0.0139 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [92/250] ---- Training Loss: 0.0151 ---- Validation Loss: 0.0138 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [93/250] ---- Training Loss: 0.0151 ---- Validation Loss: 0.0138 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [94/250] ---- Training Loss: 0.0150 ---- Validation Loss: 0.0138 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [95/250] ---- Training Loss: 0.0150 ---- Validation Loss: 0.0138 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [96/250] ---- Training Loss: 0.0150 ---- Validation Loss: 0.0137 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [97/250] ---- Training Loss: 0.0150 ---- Validation Loss: 0.0137 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [98/250] ---- Training Loss: 0.0150 ---- Validation Loss: 0.0137 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [99/250] ---- Training Loss: 0.0149 ---- Validation Loss: 0.0137 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [100/250] ---- Training Loss: 0.0149 ---- Validation Loss: 0.0137 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [101/250] ---- Training Loss: 0.0149 ---- Validation Loss: 0.0137 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [102/250] ---- Training Loss: 0.0149 ---- Validation Loss: 0.0137 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [103/250] ---- Training Loss: 0.0149 ---- Validation Loss: 0.0136 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [104/250] ---- Training Loss: 0.0149 ---- Validation Loss: 0.0136 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [105/250] ---- Training Loss: 0.0149 ---- Validation Loss: 0.0136 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [106/250] ---- Training Loss: 0.0148 ---- Validation Loss: 0.0136 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [107/250] ---- Training Loss: 0.0148 ---- Validation Loss: 0.0136 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [108/250] ---- Training Loss: 0.0148 ---- Validation Loss: 0.0135 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [109/250] ---- Training Loss: 0.0148 ---- Validation Loss: 0.0136 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [110/250] ---- Training Loss: 0.0148 ---- Validation Loss: 0.0136 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [111/250] ---- Training Loss: 0.0148 ---- Validation Loss: 0.0135 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [112/250] ---- Training Loss: 0.0148 ---- Validation Loss: 0.0135 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [113/250] ---- Training Loss: 0.0148 ---- Validation Loss: 0.0135 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [114/250] ---- Training Loss: 0.0148 ---- Validation Loss: 0.0135 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [115/250] ---- Training Loss: 0.0148 ---- Validation Loss: 0.0135 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [116/250] ---- Training Loss: 0.0147 ---- Validation Loss: 0.0135 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [117/250] ---- Training Loss: 0.0147 ---- Validation Loss: 0.0134 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [118/250] ---- Training Loss: 0.0147 ---- Validation Loss: 0.0134 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [119/250] ---- Training Loss: 0.0147 ---- Validation Loss: 0.0134 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [120/250] ---- Training Loss: 0.0147 ---- Validation Loss: 0.0134 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [121/250] ---- Training Loss: 0.0147 ---- Validation Loss: 0.0134 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [122/250] ---- Training Loss: 0.0147 ---- Validation Loss: 0.0134 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [123/250] ---- Training Loss: 0.0147 ---- Validation Loss: 0.0133 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [124/250] ---- Training Loss: 0.0146 ---- Validation Loss: 0.0133 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [125/250] ---- Training Loss: 0.0146 ---- Validation Loss: 0.0133 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [126/250] ---- Training Loss: 0.0146 ---- Validation Loss: 0.0133 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [127/250] ---- Training Loss: 0.0146 ---- Validation Loss: 0.0133 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [128/250] ---- Training Loss: 0.0146 ---- Validation Loss: 0.0133 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [129/250] ---- Training Loss: 0.0146 ---- Validation Loss: 0.0133 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [130/250] ---- Training Loss: 0.0146 ---- Validation Loss: 0.0133 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [131/250] ---- Training Loss: 0.0146 ---- Validation Loss: 0.0133 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [132/250] ---- Training Loss: 0.0145 ---- Validation Loss: 0.0132 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [133/250] ---- Training Loss: 0.0145 ---- Validation Loss: 0.0132 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [134/250] ---- Training Loss: 0.0145 ---- Validation Loss: 0.0132 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [135/250] ---- Training Loss: 0.0145 ---- Validation Loss: 0.0133 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [136/250] ---- Training Loss: 0.0145 ---- Validation Loss: 0.0132 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [137/250] ---- Training Loss: 0.0145 ---- Validation Loss: 0.0132 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [138/250] ---- Training Loss: 0.0145 ---- Validation Loss: 0.0132 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [139/250] ---- Training Loss: 0.0145 ---- Validation Loss: 0.0132 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [140/250] ---- Training Loss: 0.0145 ---- Validation Loss: 0.0132 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [141/250] ---- Training Loss: 0.0145 ---- Validation Loss: 0.0132 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [142/250] ---- Training Loss: 0.0145 ---- Validation Loss: 0.0132 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [143/250] ---- Training Loss: 0.0144 ---- Validation Loss: 0.0132 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [144/250] ---- Training Loss: 0.0144 ---- Validation Loss: 0.0132 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [145/250] ---- Training Loss: 0.0144 ---- Validation Loss: 0.0131 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [146/250] ---- Training Loss: 0.0144 ---- Validation Loss: 0.0131 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [147/250] ---- Training Loss: 0.0144 ---- Validation Loss: 0.0131 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [148/250] ---- Training Loss: 0.0144 ---- Validation Loss: 0.0131 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [149/250] ---- Training Loss: 0.0144 ---- Validation Loss: 0.0131 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [150/250] ---- Training Loss: 0.0144 ---- Validation Loss: 0.0131 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [151/250] ---- Training Loss: 0.0144 ---- Validation Loss: 0.0131 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [152/250] ---- Training Loss: 0.0144 ---- Validation Loss: 0.0131 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [153/250] ---- Training Loss: 0.0144 ---- Validation Loss: 0.0130 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [154/250] ---- Training Loss: 0.0144 ---- Validation Loss: 0.0131 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [155/250] ---- Training Loss: 0.0144 ---- Validation Loss: 0.0131 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [156/250] ---- Training Loss: 0.0144 ---- Validation Loss: 0.0130 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [157/250] ---- Training Loss: 0.0143 ---- Validation Loss: 0.0131 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [158/250] ---- Training Loss: 0.0143 ---- Validation Loss: 0.0130 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [159/250] ---- Training Loss: 0.0143 ---- Validation Loss: 0.0131 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [160/250] ---- Training Loss: 0.0143 ---- Validation Loss: 0.0130 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [161/250] ---- Training Loss: 0.0143 ---- Validation Loss: 0.0130 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [162/250] ---- Training Loss: 0.0143 ---- Validation Loss: 0.0130 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [163/250] ---- Training Loss: 0.0143 ---- Validation Loss: 0.0130 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [164/250] ---- Training Loss: 0.0143 ---- Validation Loss: 0.0130 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [165/250] ---- Training Loss: 0.0143 ---- Validation Loss: 0.0130 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [166/250] ---- Training Loss: 0.0143 ---- Validation Loss: 0.0130 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [167/250] ---- Training Loss: 0.0143 ---- Validation Loss: 0.0130 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [168/250] ---- Training Loss: 0.0143 ---- Validation Loss: 0.0130 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [169/250] ---- Training Loss: 0.0143 ---- Validation Loss: 0.0130 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [170/250] ---- Training Loss: 0.0143 ---- Validation Loss: 0.0130 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [171/250] ---- Training Loss: 0.0143 ---- Validation Loss: 0.0129 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [172/250] ---- Training Loss: 0.0143 ---- Validation Loss: 0.0130 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [173/250] ---- Training Loss: 0.0143 ---- Validation Loss: 0.0130 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [174/250] ---- Training Loss: 0.0143 ---- Validation Loss: 0.0130 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [175/250] ---- Training Loss: 0.0143 ---- Validation Loss: 0.0130 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [176/250] ---- Training Loss: 0.0143 ---- Validation Loss: 0.0130 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [177/250] ---- Training Loss: 0.0143 ---- Validation Loss: 0.0129 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [178/250] ---- Training Loss: 0.0143 ---- Validation Loss: 0.0130 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [179/250] ---- Training Loss: 0.0142 ---- Validation Loss: 0.0130 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [180/250] ---- Training Loss: 0.0142 ---- Validation Loss: 0.0129 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [181/250] ---- Training Loss: 0.0142 ---- Validation Loss: 0.0129 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [182/250] ---- Training Loss: 0.0142 ---- Validation Loss: 0.0129 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [183/250] ---- Training Loss: 0.0142 ---- Validation Loss: 0.0130 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [184/250] ---- Training Loss: 0.0142 ---- Validation Loss: 0.0130 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [185/250] ---- Training Loss: 0.0142 ---- Validation Loss: 0.0129 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [186/250] ---- Training Loss: 0.0142 ---- Validation Loss: 0.0129 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [187/250] ---- Training Loss: 0.0142 ---- Validation Loss: 0.0129 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [188/250] ---- Training Loss: 0.0142 ---- Validation Loss: 0.0129 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [189/250] ---- Training Loss: 0.0142 ---- Validation Loss: 0.0129 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [190/250] ---- Training Loss: 0.0142 ---- Validation Loss: 0.0129 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [191/250] ---- Training Loss: 0.0142 ---- Validation Loss: 0.0129 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [192/250] ---- Training Loss: 0.0142 ---- Validation Loss: 0.0129 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [193/250] ---- Training Loss: 0.0142 ---- Validation Loss: 0.0129 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [194/250] ---- Training Loss: 0.0142 ---- Validation Loss: 0.0129 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [195/250] ---- Training Loss: 0.0142 ---- Validation Loss: 0.0129 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [196/250] ---- Training Loss: 0.0142 ---- Validation Loss: 0.0129 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [197/250] ---- Training Loss: 0.0142 ---- Validation Loss: 0.0129 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [198/250] ---- Training Loss: 0.0142 ---- Validation Loss: 0.0129 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [199/250] ---- Training Loss: 0.0142 ---- Validation Loss: 0.0129 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [200/250] ---- Training Loss: 0.0142 ---- Validation Loss: 0.0128 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 64\n",
            "Epoch [201/250] ---- Training Loss: 0.0142 ---- Validation Loss: 0.0128 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 64\n",
            "Epoch [202/250] ---- Training Loss: 0.0142 ---- Validation Loss: 0.0129 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 64\n",
            "Epoch [203/250] ---- Training Loss: 0.0142 ---- Validation Loss: 0.0129 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 64\n",
            "Epoch [204/250] ---- Training Loss: 0.0142 ---- Validation Loss: 0.0128 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 64\n",
            "Epoch [205/250] ---- Training Loss: 0.0142 ---- Validation Loss: 0.0128 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 64\n",
            "Epoch [206/250] ---- Training Loss: 0.0142 ---- Validation Loss: 0.0128 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 64\n",
            "Epoch [207/250] ---- Training Loss: 0.0142 ---- Validation Loss: 0.0129 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 64\n",
            "Epoch [208/250] ---- Training Loss: 0.0142 ---- Validation Loss: 0.0128 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 64\n",
            "Epoch [209/250] ---- Training Loss: 0.0141 ---- Validation Loss: 0.0128 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 64\n",
            "Epoch [210/250] ---- Training Loss: 0.0142 ---- Validation Loss: 0.0128 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 64\n",
            "Epoch [211/250] ---- Training Loss: 0.0141 ---- Validation Loss: 0.0128 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 64\n",
            "Epoch [212/250] ---- Training Loss: 0.0141 ---- Validation Loss: 0.0129 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 64\n",
            "Epoch [213/250] ---- Training Loss: 0.0141 ---- Validation Loss: 0.0128 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 64\n",
            "Epoch [214/250] ---- Training Loss: 0.0141 ---- Validation Loss: 0.0128 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 64\n",
            "Epoch [215/250] ---- Training Loss: 0.0141 ---- Validation Loss: 0.0128 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 64\n",
            "Epoch [216/250] ---- Training Loss: 0.0141 ---- Validation Loss: 0.0128 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 64\n",
            "Epoch [217/250] ---- Training Loss: 0.0141 ---- Validation Loss: 0.0128 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 64\n",
            "Epoch [218/250] ---- Training Loss: 0.0141 ---- Validation Loss: 0.0128 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 64\n",
            "Epoch [219/250] ---- Training Loss: 0.0141 ---- Validation Loss: 0.0128 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 64\n",
            "Epoch [220/250] ---- Training Loss: 0.0141 ---- Validation Loss: 0.0128 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 64\n",
            "Epoch [221/250] ---- Training Loss: 0.0141 ---- Validation Loss: 0.0128 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 64\n",
            "Epoch [222/250] ---- Training Loss: 0.0141 ---- Validation Loss: 0.0128 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 64\n",
            "Epoch [223/250] ---- Training Loss: 0.0141 ---- Validation Loss: 0.0128 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 64\n",
            "Epoch [224/250] ---- Training Loss: 0.0141 ---- Validation Loss: 0.0128 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 64\n",
            "Epoch [225/250] ---- Training Loss: 0.0141 ---- Validation Loss: 0.0128 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 64\n",
            "Epoch [226/250] ---- Training Loss: 0.0141 ---- Validation Loss: 0.0128 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 64\n",
            "Epoch [227/250] ---- Training Loss: 0.0141 ---- Validation Loss: 0.0128 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 64\n",
            "Epoch [228/250] ---- Training Loss: 0.0141 ---- Validation Loss: 0.0128 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 64\n",
            "Epoch [229/250] ---- Training Loss: 0.0141 ---- Validation Loss: 0.0128 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 64\n",
            "Epoch [230/250] ---- Training Loss: 0.0141 ---- Validation Loss: 0.0128 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 64\n",
            "Epoch [231/250] ---- Training Loss: 0.0141 ---- Validation Loss: 0.0128 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 64\n",
            "Epoch [232/250] ---- Training Loss: 0.0141 ---- Validation Loss: 0.0128 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 64\n",
            "Epoch [233/250] ---- Training Loss: 0.0141 ---- Validation Loss: 0.0128 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 64\n",
            "Epoch [234/250] ---- Training Loss: 0.0141 ---- Validation Loss: 0.0128 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 64\n",
            "Epoch [235/250] ---- Training Loss: 0.0141 ---- Validation Loss: 0.0128 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 64\n",
            "Epoch [236/250] ---- Training Loss: 0.0141 ---- Validation Loss: 0.0128 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 64\n",
            "Epoch [237/250] ---- Training Loss: 0.0141 ---- Validation Loss: 0.0128 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 64\n",
            "Epoch [238/250] ---- Training Loss: 0.0141 ---- Validation Loss: 0.0128 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 64\n",
            "Epoch [239/250] ---- Training Loss: 0.0141 ---- Validation Loss: 0.0128 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 64\n",
            "Epoch [240/250] ---- Training Loss: 0.0141 ---- Validation Loss: 0.0128 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 64\n",
            "Epoch [241/250] ---- Training Loss: 0.0141 ---- Validation Loss: 0.0128 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 64\n",
            "Epoch [242/250] ---- Training Loss: 0.0141 ---- Validation Loss: 0.0128 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 64\n",
            "Epoch [243/250] ---- Training Loss: 0.0141 ---- Validation Loss: 0.0128 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 64\n",
            "Epoch [244/250] ---- Training Loss: 0.0141 ---- Validation Loss: 0.0128 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 64\n",
            "Epoch [245/250] ---- Training Loss: 0.0141 ---- Validation Loss: 0.0128 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 64\n",
            "Epoch [246/250] ---- Training Loss: 0.0141 ---- Validation Loss: 0.0128 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 64\n",
            "Epoch [247/250] ---- Training Loss: 0.0141 ---- Validation Loss: 0.0128 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 64\n",
            "Epoch [248/250] ---- Training Loss: 0.0141 ---- Validation Loss: 0.0127 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 64\n",
            "Epoch [249/250] ---- Training Loss: 0.0141 ---- Validation Loss: 0.0128 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 64\n",
            "Epoch [250/250] ---- Training Loss: 0.0141 ---- Validation Loss: 0.0128 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 64\n",
            "Saved model: best_fuel_consumption_model_batch64_l50.pth\n",
            "Epoch [1/250] ---- Training Loss: 0.0379 ---- Validation Loss: 0.0333 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 96\n",
            "Epoch [2/250] ---- Training Loss: 0.0326 ---- Validation Loss: 0.0310 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 96\n",
            "Epoch [3/250] ---- Training Loss: 0.0305 ---- Validation Loss: 0.0285 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 96\n",
            "Epoch [4/250] ---- Training Loss: 0.0282 ---- Validation Loss: 0.0259 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 96\n",
            "Epoch [5/250] ---- Training Loss: 0.0260 ---- Validation Loss: 0.0238 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 96\n",
            "Epoch [6/250] ---- Training Loss: 0.0242 ---- Validation Loss: 0.0221 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 96\n",
            "Epoch [7/250] ---- Training Loss: 0.0227 ---- Validation Loss: 0.0208 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 96\n",
            "Epoch [8/250] ---- Training Loss: 0.0215 ---- Validation Loss: 0.0197 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 96\n",
            "Epoch [9/250] ---- Training Loss: 0.0207 ---- Validation Loss: 0.0191 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 96\n",
            "Epoch [10/250] ---- Training Loss: 0.0201 ---- Validation Loss: 0.0185 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 96\n",
            "Epoch [11/250] ---- Training Loss: 0.0196 ---- Validation Loss: 0.0181 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 96\n",
            "Epoch [12/250] ---- Training Loss: 0.0192 ---- Validation Loss: 0.0178 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 96\n",
            "Epoch [13/250] ---- Training Loss: 0.0189 ---- Validation Loss: 0.0175 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 96\n",
            "Epoch [14/250] ---- Training Loss: 0.0186 ---- Validation Loss: 0.0173 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 96\n",
            "Epoch [15/250] ---- Training Loss: 0.0184 ---- Validation Loss: 0.0171 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 96\n",
            "Epoch [16/250] ---- Training Loss: 0.0182 ---- Validation Loss: 0.0170 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 96\n",
            "Epoch [17/250] ---- Training Loss: 0.0181 ---- Validation Loss: 0.0168 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 96\n",
            "Epoch [18/250] ---- Training Loss: 0.0180 ---- Validation Loss: 0.0167 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 96\n",
            "Epoch [19/250] ---- Training Loss: 0.0179 ---- Validation Loss: 0.0167 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 96\n",
            "Epoch [20/250] ---- Training Loss: 0.0178 ---- Validation Loss: 0.0166 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 96\n",
            "Epoch [21/250] ---- Training Loss: 0.0177 ---- Validation Loss: 0.0165 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 96\n",
            "Epoch [22/250] ---- Training Loss: 0.0176 ---- Validation Loss: 0.0165 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 96\n",
            "Epoch [23/250] ---- Training Loss: 0.0176 ---- Validation Loss: 0.0164 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 96\n",
            "Epoch [24/250] ---- Training Loss: 0.0175 ---- Validation Loss: 0.0163 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 96\n",
            "Epoch [25/250] ---- Training Loss: 0.0175 ---- Validation Loss: 0.0163 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 96\n",
            "Epoch [26/250] ---- Training Loss: 0.0174 ---- Validation Loss: 0.0163 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 96\n",
            "Epoch [27/250] ---- Training Loss: 0.0174 ---- Validation Loss: 0.0163 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 96\n",
            "Epoch [28/250] ---- Training Loss: 0.0173 ---- Validation Loss: 0.0162 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 96\n",
            "Epoch [29/250] ---- Training Loss: 0.0173 ---- Validation Loss: 0.0162 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 96\n",
            "Epoch [30/250] ---- Training Loss: 0.0172 ---- Validation Loss: 0.0162 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 96\n",
            "Epoch [31/250] ---- Training Loss: 0.0172 ---- Validation Loss: 0.0161 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 96\n",
            "Epoch [32/250] ---- Training Loss: 0.0172 ---- Validation Loss: 0.0160 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 96\n",
            "Epoch [33/250] ---- Training Loss: 0.0171 ---- Validation Loss: 0.0160 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 96\n",
            "Epoch [34/250] ---- Training Loss: 0.0171 ---- Validation Loss: 0.0162 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 96\n",
            "Epoch [35/250] ---- Training Loss: 0.0170 ---- Validation Loss: 0.0159 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 96\n",
            "Epoch [36/250] ---- Training Loss: 0.0170 ---- Validation Loss: 0.0159 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 96\n",
            "Epoch [37/250] ---- Training Loss: 0.0170 ---- Validation Loss: 0.0158 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 96\n",
            "Epoch [38/250] ---- Training Loss: 0.0169 ---- Validation Loss: 0.0158 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 96\n",
            "Epoch [39/250] ---- Training Loss: 0.0169 ---- Validation Loss: 0.0157 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 96\n",
            "Epoch [40/250] ---- Training Loss: 0.0169 ---- Validation Loss: 0.0159 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 96\n",
            "Epoch [41/250] ---- Training Loss: 0.0168 ---- Validation Loss: 0.0157 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 96\n",
            "Epoch [42/250] ---- Training Loss: 0.0168 ---- Validation Loss: 0.0156 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 96\n",
            "Epoch [43/250] ---- Training Loss: 0.0168 ---- Validation Loss: 0.0156 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 96\n",
            "Epoch [44/250] ---- Training Loss: 0.0167 ---- Validation Loss: 0.0156 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 96\n",
            "Epoch [45/250] ---- Training Loss: 0.0167 ---- Validation Loss: 0.0155 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 96\n",
            "Epoch [46/250] ---- Training Loss: 0.0167 ---- Validation Loss: 0.0157 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 96\n",
            "Epoch [47/250] ---- Training Loss: 0.0166 ---- Validation Loss: 0.0155 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 96\n",
            "Epoch [48/250] ---- Training Loss: 0.0166 ---- Validation Loss: 0.0154 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 96\n",
            "Epoch [49/250] ---- Training Loss: 0.0165 ---- Validation Loss: 0.0154 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 96\n",
            "Epoch [50/250] ---- Training Loss: 0.0165 ---- Validation Loss: 0.0153 ---- Learning Rate: 1.00e-04    Init LR: 1e-04  batch: 96\n",
            "Epoch [51/250] ---- Training Loss: 0.0165 ---- Validation Loss: 0.0153 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [52/250] ---- Training Loss: 0.0165 ---- Validation Loss: 0.0153 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [53/250] ---- Training Loss: 0.0164 ---- Validation Loss: 0.0152 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [54/250] ---- Training Loss: 0.0164 ---- Validation Loss: 0.0152 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [55/250] ---- Training Loss: 0.0164 ---- Validation Loss: 0.0152 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [56/250] ---- Training Loss: 0.0164 ---- Validation Loss: 0.0152 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [57/250] ---- Training Loss: 0.0164 ---- Validation Loss: 0.0152 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [58/250] ---- Training Loss: 0.0163 ---- Validation Loss: 0.0151 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [59/250] ---- Training Loss: 0.0163 ---- Validation Loss: 0.0151 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [60/250] ---- Training Loss: 0.0163 ---- Validation Loss: 0.0151 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [61/250] ---- Training Loss: 0.0163 ---- Validation Loss: 0.0151 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [62/250] ---- Training Loss: 0.0163 ---- Validation Loss: 0.0150 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [63/250] ---- Training Loss: 0.0163 ---- Validation Loss: 0.0150 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [64/250] ---- Training Loss: 0.0163 ---- Validation Loss: 0.0150 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [65/250] ---- Training Loss: 0.0162 ---- Validation Loss: 0.0150 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [66/250] ---- Training Loss: 0.0162 ---- Validation Loss: 0.0150 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [67/250] ---- Training Loss: 0.0162 ---- Validation Loss: 0.0150 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [68/250] ---- Training Loss: 0.0162 ---- Validation Loss: 0.0149 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [69/250] ---- Training Loss: 0.0162 ---- Validation Loss: 0.0150 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [70/250] ---- Training Loss: 0.0162 ---- Validation Loss: 0.0149 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [71/250] ---- Training Loss: 0.0161 ---- Validation Loss: 0.0149 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [72/250] ---- Training Loss: 0.0161 ---- Validation Loss: 0.0149 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [73/250] ---- Training Loss: 0.0161 ---- Validation Loss: 0.0151 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [74/250] ---- Training Loss: 0.0161 ---- Validation Loss: 0.0149 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [75/250] ---- Training Loss: 0.0161 ---- Validation Loss: 0.0148 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [76/250] ---- Training Loss: 0.0161 ---- Validation Loss: 0.0149 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [77/250] ---- Training Loss: 0.0161 ---- Validation Loss: 0.0148 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [78/250] ---- Training Loss: 0.0161 ---- Validation Loss: 0.0148 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [79/250] ---- Training Loss: 0.0161 ---- Validation Loss: 0.0148 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [80/250] ---- Training Loss: 0.0160 ---- Validation Loss: 0.0148 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [81/250] ---- Training Loss: 0.0160 ---- Validation Loss: 0.0148 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [82/250] ---- Training Loss: 0.0160 ---- Validation Loss: 0.0148 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [83/250] ---- Training Loss: 0.0160 ---- Validation Loss: 0.0147 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [84/250] ---- Training Loss: 0.0160 ---- Validation Loss: 0.0147 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [85/250] ---- Training Loss: 0.0160 ---- Validation Loss: 0.0147 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [86/250] ---- Training Loss: 0.0160 ---- Validation Loss: 0.0147 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [87/250] ---- Training Loss: 0.0159 ---- Validation Loss: 0.0147 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [88/250] ---- Training Loss: 0.0160 ---- Validation Loss: 0.0147 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [89/250] ---- Training Loss: 0.0159 ---- Validation Loss: 0.0147 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [90/250] ---- Training Loss: 0.0159 ---- Validation Loss: 0.0147 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [91/250] ---- Training Loss: 0.0159 ---- Validation Loss: 0.0146 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [92/250] ---- Training Loss: 0.0159 ---- Validation Loss: 0.0146 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [93/250] ---- Training Loss: 0.0159 ---- Validation Loss: 0.0149 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [94/250] ---- Training Loss: 0.0159 ---- Validation Loss: 0.0147 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [95/250] ---- Training Loss: 0.0159 ---- Validation Loss: 0.0146 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [96/250] ---- Training Loss: 0.0158 ---- Validation Loss: 0.0146 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [97/250] ---- Training Loss: 0.0158 ---- Validation Loss: 0.0146 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [98/250] ---- Training Loss: 0.0158 ---- Validation Loss: 0.0146 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [99/250] ---- Training Loss: 0.0158 ---- Validation Loss: 0.0145 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [100/250] ---- Training Loss: 0.0158 ---- Validation Loss: 0.0146 ---- Learning Rate: 5.00e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [101/250] ---- Training Loss: 0.0158 ---- Validation Loss: 0.0145 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [102/250] ---- Training Loss: 0.0158 ---- Validation Loss: 0.0145 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [103/250] ---- Training Loss: 0.0158 ---- Validation Loss: 0.0145 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [104/250] ---- Training Loss: 0.0158 ---- Validation Loss: 0.0145 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [105/250] ---- Training Loss: 0.0158 ---- Validation Loss: 0.0145 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [106/250] ---- Training Loss: 0.0158 ---- Validation Loss: 0.0145 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [107/250] ---- Training Loss: 0.0158 ---- Validation Loss: 0.0145 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [108/250] ---- Training Loss: 0.0157 ---- Validation Loss: 0.0145 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [109/250] ---- Training Loss: 0.0157 ---- Validation Loss: 0.0145 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [110/250] ---- Training Loss: 0.0157 ---- Validation Loss: 0.0145 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [111/250] ---- Training Loss: 0.0157 ---- Validation Loss: 0.0145 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [112/250] ---- Training Loss: 0.0157 ---- Validation Loss: 0.0144 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [113/250] ---- Training Loss: 0.0157 ---- Validation Loss: 0.0145 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [114/250] ---- Training Loss: 0.0157 ---- Validation Loss: 0.0144 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [115/250] ---- Training Loss: 0.0157 ---- Validation Loss: 0.0144 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [116/250] ---- Training Loss: 0.0157 ---- Validation Loss: 0.0144 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [117/250] ---- Training Loss: 0.0157 ---- Validation Loss: 0.0144 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [118/250] ---- Training Loss: 0.0157 ---- Validation Loss: 0.0144 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [119/250] ---- Training Loss: 0.0157 ---- Validation Loss: 0.0144 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [120/250] ---- Training Loss: 0.0157 ---- Validation Loss: 0.0145 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [121/250] ---- Training Loss: 0.0157 ---- Validation Loss: 0.0144 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [122/250] ---- Training Loss: 0.0157 ---- Validation Loss: 0.0144 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [123/250] ---- Training Loss: 0.0157 ---- Validation Loss: 0.0144 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [124/250] ---- Training Loss: 0.0157 ---- Validation Loss: 0.0144 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [125/250] ---- Training Loss: 0.0157 ---- Validation Loss: 0.0144 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [126/250] ---- Training Loss: 0.0156 ---- Validation Loss: 0.0144 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [127/250] ---- Training Loss: 0.0156 ---- Validation Loss: 0.0144 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [128/250] ---- Training Loss: 0.0156 ---- Validation Loss: 0.0144 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [129/250] ---- Training Loss: 0.0156 ---- Validation Loss: 0.0143 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [130/250] ---- Training Loss: 0.0156 ---- Validation Loss: 0.0143 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [131/250] ---- Training Loss: 0.0156 ---- Validation Loss: 0.0143 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [132/250] ---- Training Loss: 0.0156 ---- Validation Loss: 0.0144 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [133/250] ---- Training Loss: 0.0156 ---- Validation Loss: 0.0143 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [134/250] ---- Training Loss: 0.0156 ---- Validation Loss: 0.0143 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [135/250] ---- Training Loss: 0.0156 ---- Validation Loss: 0.0143 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [136/250] ---- Training Loss: 0.0156 ---- Validation Loss: 0.0143 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [137/250] ---- Training Loss: 0.0156 ---- Validation Loss: 0.0143 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [138/250] ---- Training Loss: 0.0156 ---- Validation Loss: 0.0143 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [139/250] ---- Training Loss: 0.0156 ---- Validation Loss: 0.0143 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [140/250] ---- Training Loss: 0.0156 ---- Validation Loss: 0.0143 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [141/250] ---- Training Loss: 0.0156 ---- Validation Loss: 0.0143 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [142/250] ---- Training Loss: 0.0155 ---- Validation Loss: 0.0143 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [143/250] ---- Training Loss: 0.0156 ---- Validation Loss: 0.0143 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [144/250] ---- Training Loss: 0.0155 ---- Validation Loss: 0.0142 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [145/250] ---- Training Loss: 0.0155 ---- Validation Loss: 0.0142 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [146/250] ---- Training Loss: 0.0155 ---- Validation Loss: 0.0143 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [147/250] ---- Training Loss: 0.0155 ---- Validation Loss: 0.0142 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [148/250] ---- Training Loss: 0.0155 ---- Validation Loss: 0.0142 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [149/250] ---- Training Loss: 0.0155 ---- Validation Loss: 0.0142 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [150/250] ---- Training Loss: 0.0155 ---- Validation Loss: 0.0142 ---- Learning Rate: 2.50e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [151/250] ---- Training Loss: 0.0155 ---- Validation Loss: 0.0142 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [152/250] ---- Training Loss: 0.0155 ---- Validation Loss: 0.0142 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [153/250] ---- Training Loss: 0.0155 ---- Validation Loss: 0.0142 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [154/250] ---- Training Loss: 0.0155 ---- Validation Loss: 0.0142 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [155/250] ---- Training Loss: 0.0155 ---- Validation Loss: 0.0142 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [156/250] ---- Training Loss: 0.0155 ---- Validation Loss: 0.0142 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [157/250] ---- Training Loss: 0.0155 ---- Validation Loss: 0.0142 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [158/250] ---- Training Loss: 0.0155 ---- Validation Loss: 0.0142 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [159/250] ---- Training Loss: 0.0155 ---- Validation Loss: 0.0142 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [160/250] ---- Training Loss: 0.0155 ---- Validation Loss: 0.0142 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [161/250] ---- Training Loss: 0.0155 ---- Validation Loss: 0.0142 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [162/250] ---- Training Loss: 0.0154 ---- Validation Loss: 0.0142 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [163/250] ---- Training Loss: 0.0154 ---- Validation Loss: 0.0142 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [164/250] ---- Training Loss: 0.0154 ---- Validation Loss: 0.0142 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [165/250] ---- Training Loss: 0.0154 ---- Validation Loss: 0.0141 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [166/250] ---- Training Loss: 0.0154 ---- Validation Loss: 0.0141 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [167/250] ---- Training Loss: 0.0154 ---- Validation Loss: 0.0141 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [168/250] ---- Training Loss: 0.0154 ---- Validation Loss: 0.0141 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [169/250] ---- Training Loss: 0.0154 ---- Validation Loss: 0.0141 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [170/250] ---- Training Loss: 0.0154 ---- Validation Loss: 0.0142 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [171/250] ---- Training Loss: 0.0154 ---- Validation Loss: 0.0141 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [172/250] ---- Training Loss: 0.0154 ---- Validation Loss: 0.0141 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [173/250] ---- Training Loss: 0.0154 ---- Validation Loss: 0.0141 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [174/250] ---- Training Loss: 0.0154 ---- Validation Loss: 0.0141 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [175/250] ---- Training Loss: 0.0154 ---- Validation Loss: 0.0141 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [176/250] ---- Training Loss: 0.0154 ---- Validation Loss: 0.0141 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [177/250] ---- Training Loss: 0.0154 ---- Validation Loss: 0.0141 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [178/250] ---- Training Loss: 0.0154 ---- Validation Loss: 0.0141 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [179/250] ---- Training Loss: 0.0154 ---- Validation Loss: 0.0141 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [180/250] ---- Training Loss: 0.0154 ---- Validation Loss: 0.0141 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [181/250] ---- Training Loss: 0.0154 ---- Validation Loss: 0.0141 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [182/250] ---- Training Loss: 0.0154 ---- Validation Loss: 0.0141 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [183/250] ---- Training Loss: 0.0154 ---- Validation Loss: 0.0141 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [184/250] ---- Training Loss: 0.0154 ---- Validation Loss: 0.0141 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [185/250] ---- Training Loss: 0.0154 ---- Validation Loss: 0.0141 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [186/250] ---- Training Loss: 0.0154 ---- Validation Loss: 0.0141 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [187/250] ---- Training Loss: 0.0154 ---- Validation Loss: 0.0141 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [188/250] ---- Training Loss: 0.0154 ---- Validation Loss: 0.0141 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [189/250] ---- Training Loss: 0.0154 ---- Validation Loss: 0.0141 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [190/250] ---- Training Loss: 0.0154 ---- Validation Loss: 0.0141 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [191/250] ---- Training Loss: 0.0154 ---- Validation Loss: 0.0141 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [192/250] ---- Training Loss: 0.0154 ---- Validation Loss: 0.0141 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [193/250] ---- Training Loss: 0.0154 ---- Validation Loss: 0.0141 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [194/250] ---- Training Loss: 0.0153 ---- Validation Loss: 0.0140 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [195/250] ---- Training Loss: 0.0153 ---- Validation Loss: 0.0140 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [196/250] ---- Training Loss: 0.0153 ---- Validation Loss: 0.0140 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [197/250] ---- Training Loss: 0.0153 ---- Validation Loss: 0.0140 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [198/250] ---- Training Loss: 0.0153 ---- Validation Loss: 0.0140 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [199/250] ---- Training Loss: 0.0153 ---- Validation Loss: 0.0140 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [200/250] ---- Training Loss: 0.0153 ---- Validation Loss: 0.0140 ---- Learning Rate: 1.25e-05    Init LR: 1e-04  batch: 96\n",
            "Epoch [201/250] ---- Training Loss: 0.0153 ---- Validation Loss: 0.0140 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 96\n",
            "Epoch [202/250] ---- Training Loss: 0.0153 ---- Validation Loss: 0.0140 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 96\n",
            "Epoch [203/250] ---- Training Loss: 0.0153 ---- Validation Loss: 0.0140 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 96\n",
            "Epoch [204/250] ---- Training Loss: 0.0153 ---- Validation Loss: 0.0140 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 96\n",
            "Epoch [205/250] ---- Training Loss: 0.0153 ---- Validation Loss: 0.0140 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 96\n",
            "Epoch [206/250] ---- Training Loss: 0.0153 ---- Validation Loss: 0.0140 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 96\n",
            "Epoch [207/250] ---- Training Loss: 0.0153 ---- Validation Loss: 0.0140 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 96\n",
            "Epoch [208/250] ---- Training Loss: 0.0153 ---- Validation Loss: 0.0140 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 96\n",
            "Epoch [209/250] ---- Training Loss: 0.0153 ---- Validation Loss: 0.0140 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 96\n",
            "Epoch [210/250] ---- Training Loss: 0.0153 ---- Validation Loss: 0.0140 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 96\n",
            "Epoch [211/250] ---- Training Loss: 0.0153 ---- Validation Loss: 0.0140 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 96\n",
            "Epoch [212/250] ---- Training Loss: 0.0153 ---- Validation Loss: 0.0140 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 96\n",
            "Epoch [213/250] ---- Training Loss: 0.0153 ---- Validation Loss: 0.0140 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 96\n",
            "Epoch [214/250] ---- Training Loss: 0.0153 ---- Validation Loss: 0.0140 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 96\n",
            "Epoch [215/250] ---- Training Loss: 0.0153 ---- Validation Loss: 0.0140 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 96\n",
            "Epoch [216/250] ---- Training Loss: 0.0153 ---- Validation Loss: 0.0140 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 96\n",
            "Epoch [217/250] ---- Training Loss: 0.0153 ---- Validation Loss: 0.0140 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 96\n",
            "Epoch [218/250] ---- Training Loss: 0.0153 ---- Validation Loss: 0.0140 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 96\n",
            "Epoch [219/250] ---- Training Loss: 0.0153 ---- Validation Loss: 0.0140 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 96\n",
            "Epoch [220/250] ---- Training Loss: 0.0153 ---- Validation Loss: 0.0140 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 96\n",
            "Epoch [221/250] ---- Training Loss: 0.0153 ---- Validation Loss: 0.0140 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 96\n",
            "Epoch [222/250] ---- Training Loss: 0.0153 ---- Validation Loss: 0.0140 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 96\n",
            "Epoch [223/250] ---- Training Loss: 0.0153 ---- Validation Loss: 0.0140 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 96\n",
            "Epoch [224/250] ---- Training Loss: 0.0153 ---- Validation Loss: 0.0140 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 96\n",
            "Epoch [225/250] ---- Training Loss: 0.0153 ---- Validation Loss: 0.0140 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 96\n",
            "Epoch [226/250] ---- Training Loss: 0.0153 ---- Validation Loss: 0.0140 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 96\n",
            "Epoch [227/250] ---- Training Loss: 0.0153 ---- Validation Loss: 0.0140 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 96\n",
            "Epoch [228/250] ---- Training Loss: 0.0153 ---- Validation Loss: 0.0140 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 96\n",
            "Epoch [229/250] ---- Training Loss: 0.0153 ---- Validation Loss: 0.0140 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 96\n",
            "Epoch [230/250] ---- Training Loss: 0.0153 ---- Validation Loss: 0.0139 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 96\n",
            "Epoch [231/250] ---- Training Loss: 0.0153 ---- Validation Loss: 0.0139 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 96\n",
            "Epoch [232/250] ---- Training Loss: 0.0153 ---- Validation Loss: 0.0139 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 96\n",
            "Epoch [233/250] ---- Training Loss: 0.0153 ---- Validation Loss: 0.0140 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 96\n",
            "Epoch [234/250] ---- Training Loss: 0.0153 ---- Validation Loss: 0.0140 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 96\n",
            "Epoch [235/250] ---- Training Loss: 0.0153 ---- Validation Loss: 0.0139 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 96\n",
            "Epoch [236/250] ---- Training Loss: 0.0153 ---- Validation Loss: 0.0139 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 96\n",
            "Epoch [237/250] ---- Training Loss: 0.0153 ---- Validation Loss: 0.0139 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 96\n",
            "Epoch [238/250] ---- Training Loss: 0.0153 ---- Validation Loss: 0.0139 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 96\n",
            "Epoch [239/250] ---- Training Loss: 0.0153 ---- Validation Loss: 0.0139 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 96\n",
            "Epoch [240/250] ---- Training Loss: 0.0153 ---- Validation Loss: 0.0140 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 96\n",
            "Epoch [241/250] ---- Training Loss: 0.0152 ---- Validation Loss: 0.0139 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 96\n",
            "Epoch [242/250] ---- Training Loss: 0.0152 ---- Validation Loss: 0.0139 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 96\n",
            "Epoch [243/250] ---- Training Loss: 0.0152 ---- Validation Loss: 0.0139 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 96\n",
            "Epoch [244/250] ---- Training Loss: 0.0152 ---- Validation Loss: 0.0139 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 96\n",
            "Epoch [245/250] ---- Training Loss: 0.0152 ---- Validation Loss: 0.0139 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 96\n",
            "Epoch [246/250] ---- Training Loss: 0.0152 ---- Validation Loss: 0.0139 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 96\n",
            "Epoch [247/250] ---- Training Loss: 0.0152 ---- Validation Loss: 0.0139 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 96\n",
            "Epoch [248/250] ---- Training Loss: 0.0152 ---- Validation Loss: 0.0139 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 96\n",
            "Epoch [249/250] ---- Training Loss: 0.0152 ---- Validation Loss: 0.0139 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 96\n",
            "Epoch [250/250] ---- Training Loss: 0.0152 ---- Validation Loss: 0.0139 ---- Learning Rate: 6.25e-06    Init LR: 1e-04  batch: 96\n",
            "Saved model: best_fuel_consumption_model_batch96_l50.pth\n",
            "Best Validation Loss: 0.0122\n",
            "Best Initial Learning Rate: 0.0001\n",
            "Best Weight Decay: 1e-05\n",
            "Best model saved as 'best_fuel_consumption_model_overall.pth'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "QgiHLL31mUEE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "yg9vSugumUBg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "7o2xWWKvmT67"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#test"
      ],
      "metadata": {
        "id": "X7RXfsU_mNR8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "# Constants\n",
        "SEQUENCE_LENGTH = 600\n",
        "PLOT_SAVE_DIR = 'predicted_vs_actual_plots'\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Define the PyTorch model structure (same as the one used for training)\n",
        "class FuelConsumptionModel(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(FuelConsumptionModel, self).__init__()\n",
        "        self.lstm1 = nn.LSTM(input_size, 32, batch_first=True, bidirectional=True)\n",
        "        self.dropout1 = nn.Dropout(0.2)\n",
        "        self.lstm2 = nn.LSTM(64, 32, batch_first=True, bidirectional=True)\n",
        "        self.dropout2 = nn.Dropout(0.2)\n",
        "        self.dense = nn.Linear(64, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x, _ = self.lstm1(x)\n",
        "        x = self.dropout1(x)\n",
        "        x, _ = self.lstm2(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.dense(x)\n",
        "        return x\n",
        "\n",
        "# Load the trained model\n",
        "def load_trained_model(model_path, input_size):\n",
        "    model = FuelConsumptionModel(input_size=input_size)\n",
        "    model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n",
        "    model.to(DEVICE)\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "# Process the file and prepare segments\n",
        "def process_file(file_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "    df['Time'] = df['Time'] - df['Time'].iloc[0]\n",
        "    df['Trip fuel consumption'] = df['Trip fuel consumption'] - df['Trip fuel consumption'].iloc[0]\n",
        "    df['Acceleration'] = df['Speed'].diff().fillna(0)\n",
        "    features = df[['Engine speed', 'Speed', 'slope', 'Acceleration']]\n",
        "    df['Momentary fuel consumption'] = df['Trip fuel consumption'].diff().fillna(0)\n",
        "    target = df['Momentary fuel consumption']\n",
        "    return features, target\n",
        "\n",
        "# Pad and normalize the data\n",
        "def pad_and_normalize(data, sequence_length=SEQUENCE_LENGTH):\n",
        "    padded_data = np.zeros((len(data), sequence_length, data[0].shape[1]))\n",
        "    for i, seq in enumerate(data):\n",
        "        length = min(len(seq), sequence_length)\n",
        "        padded_data[i, :length] = seq[:length]\n",
        "\n",
        "    # Normalization (same as in your script)\n",
        "    min_val_x = [0, 0, -10, -10]\n",
        "    max_val_x = [8000, 150, 10, 10]\n",
        "    for i in range(padded_data.shape[-1]):\n",
        "        padded_data[:, :, i] = (padded_data[:, :, i] - min_val_x[i]) / (max_val_x[i] - min_val_x[i])\n",
        "\n",
        "    return torch.tensor(padded_data, dtype=torch.float32).to(DEVICE)\n",
        "\n",
        "# Predict and plot the results\n",
        "def plot_predicted_vs_real(input_file, model):\n",
        "    features, actual_values = process_file(input_file)\n",
        "    num_segments = len(features) // SEQUENCE_LENGTH\n",
        "    predictions = []\n",
        "\n",
        "    for i in range(num_segments):\n",
        "        segment = features.iloc[i * SEQUENCE_LENGTH:(i + 1) * SEQUENCE_LENGTH]\n",
        "        segment_normalized = pad_and_normalize([segment.values])\n",
        "        with torch.no_grad():\n",
        "            segment_predictions = model(segment_normalized).cpu().numpy()\n",
        "        predictions.extend(segment_predictions.flatten() * 20000)\n",
        "\n",
        "    # Handle any remaining data\n",
        "    remainder = len(features) % SEQUENCE_LENGTH\n",
        "    if remainder != 0:\n",
        "        last_segment = features.iloc[-remainder:]\n",
        "        last_segment_normalized = pad_and_normalize([last_segment.values])\n",
        "        with torch.no_grad():\n",
        "            last_segment_predictions = model(last_segment_normalized).cpu().numpy()\n",
        "        predictions.extend(last_segment_predictions.flatten() * 20000)\n",
        "\n",
        "    predictions = np.array(predictions)\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(np.cumsum(actual_values.values[:len(predictions)], axis=0), label='Real', color='blue')\n",
        "    plt.plot(np.cumsum(predictions[:len(actual_values)], axis=0), label='Predicted', color='red')\n",
        "    plt.xlabel('Index')\n",
        "    plt.ylabel('Fuel Consumption')\n",
        "    plt.title('Predicted vs Real Fuel Consumption')\n",
        "    plt.legend()\n",
        "\n",
        "    directory, filename = os.path.split(input_file)\n",
        "    plot_filename = os.path.join(directory, f'{os.path.splitext(filename)[0]}_predicted_vs_real.png')\n",
        "    plt.savefig(plot_filename)\n",
        "    plt.close()\n",
        "\n",
        "    print(f\"Plot saved as: {plot_filename}\")\n",
        "\n",
        "    # Save predictions and actual values to CSV\n",
        "    results_df = pd.DataFrame({\n",
        "        'Speed': features[\"Speed\"].iloc[:len(predictions)],\n",
        "        'Actual': np.cumsum(actual_values.values[:len(predictions)], axis=0),\n",
        "        'Predicted': np.cumsum(predictions[:len(actual_values)], axis=0)\n",
        "    })\n",
        "\n",
        "    csv_filename = os.path.join(directory, f'{os.path.splitext(filename)[0]}_predicted_vs_real.csv')\n",
        "    results_df.to_csv(csv_filename, index=False)\n",
        "\n",
        "# Paths to model and input file\n",
        "model_path = '/content/best_fuel_consumption_model.pth'  # Adjust this to your PyTorch model path\n",
        "input_file_path = '/content/NEDC_1000_slope_added.csv'\n",
        "\n",
        "# Load model and predict\n",
        "input_size = 4  # Number of features in the input data\n",
        "model = load_trained_model(model_path, input_size)\n",
        "plot_predicted_vs_real(input_file_path, model)\n"
      ],
      "metadata": {
        "id": "sZreM025mMne",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 405
        },
        "outputId": "bf9a0732-73be-43e8-c828-542ca9042d09"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-28-2c1f0f27068e>:34: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/best_fuel_consumption_model.pth'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-2c1f0f27068e>\u001b[0m in \u001b[0;36m<cell line: 112>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;31m# Load model and predict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0minput_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m  \u001b[0;31m# Number of features in the input data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_trained_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0mplot_predicted_vs_real\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_file_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-28-2c1f0f27068e>\u001b[0m in \u001b[0;36mload_trained_model\u001b[0;34m(model_path, input_size)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_trained_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFuelConsumptionModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1063\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1064\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1065\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1066\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1067\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    466\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/best_fuel_consumption_model.pth'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bL109hS0blFn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KWKVbu6QblCQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Test with all saved model\n"
      ],
      "metadata": {
        "id": "PT91Xk2ibldP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "# Constants\n",
        "SEQUENCE_LENGTH = 50  # Updated sequence length\n",
        "PLOT_SAVE_DIR = 'predicted_vs_actual_plots'\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Define the PyTorch model structure (same as the one used for training)\n",
        "class FuelConsumptionModel(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(FuelConsumptionModel, self).__init__()\n",
        "        self.lstm1 = nn.LSTM(input_size, 32, batch_first=True, bidirectional=True)\n",
        "        self.dropout1 = nn.Dropout(0.2)\n",
        "        self.lstm2 = nn.LSTM(64, 32, batch_first=True, bidirectional=True)\n",
        "        self.dropout2 = nn.Dropout(0.2)\n",
        "        self.dense = nn.Linear(64, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x, _ = self.lstm1(x)\n",
        "        x = self.dropout1(x)\n",
        "        x, _ = self.lstm2(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.dense(x)\n",
        "        return x\n",
        "\n",
        "# Load the trained model\n",
        "def load_trained_model(model_path, input_size):\n",
        "    model = FuelConsumptionModel(input_size=input_size)\n",
        "    model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n",
        "    model.to(DEVICE)\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "# Process the file and prepare segments\n",
        "def process_file(file_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "    df['Time'] = df['Time'] - df['Time'].iloc[0]\n",
        "    df['Trip fuel consumption'] = df['Trip fuel consumption'] - df['Trip fuel consumption'].iloc[0]\n",
        "    df['Acceleration'] = df['Speed'].diff().fillna(0)\n",
        "    features = df[['Engine speed', 'Speed', 'slope', 'Acceleration']]\n",
        "    df['Momentary fuel consumption'] = df['Trip fuel consumption'].diff().fillna(0)\n",
        "    target = df['Momentary fuel consumption']\n",
        "    return features, target\n",
        "\n",
        "# Pad and normalize the data\n",
        "def pad_and_normalize(data, sequence_length=SEQUENCE_LENGTH):\n",
        "    padded_data = np.zeros((len(data), sequence_length, data[0].shape[1]))\n",
        "    for i, seq in enumerate(data):\n",
        "        length = min(len(seq), sequence_length)\n",
        "        padded_data[i, :length] = seq[:length]\n",
        "\n",
        "    # Normalization (same as in your script)\n",
        "    min_val_x = [0, 0, -10, -10]\n",
        "    max_val_x = [8000, 150, 10, 10]\n",
        "    for i in range(padded_data.shape[-1]):\n",
        "        padded_data[:, :, i] = (padded_data[:, :, i] - min_val_x[i]) / (max_val_x[i] - min_val_x[i])\n",
        "\n",
        "    return torch.tensor(padded_data, dtype=torch.float32).to(DEVICE)\n",
        "\n",
        "# Predict and plot the results\n",
        "def plot_predicted_vs_real(input_file, model, model_name):\n",
        "    features, actual_values = process_file(input_file)\n",
        "    num_segments = len(features) // SEQUENCE_LENGTH\n",
        "    predictions = []\n",
        "\n",
        "    for i in range(num_segments):\n",
        "        segment = features.iloc[i * SEQUENCE_LENGTH:(i + 1) * SEQUENCE_LENGTH]\n",
        "        segment_normalized = pad_and_normalize([segment.values])\n",
        "        with torch.no_grad():\n",
        "            segment_predictions = model(segment_normalized).cpu().numpy()\n",
        "        predictions.extend(segment_predictions.flatten() * 20000)\n",
        "\n",
        "    # Handle any remaining data\n",
        "    remainder = len(features) % SEQUENCE_LENGTH\n",
        "    if remainder != 0:\n",
        "        last_segment = features.iloc[-remainder:]\n",
        "        last_segment_normalized = pad_and_normalize([last_segment.values], sequence_length=remainder)\n",
        "        with torch.no_grad():\n",
        "            last_segment_predictions = model(last_segment_normalized).cpu().numpy()\n",
        "        predictions.extend(last_segment_predictions.flatten() * 20000)\n",
        "\n",
        "    predictions = np.array(predictions)\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(np.cumsum(actual_values.values[:len(predictions)], axis=0), label='Real', color='blue')\n",
        "    plt.plot(np.cumsum(predictions[:len(actual_values)], axis=0), label='Predicted', color='red')\n",
        "    plt.xlabel('Index')\n",
        "    plt.ylabel('Fuel Consumption')\n",
        "    plt.title(f'Predicted vs Real Fuel Consumption ({model_name})')\n",
        "    plt.legend()\n",
        "\n",
        "    # Save plot using model name\n",
        "    directory, filename = os.path.split(input_file)\n",
        "    plot_filename = os.path.join(directory, f'{os.path.splitext(filename)[0]}_predicted_vs_real_{model_name}.png')\n",
        "    plt.savefig(plot_filename)\n",
        "    plt.close()\n",
        "\n",
        "    print(f\"Plot saved as: {plot_filename}\")\n",
        "\n",
        "    # Save predictions and actual values to CSV using model name\n",
        "    results_df = pd.DataFrame({\n",
        "        'Speed': features[\"Speed\"].iloc[:len(predictions)],\n",
        "        'Actual': np.cumsum(actual_values.values[:len(predictions)], axis=0),\n",
        "        'Predicted': np.cumsum(predictions[:len(actual_values)], axis=0)\n",
        "    })\n",
        "\n",
        "    csv_filename = os.path.join(directory, f'{os.path.splitext(filename)[0]}_predicted_vs_real_{model_name}.csv')\n",
        "    results_df.to_csv(csv_filename, index=False)\n",
        "    print(f\"CSV saved as: {csv_filename}\")\n",
        "\n",
        "# Paths to input file and directory with models\n",
        "input_file_path = '/content/NEDC_1000_slope_added.csv'\n",
        "model_dir = '/content/'  # Directory containing all saved models\n",
        "\n",
        "# List all model files in the directory\n",
        "model_files = [f for f in os.listdir(model_dir) if f.endswith('.pth')]\n",
        "\n",
        "# Load and test each model\n",
        "input_size = 4  # Number of features in the input data\n",
        "\n",
        "for model_file in model_files:\n",
        "    model_path = os.path.join(model_dir, model_file)\n",
        "    model_name = os.path.splitext(model_file)[0]  # Get the base name of the model file\n",
        "    print(f\"Testing model: {model_name}\")\n",
        "\n",
        "    model = load_trained_model(model_path, input_size)\n",
        "    plot_predicted_vs_real(input_file_path, model, model_name)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N8OBvq4hbk8Y",
        "outputId": "2db871c1-8024-4bc8-b0e3-dad09454bcca"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing model: best_fuel_consumption_model_batch96_l50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-29-a4a523836d94>:34: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Plot saved as: /content/NEDC_1000_slope_added_predicted_vs_real_best_fuel_consumption_model_batch96_l50.png\n",
            "CSV saved as: /content/NEDC_1000_slope_added_predicted_vs_real_best_fuel_consumption_model_batch96_l50.csv\n",
            "Testing model: best_fuel_consumption_model_batch64_l50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-29-a4a523836d94>:34: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Plot saved as: /content/NEDC_1000_slope_added_predicted_vs_real_best_fuel_consumption_model_batch64_l50.png\n",
            "CSV saved as: /content/NEDC_1000_slope_added_predicted_vs_real_best_fuel_consumption_model_batch64_l50.csv\n",
            "Testing model: best_fuel_consumption_model_batch45_l50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-29-a4a523836d94>:34: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Plot saved as: /content/NEDC_1000_slope_added_predicted_vs_real_best_fuel_consumption_model_batch45_l50.png\n",
            "CSV saved as: /content/NEDC_1000_slope_added_predicted_vs_real_best_fuel_consumption_model_batch45_l50.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "# Constants\n",
        "SEQUENCE_LENGTH = 50  # Updated sequence length\n",
        "PLOT_SAVE_DIR = 'predicted_vs_actual_plots'  # Base directory to save plots and CSVs\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Ensure the base save directory exists\n",
        "os.makedirs(PLOT_SAVE_DIR, exist_ok=True)\n",
        "\n",
        "# Define the PyTorch model structure (same as the one used for training)\n",
        "class FuelConsumptionModel(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(FuelConsumptionModel, self).__init__()\n",
        "        self.lstm1 = nn.LSTM(input_size, 32, batch_first=True, bidirectional=True)\n",
        "        self.dropout1 = nn.Dropout(0.2)\n",
        "        self.lstm2 = nn.LSTM(64, 32, batch_first=True, bidirectional=True)\n",
        "        self.dropout2 = nn.Dropout(0.2)\n",
        "        self.dense = nn.Linear(64, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x, _ = self.lstm1(x)\n",
        "        x = self.dropout1(x)\n",
        "        x, _ = self.lstm2(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.dense(x)\n",
        "        return x\n",
        "\n",
        "# Load the trained model\n",
        "def load_trained_model(model_path, input_size):\n",
        "    model = FuelConsumptionModel(input_size=input_size)\n",
        "    model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n",
        "    model.to(DEVICE)\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "# Process the file and prepare segments\n",
        "def process_file(file_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "    df['Time'] = df['time'] - df['time'].iloc[0]\n",
        "    df['Trip fuel consumption'] = df['Trip_fuel_consumption'] - df['Trip_fuel_consumption'].iloc[0]\n",
        "    df['Acceleration'] = df['Vehicle_Speed'].diff().fillna(0)\n",
        "    features = df[['Engine_speed', 'Vehicle_Speed', 'slope', 'Acceleration']]\n",
        "    df['Momentary fuel consumption'] = df['Trip fuel consumption'].diff().fillna(0)\n",
        "    target = df['Momentary fuel consumption']\n",
        "    return features, target\n",
        "\n",
        "# Pad and normalize the data\n",
        "def pad_and_normalize(data, sequence_length=SEQUENCE_LENGTH):\n",
        "    padded_data = np.zeros((len(data), sequence_length, data[0].shape[1]))\n",
        "    for i, seq in enumerate(data):\n",
        "        length = min(len(seq), sequence_length)\n",
        "        padded_data[i, :length] = seq[:length]\n",
        "\n",
        "    # Normalization (same as in your script)\n",
        "    min_val_x = [0, 0, -10, -10]\n",
        "    max_val_x = [8000, 150, 10, 10]\n",
        "    for i in range(padded_data.shape[-1]):\n",
        "        padded_data[:, :, i] = (padded_data[:, :, i] - min_val_x[i]) / (max_val_x[i] - min_val_x[i])\n",
        "\n",
        "    return torch.tensor(padded_data, dtype=torch.float32).to(DEVICE)\n",
        "\n",
        "# Predict and plot the results\n",
        "def plot_predicted_vs_real(input_file, model, model_name):\n",
        "    features, actual_values = process_file(input_file)\n",
        "    num_segments = len(features) // SEQUENCE_LENGTH\n",
        "    predictions = []\n",
        "\n",
        "    for i in range(num_segments):\n",
        "        segment = features.iloc[i * SEQUENCE_LENGTH:(i + 1) * SEQUENCE_LENGTH]\n",
        "        segment_normalized = pad_and_normalize([segment.values])\n",
        "        with torch.no_grad():\n",
        "            segment_predictions = model(segment_normalized).cpu().numpy()\n",
        "        predictions.extend(segment_predictions.flatten() * 20000)\n",
        "\n",
        "    # Handle any remaining data\n",
        "    remainder = len(features) % SEQUENCE_LENGTH\n",
        "    if remainder != 0:\n",
        "        last_segment = features.iloc[-remainder:]\n",
        "        last_segment_normalized = pad_and_normalize([last_segment.values], sequence_length=remainder)\n",
        "        with torch.no_grad():\n",
        "            last_segment_predictions = model(last_segment_normalized).cpu().numpy()\n",
        "        predictions.extend(last_segment_predictions.flatten() * 20000)\n",
        "\n",
        "    predictions = np.array(predictions)\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(np.cumsum(actual_values.values[:len(predictions)], axis=0), label='Real', color='blue')\n",
        "    plt.plot(np.cumsum(predictions[:len(actual_values)], axis=0), label='Predicted', color='red')\n",
        "    plt.xlabel('Index')\n",
        "    plt.ylabel('Fuel Consumption')\n",
        "    plt.title(f'Predicted vs Real Fuel Consumption ({model_name})')\n",
        "    plt.legend()\n",
        "\n",
        "    # Create model-specific directory\n",
        "    model_save_dir = os.path.join(PLOT_SAVE_DIR, model_name)\n",
        "    os.makedirs(model_save_dir, exist_ok=True)\n",
        "\n",
        "    # Save plot using model name in the designated directory\n",
        "    plot_filename = os.path.join(model_save_dir, f'{os.path.splitext(os.path.basename(input_file))[0]}_predicted_vs_real_{model_name}.png')\n",
        "    plt.savefig(plot_filename)\n",
        "    plt.close()\n",
        "\n",
        "    print(f\"Plot saved as: {plot_filename}\")\n",
        "\n",
        "    # Save predictions and actual values to CSV using model name in the designated directory\n",
        "    results_df = pd.DataFrame({\n",
        "        'Speed': features[\"Vehicle_Speed\"].iloc[:len(predictions)],\n",
        "        'Actual': np.cumsum(actual_values.values[:len(predictions)], axis=0),\n",
        "        'Predicted': np.cumsum(predictions[:len(actual_values)], axis=0)\n",
        "    })\n",
        "\n",
        "    csv_filename = os.path.join(model_save_dir, f'{os.path.splitext(os.path.basename(input_file))[0]}_predicted_vs_real_{model_name}.csv')\n",
        "    results_df.to_csv(csv_filename, index=False)\n",
        "    print(f\"CSV saved as: {csv_filename}\")\n",
        "\n",
        "# Paths to input file and directory with models\n",
        "input_file_path = '/content/output_file_56.csv'\n",
        "model_dir = '/content/'  # Directory containing all saved models\n",
        "\n",
        "# List all model files in the directory\n",
        "model_files = [f for f in os.listdir(model_dir) if f.endswith('.pth')]\n",
        "\n",
        "# Load and test each model\n",
        "input_size = 4  # Number of features in the input data\n",
        "\n",
        "for model_file in model_files:\n",
        "    model_path = os.path.join(model_dir, model_file)\n",
        "    model_name = os.path.splitext(model_file)[0]  # Get the base name of the model file\n",
        "    print(f\"Testing model: {model_name}\")\n",
        "\n",
        "    model = load_trained_model(model_path, input_size)\n",
        "    plot_predicted_vs_real(input_file_path, model, model_name)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89nsl4H08I7T",
        "outputId": "7ed5aaaa-2db9-45c6-9b09-6b1a180dfa05"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing model: best_fuel_consumption_model_batch96_l50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-40-ca137ff1d95e>:37: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Plot saved as: predicted_vs_actual_plots/best_fuel_consumption_model_batch96_l50/output_file_56_predicted_vs_real_best_fuel_consumption_model_batch96_l50.png\n",
            "CSV saved as: predicted_vs_actual_plots/best_fuel_consumption_model_batch96_l50/output_file_56_predicted_vs_real_best_fuel_consumption_model_batch96_l50.csv\n",
            "Testing model: best_fuel_consumption_model_batch64_l50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-40-ca137ff1d95e>:37: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Plot saved as: predicted_vs_actual_plots/best_fuel_consumption_model_batch64_l50/output_file_56_predicted_vs_real_best_fuel_consumption_model_batch64_l50.png\n",
            "CSV saved as: predicted_vs_actual_plots/best_fuel_consumption_model_batch64_l50/output_file_56_predicted_vs_real_best_fuel_consumption_model_batch64_l50.csv\n",
            "Testing model: best_fuel_consumption_model_batch45_l50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-40-ca137ff1d95e>:37: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Plot saved as: predicted_vs_actual_plots/best_fuel_consumption_model_batch45_l50/output_file_56_predicted_vs_real_best_fuel_consumption_model_batch45_l50.png\n",
            "CSV saved as: predicted_vs_actual_plots/best_fuel_consumption_model_batch45_l50/output_file_56_predicted_vs_real_best_fuel_consumption_model_batch45_l50.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/predicted_vs_actual_plots.zip /content/predicted_vs_actual_plots"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ld0z9cg9-Fth",
        "outputId": "37f759ba-bd0a-4194-9c63-3be092306d20"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/predicted_vs_actual_plots/ (stored 0%)\n",
            "  adding: content/predicted_vs_actual_plots/NEDC_1000_slope_added_predicted_vs_real_best_fuel_consumption_model_batch45_l50.png (deflated 9%)\n",
            "  adding: content/predicted_vs_actual_plots/best_fuel_consumption_model_batch96_l50/ (stored 0%)\n",
            "  adding: content/predicted_vs_actual_plots/best_fuel_consumption_model_batch96_l50/output_file_53_predicted_vs_real_best_fuel_consumption_model_batch96_l50.csv (deflated 57%)\n",
            "  adding: content/predicted_vs_actual_plots/best_fuel_consumption_model_batch96_l50/output_file_17_predicted_vs_real_best_fuel_consumption_model_batch96_l50.csv (deflated 57%)\n",
            "  adding: content/predicted_vs_actual_plots/best_fuel_consumption_model_batch96_l50/output_file_56_predicted_vs_real_best_fuel_consumption_model_batch96_l50.png (deflated 9%)\n",
            "  adding: content/predicted_vs_actual_plots/best_fuel_consumption_model_batch96_l50/output_file_44_predicted_vs_real_best_fuel_consumption_model_batch96_l50.csv (deflated 58%)\n",
            "  adding: content/predicted_vs_actual_plots/best_fuel_consumption_model_batch96_l50/output_file_6_predicted_vs_real_best_fuel_consumption_model_batch96_l50.csv (deflated 58%)\n",
            "  adding: content/predicted_vs_actual_plots/best_fuel_consumption_model_batch96_l50/output_file_35_predicted_vs_real_best_fuel_consumption_model_batch96_l50.png (deflated 9%)\n",
            "  adding: content/predicted_vs_actual_plots/best_fuel_consumption_model_batch96_l50/output_file_17_predicted_vs_real_best_fuel_consumption_model_batch96_l50.png (deflated 9%)\n",
            "  adding: content/predicted_vs_actual_plots/best_fuel_consumption_model_batch96_l50/output_file_21_predicted_vs_real_best_fuel_consumption_model_batch96_l50.png (deflated 9%)\n",
            "  adding: content/predicted_vs_actual_plots/best_fuel_consumption_model_batch96_l50/output_file_53_predicted_vs_real_best_fuel_consumption_model_batch96_l50.png (deflated 9%)\n",
            "  adding: content/predicted_vs_actual_plots/best_fuel_consumption_model_batch96_l50/output_file_11_predicted_vs_real_best_fuel_consumption_model_batch96_l50.png (deflated 10%)\n",
            "  adding: content/predicted_vs_actual_plots/best_fuel_consumption_model_batch96_l50/output_file_27_predicted_vs_real_best_fuel_consumption_model_batch96_l50.csv (deflated 55%)\n",
            "  adding: content/predicted_vs_actual_plots/best_fuel_consumption_model_batch96_l50/output_file_44_predicted_vs_real_best_fuel_consumption_model_batch96_l50.png (deflated 9%)\n",
            "  adding: content/predicted_vs_actual_plots/best_fuel_consumption_model_batch96_l50/output_file_21_predicted_vs_real_best_fuel_consumption_model_batch96_l50.csv (deflated 56%)\n",
            "  adding: content/predicted_vs_actual_plots/best_fuel_consumption_model_batch96_l50/output_file_48_predicted_vs_real_best_fuel_consumption_model_batch96_l50.csv (deflated 55%)\n",
            "  adding: content/predicted_vs_actual_plots/best_fuel_consumption_model_batch96_l50/output_file_27_predicted_vs_real_best_fuel_consumption_model_batch96_l50.png (deflated 9%)\n",
            "  adding: content/predicted_vs_actual_plots/best_fuel_consumption_model_batch96_l50/output_file_35_predicted_vs_real_best_fuel_consumption_model_batch96_l50.csv (deflated 55%)\n",
            "  adding: content/predicted_vs_actual_plots/best_fuel_consumption_model_batch96_l50/output_file_56_predicted_vs_real_best_fuel_consumption_model_batch96_l50.csv (deflated 55%)\n",
            "  adding: content/predicted_vs_actual_plots/best_fuel_consumption_model_batch96_l50/output_file_48_predicted_vs_real_best_fuel_consumption_model_batch96_l50.png (deflated 10%)\n",
            "  adding: content/predicted_vs_actual_plots/best_fuel_consumption_model_batch96_l50/output_file_11_predicted_vs_real_best_fuel_consumption_model_batch96_l50.csv (deflated 64%)\n",
            "  adding: content/predicted_vs_actual_plots/best_fuel_consumption_model_batch96_l50/output_file_6_predicted_vs_real_best_fuel_consumption_model_batch96_l50.png (deflated 9%)\n",
            "  adding: content/predicted_vs_actual_plots/NEDC_1000_slope_added_predicted_vs_real_best_fuel_consumption_model_batch96_l50.png (deflated 9%)\n",
            "  adding: content/predicted_vs_actual_plots/NEDC_1000_slope_added_predicted_vs_real_best_fuel_consumption_model_batch96_l50.csv (deflated 52%)\n",
            "  adding: content/predicted_vs_actual_plots/NEDC_1000_slope_added_predicted_vs_real_best_fuel_consumption_model_batch64_l50.png (deflated 9%)\n",
            "  adding: content/predicted_vs_actual_plots/NEDC_1000_slope_added_predicted_vs_real_best_fuel_consumption_model_batch64_l50.csv (deflated 52%)\n",
            "  adding: content/predicted_vs_actual_plots/best_fuel_consumption_model_batch45_l50/ (stored 0%)\n",
            "  adding: content/predicted_vs_actual_plots/best_fuel_consumption_model_batch45_l50/output_file_11_predicted_vs_real_best_fuel_consumption_model_batch45_l50.csv (deflated 64%)\n",
            "  adding: content/predicted_vs_actual_plots/best_fuel_consumption_model_batch45_l50/output_file_6_predicted_vs_real_best_fuel_consumption_model_batch45_l50.png (deflated 9%)\n",
            "  adding: content/predicted_vs_actual_plots/best_fuel_consumption_model_batch45_l50/output_file_53_predicted_vs_real_best_fuel_consumption_model_batch45_l50.png (deflated 9%)\n",
            "  adding: content/predicted_vs_actual_plots/best_fuel_consumption_model_batch45_l50/output_file_35_predicted_vs_real_best_fuel_consumption_model_batch45_l50.png (deflated 9%)\n",
            "  adding: content/predicted_vs_actual_plots/best_fuel_consumption_model_batch45_l50/output_file_56_predicted_vs_real_best_fuel_consumption_model_batch45_l50.csv (deflated 55%)\n",
            "  adding: content/predicted_vs_actual_plots/best_fuel_consumption_model_batch45_l50/output_file_21_predicted_vs_real_best_fuel_consumption_model_batch45_l50.csv (deflated 56%)\n",
            "  adding: content/predicted_vs_actual_plots/best_fuel_consumption_model_batch45_l50/output_file_48_predicted_vs_real_best_fuel_consumption_model_batch45_l50.png (deflated 9%)\n",
            "  adding: content/predicted_vs_actual_plots/best_fuel_consumption_model_batch45_l50/output_file_44_predicted_vs_real_best_fuel_consumption_model_batch45_l50.csv (deflated 58%)\n",
            "  adding: content/predicted_vs_actual_plots/best_fuel_consumption_model_batch45_l50/output_file_44_predicted_vs_real_best_fuel_consumption_model_batch45_l50.png (deflated 9%)\n",
            "  adding: content/predicted_vs_actual_plots/best_fuel_consumption_model_batch45_l50/output_file_21_predicted_vs_real_best_fuel_consumption_model_batch45_l50.png (deflated 9%)\n",
            "  adding: content/predicted_vs_actual_plots/best_fuel_consumption_model_batch45_l50/output_file_27_predicted_vs_real_best_fuel_consumption_model_batch45_l50.png (deflated 9%)\n",
            "  adding: content/predicted_vs_actual_plots/best_fuel_consumption_model_batch45_l50/output_file_6_predicted_vs_real_best_fuel_consumption_model_batch45_l50.csv (deflated 58%)\n",
            "  adding: content/predicted_vs_actual_plots/best_fuel_consumption_model_batch45_l50/output_file_27_predicted_vs_real_best_fuel_consumption_model_batch45_l50.csv (deflated 55%)\n",
            "  adding: content/predicted_vs_actual_plots/best_fuel_consumption_model_batch45_l50/output_file_17_predicted_vs_real_best_fuel_consumption_model_batch45_l50.csv (deflated 57%)\n",
            "  adding: content/predicted_vs_actual_plots/best_fuel_consumption_model_batch45_l50/output_file_35_predicted_vs_real_best_fuel_consumption_model_batch45_l50.csv (deflated 55%)\n",
            "  adding: content/predicted_vs_actual_plots/best_fuel_consumption_model_batch45_l50/output_file_56_predicted_vs_real_best_fuel_consumption_model_batch45_l50.png (deflated 9%)\n",
            "  adding: content/predicted_vs_actual_plots/best_fuel_consumption_model_batch45_l50/output_file_48_predicted_vs_real_best_fuel_consumption_model_batch45_l50.csv (deflated 55%)\n",
            "  adding: content/predicted_vs_actual_plots/best_fuel_consumption_model_batch45_l50/output_file_17_predicted_vs_real_best_fuel_consumption_model_batch45_l50.png (deflated 9%)\n",
            "  adding: content/predicted_vs_actual_plots/best_fuel_consumption_model_batch45_l50/output_file_11_predicted_vs_real_best_fuel_consumption_model_batch45_l50.png (deflated 10%)\n",
            "  adding: content/predicted_vs_actual_plots/best_fuel_consumption_model_batch45_l50/output_file_53_predicted_vs_real_best_fuel_consumption_model_batch45_l50.csv (deflated 57%)\n",
            "  adding: content/predicted_vs_actual_plots/NEDC_1000_slope_added_predicted_vs_real_best_fuel_consumption_model_batch45_l50.csv (deflated 52%)\n",
            "  adding: content/predicted_vs_actual_plots/best_fuel_consumption_model_batch64_l50/ (stored 0%)\n",
            "  adding: content/predicted_vs_actual_plots/best_fuel_consumption_model_batch64_l50/output_file_11_predicted_vs_real_best_fuel_consumption_model_batch64_l50.csv (deflated 64%)\n",
            "  adding: content/predicted_vs_actual_plots/best_fuel_consumption_model_batch64_l50/output_file_44_predicted_vs_real_best_fuel_consumption_model_batch64_l50.csv (deflated 58%)\n",
            "  adding: content/predicted_vs_actual_plots/best_fuel_consumption_model_batch64_l50/output_file_17_predicted_vs_real_best_fuel_consumption_model_batch64_l50.png (deflated 9%)\n",
            "  adding: content/predicted_vs_actual_plots/best_fuel_consumption_model_batch64_l50/output_file_11_predicted_vs_real_best_fuel_consumption_model_batch64_l50.png (deflated 10%)\n",
            "  adding: content/predicted_vs_actual_plots/best_fuel_consumption_model_batch64_l50/output_file_48_predicted_vs_real_best_fuel_consumption_model_batch64_l50.csv (deflated 55%)\n",
            "  adding: content/predicted_vs_actual_plots/best_fuel_consumption_model_batch64_l50/output_file_53_predicted_vs_real_best_fuel_consumption_model_batch64_l50.csv (deflated 57%)\n",
            "  adding: content/predicted_vs_actual_plots/best_fuel_consumption_model_batch64_l50/output_file_6_predicted_vs_real_best_fuel_consumption_model_batch64_l50.csv (deflated 58%)\n",
            "  adding: content/predicted_vs_actual_plots/best_fuel_consumption_model_batch64_l50/output_file_21_predicted_vs_real_best_fuel_consumption_model_batch64_l50.csv (deflated 56%)\n",
            "  adding: content/predicted_vs_actual_plots/best_fuel_consumption_model_batch64_l50/output_file_21_predicted_vs_real_best_fuel_consumption_model_batch64_l50.png (deflated 9%)\n",
            "  adding: content/predicted_vs_actual_plots/best_fuel_consumption_model_batch64_l50/output_file_53_predicted_vs_real_best_fuel_consumption_model_batch64_l50.png (deflated 9%)\n",
            "  adding: content/predicted_vs_actual_plots/best_fuel_consumption_model_batch64_l50/output_file_35_predicted_vs_real_best_fuel_consumption_model_batch64_l50.png (deflated 9%)\n",
            "  adding: content/predicted_vs_actual_plots/best_fuel_consumption_model_batch64_l50/output_file_27_predicted_vs_real_best_fuel_consumption_model_batch64_l50.csv (deflated 55%)\n",
            "  adding: content/predicted_vs_actual_plots/best_fuel_consumption_model_batch64_l50/output_file_56_predicted_vs_real_best_fuel_consumption_model_batch64_l50.csv (deflated 56%)\n",
            "  adding: content/predicted_vs_actual_plots/best_fuel_consumption_model_batch64_l50/output_file_35_predicted_vs_real_best_fuel_consumption_model_batch64_l50.csv (deflated 55%)\n",
            "  adding: content/predicted_vs_actual_plots/best_fuel_consumption_model_batch64_l50/output_file_48_predicted_vs_real_best_fuel_consumption_model_batch64_l50.png (deflated 9%)\n",
            "  adding: content/predicted_vs_actual_plots/best_fuel_consumption_model_batch64_l50/output_file_17_predicted_vs_real_best_fuel_consumption_model_batch64_l50.csv (deflated 57%)\n",
            "  adding: content/predicted_vs_actual_plots/best_fuel_consumption_model_batch64_l50/output_file_6_predicted_vs_real_best_fuel_consumption_model_batch64_l50.png (deflated 9%)\n",
            "  adding: content/predicted_vs_actual_plots/best_fuel_consumption_model_batch64_l50/output_file_27_predicted_vs_real_best_fuel_consumption_model_batch64_l50.png (deflated 9%)\n",
            "  adding: content/predicted_vs_actual_plots/best_fuel_consumption_model_batch64_l50/output_file_44_predicted_vs_real_best_fuel_consumption_model_batch64_l50.png (deflated 9%)\n",
            "  adding: content/predicted_vs_actual_plots/best_fuel_consumption_model_batch64_l50/output_file_56_predicted_vs_real_best_fuel_consumption_model_batch64_l50.png (deflated 9%)\n"
          ]
        }
      ]
    }
  ]
}
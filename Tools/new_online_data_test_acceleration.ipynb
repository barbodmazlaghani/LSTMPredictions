{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E0o0gASdzJv3",
        "outputId": "985626d9-6d2f-4c2d-eda2-c5024a239799"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:tensorflow:5 out of the last 9 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7e1094e9b370> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 636ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step\n",
            "Plot saved as: /content/output_file_17_predicted_vs_real.png\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import joblib\n",
        "\n",
        "# Load scalers\n",
        "scaler_X = joblib.load('scaler_X.pkl')\n",
        "scaler_y = joblib.load('scaler_y.pkl')\n",
        "\n",
        "SEQUENCE_LENGTH = 600\n",
        "PLOT_SAVE_DIR = 'predicted_vs_actual_plots'\n",
        "\n",
        "# Load the trained model\n",
        "def load_trained_model(model_path):\n",
        "    return load_model(model_path)\n",
        "\n",
        "# Process the file and prepare segments\n",
        "def process_file(file_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "    # df['Time'] = df['Time'] - df['Time'].iloc[0]\n",
        "    df['Trip fuel consumption'] = df['Trip_fuel_consumption'] - df['Trip_fuel_consumption'].iloc[0]\n",
        "    df['Acceleration'] = df['Vehicle_Speed'].diff().fillna(0)\n",
        "    features = df[['Engine_speed', 'Vehicle_Speed', 'slope', 'Acceleration']]\n",
        "    df['Momentary fuel consumption'] = df['Trip_fuel_consumption'].diff().fillna(0)\n",
        "    target = df['Momentary fuel consumption']\n",
        "    return features, target\n",
        "\n",
        "# Pad and normalize the data\n",
        "def pad_and_normalize(data, scaler, sequence_length=SEQUENCE_LENGTH):\n",
        "    padded_data = pad_sequences(data, maxlen=sequence_length, dtype='float32', padding='post', truncating='post')\n",
        "    normalized_data = scaler.transform(padded_data.reshape(-1, padded_data.shape[-1])).reshape(padded_data.shape)\n",
        "    return normalized_data\n",
        "\n",
        "# Predict and plot the results\n",
        "def plot_predicted_vs_real(input_file, model, scaler_X, scaler_y):\n",
        "    features, actual_values = process_file(input_file)\n",
        "    num_segments = len(features) // SEQUENCE_LENGTH\n",
        "    predictions = []\n",
        "\n",
        "    for i in range(num_segments):\n",
        "        segment = features.iloc[i * SEQUENCE_LENGTH:(i + 1) * SEQUENCE_LENGTH]\n",
        "        segment_normalized = pad_and_normalize([segment.values], scaler_X)\n",
        "        segment_predictions = model.predict(segment_normalized)\n",
        "        predictions.extend(scaler_y.inverse_transform(segment_predictions.reshape(-1, 1)))\n",
        "\n",
        "    # Handle any remaining data\n",
        "    remainder = len(features) % SEQUENCE_LENGTH\n",
        "    if remainder != 0:\n",
        "        last_segment = features.iloc[-remainder:]\n",
        "        last_segment_normalized = pad_and_normalize([last_segment.values], scaler_X)\n",
        "        last_segment_predictions = model.predict(last_segment_normalized)\n",
        "        predictions.extend(scaler_y.inverse_transform(last_segment_predictions.reshape(-1, 1)))\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(np.cumsum(actual_values.values, axis=0), label='Real', color='blue')\n",
        "    plt.plot(np.cumsum(predictions[:len(actual_values)], axis=0), label='Predicted', color='red')\n",
        "    plt.xlabel('Index')\n",
        "    plt.ylabel('Fuel Consumption')\n",
        "    plt.title('Predicted vs Real Fuel Consumption')\n",
        "    plt.legend()\n",
        "\n",
        "    directory, filename = os.path.split(input_file)\n",
        "    plot_filename = os.path.join(directory, f'{os.path.splitext(filename)[0]}_predicted_vs_real.png')\n",
        "    plt.savefig(plot_filename)\n",
        "    plt.close()\n",
        "\n",
        "    print(f\"Plot saved as: {plot_filename}\")\n",
        "    predictions = predictions[:len(actual_values)]\n",
        "\n",
        "    # Save predictions and actual values to CSV\n",
        "    results_df = pd.DataFrame({\n",
        "        'Speed': features[\"Vehicle_Speed\"],\n",
        "        'Actual': np.cumsum(actual_values.values, axis=0),\n",
        "        'Predicted': np.cumsum(predictions, axis=0).flatten()  # flatten the cumulative predictions\n",
        "    })\n",
        "\n",
        "    directory, filename = os.path.split(input_file)\n",
        "    csv_filename = os.path.join(directory, f'{os.path.splitext(filename)[0]}_predicted_vs_real.csv')\n",
        "    results_df.to_csv(csv_filename, index=False)\n",
        "\n",
        "# Paths to model and input file\n",
        "model_path = '/content/modelBLSTM_new_majid_added_acceleration_added.h5'\n",
        "input_file_path = '/content/output_file_17.csv'\n",
        "\n",
        "# Load model and predict\n",
        "model = load_trained_model(model_path)\n",
        "plot_predicted_vs_real(input_file_path, model, scaler_X, scaler_y)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}